[{"title":"0.è§£æ„åŸºæœ¬pipeline","url":"/2024/04/07/0.%20%E8%A7%A3%E6%9E%84%E5%9F%BA%E6%9C%ACpipeline/","content":"è§£æ„åŸºæœ¬pipelineå€ŸåŠ©pipelineï¼Œä»…éœ€å››è¡Œä»£ç å³å¯ç”Ÿæˆå›¾åƒï¼š\nfrom diffusers import DDPMPipelineddpm = DDPMPipeline.from_pretrained(&quot;google/ddpm-cat-256&quot;, use_safetensors=True).to(&quot;cuda&quot;)image = ddpm(num_inference_steps=25).images[0]\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼ŒpipelineåŒ…å«ï¼š\n\nå»å™ªæ¨¡å‹ï¼šUNet2DModel\né‡‡æ ·å™¨ï¼šDDPMScheduler\n\npipelineé€šè¿‡è·å–æ‰€éœ€è¾“å‡ºå¤§å°çš„éšæœºå™ªå£°å¹¶å°†å…¶å¤šæ¬¡é€šè¿‡å»å™ªæ¨¡å‹æ¥å¯¹å›¾åƒè¿›è¡Œå»å™ªã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œå»å™ªæ¨¡å‹é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œè°ƒé‡‡æ ·å™¨ä½¿ç”¨å®ƒæ¥é¢„æµ‹å™ªå£°è¾ƒå°çš„å›¾åƒã€‚ç®¡é“é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°åˆ°è¾¾æŒ‡å®šæ•°é‡çš„æ¨ç†æ­¥éª¤çš„å›¾åƒã€‚\nè¦åˆ†åˆ«ä½¿ç”¨å»å™ªæ¨¡å‹å’Œé‡‡æ ·å™¨é‡æ–°åˆ›å»ºpipelineï¼Œè®©æˆ‘ä»¬ç¼–å†™è‡ªå·±çš„å»å™ªè¿‡ç¨‹ï¼š\n\nè½½å…¥å»å™ªæ¨¡å‹å’Œé‡‡æ ·å™¨ï¼š\nfrom diffusers import DDPMScheduler, UNet2DModelscheduler = DDPMScheduler.from_pretrained(&quot;google/ddpm-cat-256&quot;)model = UNet2DModel.from_pretrained(&quot;google/ddpm-cat-256&quot;, use_safetensors=True).to(&quot;cuda&quot;)\n\né€šè¿‡é‡‡æ ·å™¨è®¾ç½®å»å™ªè¿‡ç¨‹çš„æ—¶é—´æ­¥æ•°ï¼š\nscheduler.set_timesteps(50)\n\nè®¾ç½®é‡‡æ ·å™¨çš„æ—¶é—´æ­¥æ•°ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«å‡åŒ€é—´éš”å…ƒç´ çš„å¼ é‡ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­ä¸º 50ã€‚æ¯ä¸ªå…ƒç´ å¯¹åº”äºæ¨¡å‹å¯¹å›¾åƒè¿›è¡Œå»å™ªçš„æ—¶é—´æ­¥ã€‚åœ¨è¿›è¡Œå»å™ªå¾ªç¯æ—¶ï¼Œå°†è¿­ä»£è¯¥å¼ é‡ä»¥å¯¹å›¾åƒè¿›è¡Œå»å™ªï¼š\nscheduler.timestepstensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,    \t700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,    \t420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,    \t140, 120, 100,  80,  60,  40,  20,   0])\n\nåˆ›å»ºä¸æ‰€éœ€è¾“å‡ºå›¾åƒå½¢çŠ¶ç›¸åŒçš„éšæœºå™ªå£°ï¼š\nimport torchsample_size = model.config.sample_sizenoise = torch.randn((1, 3, sample_size, sample_size), device=&quot;cuda&quot;)\n\nç°åœ¨ç¼–å†™ä¸€ä¸ªå¾ªç¯æ¥è¿­ä»£ã€‚\n\nåœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œéƒ½ä¼šæ‰§è¡Œ UNet2DModel.forward() ï¼Œæ ¹æ®è¾“å…¥çš„å™ªå£°å›¾åƒ(input)å’Œæ—¶é—´æ­¥(t)è¿”å›å™ªå£°æ®‹å·®(noisy_residual)ã€‚\né‡‡æ ·å™¨çš„ step() æ–¹æ³•åˆ©ç”¨å™ªå£°æ®‹å·®(noisy_residual)ã€æ—¶é—´æ­¥(t))å’Œå™ªå£°å›¾åƒ(input)é¢„æµ‹å‰ä¸€ä¸ªæ—¶é—´æ­¥é•¿çš„å›¾åƒã€‚è¯¥è¾“å‡ºæˆä¸ºå»å™ªå¾ªç¯çš„ä¸‹ä¸€ä¸ªè¾“å…¥ã€‚\n\ninput = noisefor t in scheduler.timesteps:    with torch.no_grad():        noisy_residual = model(input, t).sample    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample    input = previous_noisy_sample\n\næœ€åä¸€æ­¥æ˜¯å°†å»å™ªè¾“å‡ºè½¬æ¢ä¸ºå›¾åƒï¼š\nfrom PIL import Imageimport numpy as np# inputçš„æ•°æ®èŒƒå›´ä¸º[-1, 1], è½¬ä¸º[0, 1]# shape: [1, 3, sample_size, sample_size] -&gt; [3, sample_size, sample_size]image = (input / 2 + 0.5).clamp(0, 1).squeeze()# shape: [3, sample_size, sample_size] -&gt; [sample_size, sample_size, 3]# data range: (float32)[0, 1] -&gt; (uint8)[0, 255]# device: cuda -&gt; cpu# type: torch.tensor -&gt; numpy.arrayimage = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()# è¾“å‡ºä¸ºå›¾åƒimage = Image.fromarray(image)image\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"1.è§£æ„Stable Diffusion","url":"/2024/03/14/1.%20%E8%A7%A3%E6%9E%84%20Stable%20Diffusion/","content":"è§£æ„Stable DiffusionStable Diffusionæ˜¯ä¸€ç§æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚å®ƒä½¿ç”¨å›¾åƒçš„ä½ç»´è¡¨ç¤ºè€Œä¸æ˜¯å®é™…çš„åƒç´ ç©ºé—´ï¼Œè¿™ä½¿å¾—å®ƒçš„å†…å­˜æ•ˆç‡æ›´é«˜ã€‚\n\nVAEç¼–ç å™¨å°†å›¾åƒå‹ç¼©ä¸ºä½ç»´è¡¨ç¤ºï¼ŒVAEè§£ç å™¨å°†å‹ç¼©çš„ä½ç»´è¡¨ç¤ºè½¬æ¢å›å›¾åƒã€‚\n\nå¯¹äºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œéœ€è¦ä¸€ä¸ªåˆ†è¯å™¨ï¼ˆtokenizerï¼‰å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆtext encoderï¼‰æ¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆtext embeddingsï¼‰ã€‚\n\n\nå¦‚ä¸Šæ‰€è¿°ï¼ŒSD pipelineæ¯”ä»…åŒ…å« UNet æ¨¡å‹çš„ DDPM pipelineæ›´å¤æ‚ã€‚Stable Diffusionå…·æœ‰ä¸‰ä¸ªç‹¬ç«‹çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚\n\nè¿ä½œæœºåˆ¶ï¼šStable Diffusion with ğŸ§¨ Diffusers (huggingface.co)\n\n\nThe autoencoder (VAE): AutoencoderKL\nThe U-Net: UNet2DConditionModel\nThe Text-encoder: CLIPTextModel\n\nSD pipeline è¿ä½œæœºåˆ¶è½½å…¥æ‰€æœ‰ç»„ä»¶ä½¿ç”¨ from_pretrained() æ–¹æ³•åŠ è½½æ‰€æœ‰ç»„ä»¶ã€‚å¯ä»¥åœ¨é¢„è®­ç»ƒçš„ runwayml&#x2F;stable-diffusion-v1-5 ä¸­æ‰¾åˆ°å®ƒä»¬ï¼Œæ¯ä¸ªç»„ä»¶éƒ½å­˜å‚¨åœ¨å•ç‹¬çš„å­æ–‡ä»¶å¤¹ä¸­ï¼š\nfrom PIL import Imageimport torchfrom transformers import CLIPTextModel, CLIPTokenizerfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler# è‡ªç¼–ç å™¨ï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡vae = AutoencoderKL.from_pretrained(&quot;sd-v1.5&quot;, subfolder=&quot;vae&quot;, variant=&#x27;fp16&#x27;, use_safetensors=True)# åˆ†è¯å™¨tokenizer = CLIPTokenizer.from_pretrained(&quot;sd-v1.5&quot;, subfolder=&quot;tokenizer&quot;)# æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡text_encoder = CLIPTextModel.from_pretrained(&quot;sd-v1.5&quot;, subfolder=&quot;text_encoder&quot;, variant=&#x27;fp16&#x27;, use_safetensors=True)# å»å™ªå™¨Unetï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡unet = UNet2DConditionModel.from_pretrained(&quot;sd-v1.5&quot;, subfolder=&quot;unet&quot;, variant=&#x27;fp16&#x27;, use_safetensors=True)\n\nå°†é‡‡æ ·å™¨æ›¿æ¢ä¸º UniPCMultistepSchedulerï¼Œè€Œä¸æ˜¯ sd1.5 é»˜è®¤çš„ PNDMSchedulerï¼Œé€šè¿‡ä»¥ä¸‹ä»£ç å®ç°ï¼š\nfrom diffusers import UniPCMultistepScheduler# é‡‡æ ·å™¨ï¼Œä¿®æ”¹ä¸ºUniPCMultistepScheduler è€Œä¸æ˜¯é»˜è®¤çš„PNDMSchedulerscheduler = UniPCMultistepScheduler.from_pretrained(&quot;sd-v1.5&quot;, subfolder=&quot;scheduler&quot;)\n\nä¸ºäº†åŠ é€Ÿæ¨ç†ï¼Œå°†å…·æœ‰å¯è®­ç»ƒæƒé‡çš„æ¨¡å‹ç§»è‡³ GPUï¼š\ntorch_device = torch.device(&quot;cuda&quot;)vae.to(torch_device)text_encoder.to(torch_device)unet.to(torch_device)\n\n\n\nåˆ›å»ºæ–‡æœ¬åµŒå…¥å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ä»¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚è¯¥æ–‡æœ¬ç”¨äºä¸º UNet æ¨¡å‹è¾“å…¥æ–‡æœ¬æ¡ä»¶ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å¯¼ç”Ÿæˆå†…å®¹ã€‚\n\nguidance_scale: è¯¥å‚æ•°å†³å®šç”Ÿæˆå›¾åƒæ—¶åº”ç»™äºˆæ–‡æœ¬æç¤ºçš„æƒé‡ï¼Œå³æ–‡æœ¬å¼•å¯¼çš„å¼ºåº¦\n\néšæ„é€‰æ‹©ä½ å–œæ¬¢çš„ä»»ä½•æ–‡æœ¬æç¤ºï¼è®¾å®šåŸºæœ¬å‚æ•°å¦‚ä¸‹ï¼š\nprompt = [&quot;a photograph of an astronaut riding a horse&quot;]batch_size = len(prompt)          # ç”Ÿæˆçš„æ‰¹é‡å¤§å°height = 512                      # ç›®æ ‡ç”Ÿæˆçš„å›¾åƒçš„é«˜width = 512                       # ç›®æ ‡ç”Ÿæˆçš„å›¾åƒçš„å®½num_inference_steps = 25          # å»å™ªæ€»æ­¥æ•°guidance_scale = 7.5              # classifier-free guidance æ¡ä»¶å¼•å¯¼å¼ºåº¦generator = torch.manual_seed(0)  # ç”¨äºç”Ÿæˆéšæœºå™ªå£°çš„éšæœºæ•°ç§å­\n\nä½¿ç”¨åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°å¹¶ç”ŸæˆåµŒå…¥ï¼š\n# åˆ†è¯text_input = tokenizer(    prompt,                                # æ–‡æœ¬    padding=&quot;max_length&quot;,                  # å¡«å……æ–‡æœ¬ä»¥è¾¾åˆ°æœ€å¤§é•¿åº¦    max_length=tokenizer.model_max_length, # è®¾ç½®äº†æ–‡æœ¬åˆ†è¯çš„æœ€å¤§é•¿åº¦ max_length = 77    truncation=True,                       # å¦‚æœæ–‡æœ¬è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå°†æˆªæ–­å®ƒã€‚    return_tensors=&quot;pt&quot;                    # è¡¨ç¤ºè¿”å› PyTorch å¼ é‡æ ¼å¼çš„ç»“æœã€‚).to(torch_device)# å°†åˆ†è¯ç»“æœè½¬ä¸ºæ–‡æœ¬åµŒå…¥with torch.no_grad():    text_embeddings = text_encoder(text_input.input_ids)[0]\n\néƒ¨åˆ†å˜é‡ç»†èŠ‚ï¼š\ntext_input:&lt;class &#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;&gt;text_input.input_ids:  - type: &lt;class &#x27;torch.Tensor&#x27;&gt;  - shape: torch.Size([1, 77]) [batch, max_length]  - dtype: torch.int64  - device: cuda:0    text_embeddings:  - &lt;class &#x27;torch.Tensor&#x27;&gt;  - torch.Size([1, 77, 768])  - torch.float32  - cuda:0\n\næ¥ä¸‹æ¥è¿˜éœ€è¦ç”Ÿæˆæ— æ¡ä»¶æ–‡æœ¬åµŒå…¥ï¼Œå³å¡«å……æ ‡è®°çš„åµŒå…¥ã€‚è¿™äº›éœ€è¦ä¸æ¡ä»¶ text_embeddings å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼ˆbatch_size å’Œ seq_lengthï¼‰ï¼š\nmax_length = text_input.input_ids.shape[-1] # 77# ç”¨ç©ºæ–‡æœ¬è·å–idsuncond_input = tokenizer(    [&quot;&quot;] * batch_size,     padding=&quot;max_length&quot;,     max_length=max_length,     return_tensors=&quot;pt&quot;)uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\nè®©æˆ‘ä»¬å°†æ¡ä»¶åµŒå…¥å’Œæ— æ¡ä»¶åµŒå…¥è¿æ¥ä¸ºä¸€ä¸ªæ‰¹é‡ï¼Œä»¥é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼š\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])  - &lt;class &#x27;torch.Tensor&#x27;&gt;  - torch.Size([2, 77, 768])  - torch.float32  - cuda:0\n\n\n\nåˆ›å»ºéšæœºå™ªå£°æ¥ä¸‹æ¥ï¼Œç”Ÿæˆä¸€äº›åˆå§‹éšæœºå™ªå£°ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„èµ·ç‚¹ã€‚å®ƒå°†é€æ¸å»å™ªæ¥ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚æ­¤æ—¶ï¼Œæ½œåœ¨è¡¨ç¤ºå°äºæœ€ç»ˆå›¾åƒå°ºå¯¸ï¼ŒVAEè§£ç å™¨åä¼šå°†å…¶è½¬æ¢ä¸ºæœ€ç»ˆçš„ $512 \\times 512$ å›¾åƒã€‚\nprint(vae.config.block_out_channels) # [128, 256, 512, 512]print(2 ** (len(vae.config.block_out_channels) - 1))VAEçš„å±‚æ•°ä¸º4ï¼Œæ¯ä¸¤å±‚ä¹‹é—´ä¸€æ¬¡ä¸‹é‡‡æ ·ï¼Œä¸‹é‡‡æ ·å€ç‡ä¸º8\n\nç”Ÿæˆåˆå§‹å™ªå£°ï¼š\n# shape: [1, 4, 64, 64]latents = torch.randn(    (batch_size, unet.config.in_channels, height // 8, width // 8),     generator=generator                                             ).to(torch_device)\n\n\n\nå¯¹å›¾åƒè¿›è¡Œå»å™ªé¦–å…ˆä½¿ç”¨ sigmaï¼ˆå™ªå£°ç¼©æ”¾å€¼ï¼‰ç¼©æ”¾åˆå§‹å™ªå£°åˆ†å¸ƒï¼Œè¿™æ˜¯ UniPCMultistepScheduler ç­‰æ”¹è¿›é‡‡æ ·å™¨æ‰€éœ€çš„ï¼š\nlatents = latents * scheduler.init_noise_sigma# init_noise_sigma = 1.0\n\næœ€åä¸€æ­¥æ˜¯åˆ›å»ºå»å™ªå¾ªç¯ï¼Œè¯¥å¾ªç¯å°†é€æ­¥å°†çº¯å™ªå£°è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºæ‰€æè¿°çš„å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚å»å™ªå¾ªç¯éœ€è¦åšä¸‰ä»¶äº‹ï¼š\n\nè®¾ç½®å»å™ªæœŸé—´é‡‡æ ·å™¨ä½¿ç”¨çš„æ—¶é—´æ­¥é•¿ã€‚\n\nè¿­ä»£æ—¶é—´æ­¥ã€‚\n\nåœ¨æ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼Œè°ƒç”¨ UNet æ¨¡å‹æ¥æ ¹æ® $x_t$ é¢„æµ‹å™ªå£°æ®‹å·® $\\epsilon_t$ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™é‡‡æ ·å™¨ä»¥è®¡ç®—ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„å™ªå£°æ ·æœ¬ $x_{t-1}$ã€‚\n from tqdm.auto import tqdm\nè®¾å®šé‡‡æ ·å™¨çš„è¿­ä»£æ­¥æ•° scheduler.set_timesteps(num_inference_steps) # 25\nè¿­ä»£æ—¶é—´æ­¥ï¼šscheduler.timesteps- &lt;class â€˜torch.Tensorâ€™&gt;- torch.Size([25])- torch.int64- cpu for t in tqdm(scheduler.timesteps): # å¦‚æœä½¿ç”¨CFGï¼Œä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼Œåˆ™å¯ä»¥åœ¨batchç»´åº¦æ‰©å±•è¾“å…¥ã€‚ latent_model_input &#x3D; torch.cat([latents] * 2) # é¢„æµ‹å™ªå£°å‰ï¼Œæ ¹æ®æ—¶é—´æ­¥tç¼©æ”¾Unetçš„è¾“å…¥x_t latent_model_input &#x3D; scheduler.scale_model_input(latent_model_input, timestep&#x3D;t) # é¢„æµ‹å™ªå£°æ®‹å·® with torch.no_grad():     # unetçš„è¾“å‡ºç±»å‹ï¼š&lt;class â€˜diffusers.models.unets.unet_2d_condition.UNet2DConditionOutputâ€™&gt;     noise_pred &#x3D; unet(latent_model_input, t, encoder_hidden_states&#x3D;text_embeddings)     noise_pred &#x3D; noise_pred.sample     # &lt;class â€˜torch.Tensorâ€™&gt;     # torch.Size([2, 4, 64, 64])     # torch.float32     # cuda: 0 # ä½¿ç”¨CFGè®¡ç®—æ–°çš„å™ªå£°æ®‹å·® noise_pred_uncond, noise_pred_text &#x3D; noise_pred.chunk(2) noise_pred &#x3D; noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # ä½¿ç”¨é‡‡æ ·å™¨è®¡ç®—ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„æ ·æœ¬ï¼šx_t -&gt; x_t-1 # é‡‡æ ·å™¨çš„è¾“å‡ºç±»å‹ï¼š&lt;class â€˜diffusers.schedulers.scheduling_utils.SchedulerOutputâ€™&gt; latents &#x3D; scheduler.step(noise_pred, t, latents) latents &#x3D; latents.prev_sample\n\n\nè§£ç æ½œåœ¨è¡¨ç¤ºä¸ºå›¾åƒæœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ vae å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒï¼Œå¹¶é€šè¿‡æ ·æœ¬è·å–è§£ç è¾“å‡ºï¼š\n# åœ¨å°†æ½œåœ¨è¡¨ç¤ºè¾“å…¥VAEè§£ç å™¨å‰è¿›è¡Œç¼©æ”¾latents = 1 / 0.18215 * latentswith torch.no_grad():    # VAEè§£ç å™¨è¾“å‡ºç±»å‹ï¼š&lt;class &#x27;diffusers.models.autoencoders.vae.DecoderOutput&#x27;&gt;    image = vae.decode(latents)    image = image.sample\n\næœ€åï¼Œå°†å›¾åƒè½¬æ¢ä¸º PIL.Image ä»¥æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼\n# è°ƒæ•´å›¾åƒçš„èŒƒå›´å’Œæ•°æ®ç±»å‹num = len(os.listdir(&#x27;./results&#x27;))image = (image / 2 + 0.5).clamp(0, 1).squeeze()image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()image = Image.fromarray(image)image.save(f&#x27;./results/learn_sp&#123;num&#125;.png&#x27;)\n\n\n\nä»åŸºæœ¬ç®¡é“åˆ°å¤æ‚ç®¡é“ï¼Œæ‚¨å·²ç»çœ‹åˆ°ç¼–å†™è‡ªå·±çš„æ‰©æ•£ç³»ç»ŸçœŸæ­£éœ€è¦çš„åªæ˜¯ä¸€ä¸ªé™å™ªå¾ªç¯ã€‚è¯¥å¾ªç¯åº”è®¾ç½®è°ƒåº¦ç¨‹åºçš„æ—¶é—´æ­¥é•¿ï¼Œå¯¹å…¶è¿›è¡Œè¿­ä»£ï¼Œå¹¶äº¤æ›¿è°ƒç”¨ UNet æ¨¡å‹æ¥é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™è°ƒåº¦ç¨‹åºä»¥è®¡ç®—å…ˆå‰çš„å™ªå£°æ ·æœ¬ã€‚\nè¿™å°±æ˜¯Diffusers çš„è®¾è®¡ç›®çš„ï¼šè®©ä½¿ç”¨æ¨¡å‹å’Œè°ƒåº¦ç¨‹åºç›´è§‚ã€è½»æ¾åœ°ç¼–å†™è‡ªå·±çš„æ‰©æ•£ç³»ç»Ÿã€‚\nNext:\n\nContribute a community pipeline (huggingface.co)\nPipelines (huggingface.co)\nè¿ä½œæœºåˆ¶ï¼šStable Diffusion with ğŸ§¨ Diffusers (huggingface.co)\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"3.è®­ç»ƒdiffusion model","url":"/2024/03/09/3.%20%E8%AE%AD%E7%BB%83Diffusion%20Model/","content":"Train a diffusion modelæ— æ¡ä»¶å›¾åƒç”Ÿæˆæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§æµè¡Œåº”ç”¨ï¼Œå®ƒç”Ÿæˆçš„å›¾åƒä¸ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ä¸­çš„å›¾åƒç›¸ä¼¼ã€‚é€šå¸¸ï¼Œæœ€å¥½çš„ç»“æœæ˜¯é€šè¿‡åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥è·å¾—çš„ã€‚\næ‚¨å¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ°è®¸å¤šè¿™æ ·çš„æ£€æŸ¥ç‚¹ï¼Œä½†å¦‚æœæ‚¨æ‰¾ä¸åˆ°æ‚¨å–œæ¬¢çš„æ£€æŸ¥ç‚¹ï¼Œæ‚¨å¯ä»¥éšæ—¶è®­ç»ƒè‡ªå·±çš„æ£€æŸ¥ç‚¹ï¼\næœ¬æ•™ç¨‹å°†æ•™æ‚¨å¦‚ä½•åœ¨ Smithsonian Butterflies æ•°æ®é›†çš„å­é›†ä¸Šä»å¤´å¼€å§‹è®­ç»ƒ UNet2DModelï¼Œä»¥ç”Ÿæˆæ‚¨è‡ªå·±çš„Butterflies\n\ndiffusers_training_example.ipynb - Colaboratory (google.com)\n\nè®­ç»ƒé…ç½®ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«è®­ç»ƒè¶…å‚æ•°çš„ TrainingConfig ç±»ï¼š\nfrom dataclasses import dataclass@dataclassclass TrainingConfig:    image_size = 128                       # ç”Ÿæˆå›¾åƒçš„åˆ†è¾¨ç‡    train_batch_size = 16                  # è®­ç»ƒbatch    eval_batch_size = 16                   # éªŒè¯é˜¶æ®µçš„å›¾åƒæ•°é‡    num_epochs = 50    gradient_accumulation_steps = 1        #    learning_rate = 1e-4    lr_warmup_steps = 500    save_image_epochs = 10    save_model_epochs = 30    mixed_precision = &quot;fp16&quot;               # `no` for float32, `fp16` for automatic mixed precision    output_dir = &quot;ddpm-butterflies-128&quot;  # the model name locally and on the HF Hub    push_to_hub = False  # whether to upload the saved model to the HF Hub    hub_model_id = &quot;TO-Hitori/my-awesome-model&quot;  # the name of the repository to create on the HF Hub    hub_private_repo = False    overwrite_output_dir = True  # overwrite the old model when re-running the notebook    seed = 0config = TrainingConfig()\n\n\n\nè½½å…¥æ•°æ®é›†æ‚¨å¯ä»¥ä½¿ç”¨datasetsåº“è½»æ¾åŠ è½½Smithsonian Butterfliesæ•°æ®é›†ï¼š\nfrom datasets import load_dataset# æ•°æ®é›†è·¯å¾„ï¼šæœ¬åœ°è·¯å¾„æˆ–hugging-faceè·¯å¾„config.dataset_name = r&quot;D:\\MyData\\smithsonian_butterflies_subset&quot;dataset = load_dataset(config.dataset_name, split=&quot;train&quot;)\n\n\nåœ¨æ­¤æŸ¥æ‰¾å…¶ä»–æ•°æ®é›†ï¼šhuggan (HugGAN Community) (huggingface.co)\n\nDatasets ä½¿ç”¨ Image åŠŸèƒ½è‡ªåŠ¨è§£ç å›¾åƒæ•°æ®å¹¶å°†å…¶åŠ è½½ä¸ºæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–çš„ PIL.Imageï¼š\nimport matplotlib.pyplot as pltfig, axs = plt.subplots(1, 4, figsize=(16, 4))for i, image in enumerate(dataset[:4][&quot;image&quot;]):    axs[i].imshow(image)    axs[i].set_axis_off()fig.show()\n\nè¿™äº›å›¾åƒçš„å°ºå¯¸å„ä¸ç›¸åŒï¼Œå› æ­¤éœ€è¦å…ˆå¯¹å®ƒä»¬è¿›è¡Œé¢„å¤„ç†ï¼š\n\nResize å°†å›¾åƒå¤§å°æ›´æ”¹ä¸º config.image_size ä¸­å®šä¹‰çš„å¤§å°ã€‚ \nRandomHorizontalFlip é€šè¿‡éšæœºé•œåƒç¿»è½¬å›¾åƒæ¥å¢å¼ºæ•°æ®é›†ã€‚ \nNormalizeå¯¹äºå°†åƒç´ å€¼é‡æ–°ç¼©æ”¾åˆ° [-1, 1] èŒƒå›´éå¸¸é‡è¦ï¼Œè¿™æ˜¯æ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥èŒƒå›´ã€‚\n\nfrom torchvision import transformspreprocess = transforms.Compose(    [        transforms.Resize((config.image_size, config.image_size)),        transforms.RandomHorizontalFlip(),        transforms.ToTensor(),        transforms.Normalize([0.5], [0.5]),    ])\n\nä½¿ç”¨Datasetsçš„ set_transform æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€åº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼š\ndef transform(examples):    images = [preprocess(image.convert(&quot;RGB&quot;)) for image in examples[&quot;image&quot;]]    return &#123;&quot;images&quot;: images&#125;dataset.set_transform(transform)\n\nå°†æ•°æ®é›†åŒ…è£…åœ¨torchçš„ DataLoader ä¸­è¿›è¡Œè®­ç»ƒï¼\nimport torchtrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n\n\n\nåˆ›å»º UNet2D æ¨¡å‹ Diffusers ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ä½¿ç”¨æ‚¨æƒ³è¦çš„å‚æ•°è½»æ¾åœ°ä»å…¶æ¨¡å‹ç±»åˆ›å»ºã€‚ä¾‹å¦‚ï¼Œè¦åˆ›å»º UNet2DModelï¼š\n# åˆ›å»ºU-Netfrom diffusers import UNet2DModelmodel = UNet2DModel(    sample_size=config.image_size,  # å›¾åƒåˆ†è¾¨ç‡    in_channels=3,                  # è¾“å…¥å›¾åƒçš„é€šé“æ•°é‡    out_channels=3,                 # è¾“å‡ºå›¾åƒçš„é€šé“æ•°é‡    layers_per_block=2,             # æ¯å±‚ä½¿ç”¨çš„æ®‹å·®å—ä¸ªæ•°    block_out_channels=(128, 128, 256, 256, 512, 512),  # æ¯ä¸€å±‚çš„è¾“å‡ºé€šé“æ•°é‡    down_block_types=(        &quot;DownBlock2D&quot;,              # æ®‹å·®ä¸‹é‡‡æ ·æ¨¡å—        &quot;DownBlock2D&quot;,        &quot;DownBlock2D&quot;,        &quot;DownBlock2D&quot;,        &quot;AttnDownBlock2D&quot;,          # æœ‰spatial self-attentionçš„ä¸‹é‡‡æ ·æ®‹å·®æ¨¡å—        &quot;DownBlock2D&quot;,    ),    up_block_types=(        &quot;UpBlock2D&quot;,                # æ®‹å·®ä¸Šé‡‡æ ·æ¨¡å—        &quot;AttnUpBlock2D&quot;,            # æœ‰spatial self-attentionçš„ä¸Šé‡‡æ ·æ®‹å·®æ¨¡å—        &quot;UpBlock2D&quot;,        &quot;UpBlock2D&quot;,        &quot;UpBlock2D&quot;,        &quot;UpBlock2D&quot;,    ),)print(&quot;UNet2DModel have &#123;&#125; paramerters in total&quot;.format(sum(x.numel() for x in model.parameters())))# UNet2DModel have 113673219 paramerters in total\n\næ£€æŸ¥æ ·æœ¬å›¾åƒå½¢çŠ¶ä¸æ¨¡å‹è¾“å‡ºå½¢çŠ¶æ˜¯å¦åŒ¹é…ï¼š\nsample_image = dataset[0][&quot;images&quot;].unsqueeze(0)print(&quot;Input shape:&quot;, sample_image.shape)print(&quot;Output shape:&quot;, model(sample_image, timestep=0).sample.shape)&#x27;&#x27;&#x27;Input shape: torch.Size([1, 3, 128, 128])Output shape: torch.Size([1, 3, 128, 128])&#x27;&#x27;&#x27;\n\n\n\nåˆ›å»ºé‡‡æ ·å™¨æ ¹æ®æ‚¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œè®­ç»ƒè¿˜æ˜¯æ¨ç†ï¼Œé‡‡æ ·å™¨çš„è¡Œä¸ºä¼šæœ‰æ‰€ä¸åŒã€‚\n\nåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡æ ·å™¨æ ¹æ®å™ªå£°ç”Ÿæˆå›¾åƒã€‚\nåœ¨è®­ç»ƒæœŸé—´ï¼Œé‡‡æ ·å™¨ä»æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç‰¹å®šç‚¹è·å–æ¨¡å‹è¾“å‡ºï¼ˆæˆ–æ ·æœ¬ï¼‰ï¼Œå¹¶æ ¹æ®å™ªå£°è°ƒåº¦noise scheduleå’Œæ›´æ–°è§„åˆ™update ruleå°†å™ªå£°æ³¨å…¥å›¾åƒã€‚\n\nè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ DDPMScheduler å¹¶ä½¿ç”¨ add_noise æ–¹æ³•å‘sample_image æ·»åŠ ä¸€äº›éšæœºå™ªå£°ï¼š\nimport torchfrom PIL import Imagefrom diffusers import DDPMSchedulerfrom torchvision.utils import save_image, make_grid# ç”¨æ€»æ­¥æ•°æ¥åˆå§‹åŒ–é‡‡æ ·å™¨noise_scheduler = DDPMScheduler(num_train_timesteps=1000)# è®¾å®šåŠ å™ªåºåˆ—ï¼Œè¿™é‡Œé€‰æ‹©äº†7ä¸ªä¾æ¬¡å¢å¤§çš„æ—¶é—´æ­¥timesteps = torch.LongTensor([50, 150, 250, 450, 650, 850, 990])# æ—¶é—´æ­¥çš„æ•°é‡ä¸ºbatchï¼Œé‡‡æ ·å›¾ç‰‡çš„å½¢çŠ¶åä¸‰ä¸ªç»´åº¦ï¼Œæ„å»ºé‡‡æ ·å™ªå£°noise = torch.randn(timesteps.shape + sample_image.shape[1:])# åˆ©ç”¨é‡‡æ ·å™¨çš„add_noiseæ–¹æ³•å°†å™ªå£°æ³¨å…¥é‡‡æ ·å›¾ç‰‡noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)# ä¿å­˜å›¾ç‰‡è§‚æµ‹é‡‡æ ·å™¨çš„åŠ å™ªæ•ˆæœsave_to_show = make_grid(torch.cat([sample_image, noisy_image], dim=0))save_image(save_to_show, &#x27;./test_scheduler.png&#x27;)\n\næ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯é¢„æµ‹æ·»åŠ åˆ°å›¾åƒä¸­çš„å™ªå£°ã€‚è¿™ä¸€æ­¥çš„æŸå¤±å¯ä»¥é€šè¿‡ä¸‹å¼è®¡ç®—ï¼š\n# æŸå¤±å‡½æ•°ï¼šæœ€ç®€å•çš„MSEæŸå¤±å‡½æ•°import torch.nn.functional as Fprint(&#x27;test loss func&#x27;)noise_pred = model(noisy_image, timesteps).sampleloss = F.mse_loss(noise_pred, noise)print(&#x27;loss value = &#x27;, loss.item())\n\n\n\nè®­ç»ƒæ¨¡å‹åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»æŒæ¡äº†å¼€å§‹è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†å†…å®¹ï¼Œå‰©ä¸‹çš„å°±æ˜¯å°†æ‰€æœ‰å†…å®¹ç»„åˆåœ¨ä¸€èµ·ã€‚ é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨å’Œä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š\n# å¼€å§‹è®­ç»ƒï¼šè®¾ç½®è°ƒåº¦å™¨from diffusers.optimization import get_cosine_schedule_with_warmupoptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)lr_scheduler = get_cosine_schedule_with_warmup(    optimizer=optimizer,    num_warmup_steps=config.lr_warmup_steps,    num_training_steps=(len(train_dataloader) * config.num_epochs),)\n\nç„¶åï¼Œæ‚¨éœ€è¦ä¸€ç§è¯„ä¼°æ¨¡å‹çš„æ–¹æ³•ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ DDPMPipeline ç”Ÿæˆä¸€æ‰¹æ ·æœ¬å›¾åƒå¹¶å°†å…¶ä¿å­˜ä¸ºç½‘æ ¼ï¼š\nfrom diffusers import DDPMPipelinefrom diffusers.utils import make_image_gridimport osdef evaluate(config, epoch, pipeline):    # Sample some images from random noise (this is the backward diffusion process).    # The default pipeline output type is `List[PIL.Image]`    # ä½¿ç”¨pipelineç”Ÿæˆå›¾åƒ    images = pipeline(        batch_size=config.eval_batch_size,        generator=torch.manual_seed(config.seed),    ).images    # å°†å›¾åƒæ‹¼æ¥ä¸ºç½‘æ ¼    image_grid = make_image_grid(images, rows=4, cols=4)    # ä¿å­˜éªŒè¯å›¾åƒ    test_dir = os.path.join(config.output_dir, &quot;samples&quot;)    os.makedirs(test_dir, exist_ok=True)    image_grid.save(f&quot;&#123;test_dir&#125;/&#123;epoch:04d&#125;.png&quot;)\n\nç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Accelerate å°†æ‰€æœ‰è¿™äº›ç»„ä»¶åŒ…è£…åœ¨ä¸€ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œä»¥è½»æ¾è¿›è¡Œï¼š\n\nTensorBoard æ—¥å¿—è®°å½•\næ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚\nè¦å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¯·ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è·å–å­˜å‚¨åº“åç§°å’Œä¿¡æ¯ï¼Œç„¶åå°†å…¶æ¨é€åˆ° Hubã€‚\n\næ¥ä¸‹æ¥æ˜¯è®­ç»ƒæ ¸å¿ƒéƒ¨åˆ†ï¼š\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):    # åˆå§‹åŒ– accelerator å’Œ tensorboard æ—¥å¿—è®°å½•    accelerator = Accelerator(        mixed_precision=config.mixed_precision,                          # æ˜¯å¦æ··åˆç²¾åº¦è®­ç»ƒ        gradient_accumulation_steps=config.gradient_accumulation_steps,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°        log_with=&quot;tensorboard&quot;,                                          # ä½¿ç”¨tensorboardè®°å½•æ—¥å¿—        project_dir=os.path.join(config.output_dir, &quot;logs&quot;),             # æ—¥å¿—è·¯å¾„    )    # å¦‚æœåœ¨ä¸»è¿›ç¨‹    if accelerator.is_main_process:        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹        if config.output_dir is not None:            os.makedirs(config.output_dir, exist_ok=True)        # ä¸Šä¼ åˆ°HFçš„è®¾ç½®        if config.push_to_hub:            repo_id = create_repo(                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True            ).repo_id        # åˆå§‹åŒ–è¿½è¸ªå™¨        accelerator.init_trackers(&quot;train_example&quot;)    # ç”¨accelerateåŒ…è£…ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®åŠ è½½å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨    # ä¿è¯è¾“å…¥å’Œè¾“å‡ºçš„é¡ºåºä¸€è‡´å³å¯    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(        model, optimizer, train_dataloader, lr_scheduler    )    # åˆå§‹åŒ–å…¨å±€æ­¥æ•°    global_step = 0    # å¼€å§‹è®­ç»ƒæ¨¡å‹    for epoch in range(config.num_epochs):        # åˆ›å»ºè¿›åº¦æ¡        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)        # è®¾ç½®è¿›åº¦æ¡æè¿°        progress_bar.set_description(f&quot;Epoch &#123;epoch&#125;&quot;)        # å¯¹æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå¾ªç¯        for step, batch in enumerate(train_dataloader):            # ä»æ•°æ®é›†è·å–å›¾åƒ            clean_images = batch[&quot;images&quot;]            # ç”Ÿæˆå³å°†åŠ å…¥å›¾åƒçš„å™ªå£°            noise = torch.randn(clean_images.shape, device=clean_images.device)            # è·å–å½“å‰batch-size            bs = clean_images.shape[0]            # ä¸ºå½“å‰batchä¸­æ¯ä¸ªå›¾åƒéšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥            timesteps = torch.randint(                0,                                          # èµ·ç‚¹                noise_scheduler.config.num_train_timesteps, # ç»ˆç‚¹                (bs,),                                      # å½¢çŠ¶/æ•°é‡                device=clean_images.device,                dtype=torch.int64            )            # ä½¿ç”¨é‡‡æ ·å™¨ï¼Œæ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥çš„å™ªå£°å¹…åº¦å‘å¹²å‡€çš„å›¾åƒæ·»åŠ å™ªå£°            # è¿™æ˜¯å‰å‘æ‰©æ•£è¿‡ç¨‹            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)            # ä½¿ç”¨acceleratorç´¯ç§¯æ¨¡å‹æ¢¯åº¦            with accelerator.accumulate(model):                # é¢„æµ‹å™ªå£°æ®‹å·®                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]                # è®¡ç®—æŸå¤±å‡½æ•°                loss = F.mse_loss(noise_pred, noise)                # åå‘ä¼ æ’­æ¢¯åº¦                accelerator.backward(loss)                # æ¢¯åº¦è£å‰ª                accelerator.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()      # è¿­ä»£ä¼˜åŒ–å™¨                lr_scheduler.step()   # è¿­ä»£å­¦ä¹ ç‡è°ƒåº¦å™¨                optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦            # æ›´æ–°åŸºç£å¾’            progress_bar.update(1)            # è®°å½•æ—¥å¿—ï¼šæŸå¤±å‡½æ•°ï¼Œå­¦ä¹ ç‡å˜åŒ–            logs = &#123;&quot;loss&quot;: loss.detach().item(), &quot;lr&quot;: lr_scheduler.get_last_lr()[0], &quot;step&quot;: global_step&#125;            progress_bar.set_postfix(**logs)            accelerator.log(logs, step=global_step)            # å…¨å±€æ­¥æ•°å¢åŠ             global_step += 1        # åœ¨æ¯ä¸ªepochåï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨evaluate()é‡‡æ ·ä¸€äº›æ¼”ç¤ºå›¾åƒå¹¶ä¿å­˜æ¨¡å‹        if accelerator.is_main_process:            # åˆå§‹åŒ–ä¸€ä¸ªpipelineï¼Œä¼ å…¥å½“å‰è®­ç»ƒçš„æ¨¡å‹å’Œè°ƒåº¦å™¨            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)            # æ ¹æ®éªŒè¯é¢‘ç‡ä¿å­˜ç ”ç©¶ç»“æœ            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:                evaluate(config, epoch, pipeline)            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:                if config.push_to_hub:                    upload_folder(                        repo_id=repo_id,                        folder_path=config.output_dir,                        commit_message=f&quot;Epoch &#123;epoch&#125;&quot;,                        ignore_patterns=[&quot;step_*&quot;, &quot;epoch_*&quot;],                    )                else:                    pipeline.save_pretrained(config.output_dir)train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextdiffusers_training_example.ipynb - Colaboratory (google.com)\nTextual Inversion (huggingface.co)\nDreamBooth (huggingface.co)\nText-to-image (huggingface.co)\nLoRA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"condaå¸¸ç”¨å‘½ä»¤","url":"/2021/11/25/conda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/","content":"condaå¸¸ç”¨å‘½ä»¤åœ¨windowsçš„cmdä¸‹ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤è¿›å…¥condaï¼š\nactivate\n\nç¯å¢ƒç®¡ç†åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼šconda create -n [env_name] python=[X.X]\n\n\nenv_nameï¼šè¦åˆ›å»ºçš„ç¯å¢ƒçš„åå­—\nX.Xï¼šè¦åˆ›å»ºçš„ç¯å¢ƒçš„pythonçš„ç‰ˆæœ¬ï¼Œå¦‚3.7\n\næ¿€æ´»è™šæ‹Ÿç¯å¢ƒconda activate [env_name]\n\nåœç”¨å½“å‰ç¯å¢ƒconda deactivate\n\næŸ¥çœ‹å½“å‰ç¯å¢ƒçš„pythonç‰ˆæœ¬python --version\n\næŸ¥çœ‹æ‰€æœ‰å­˜åœ¨çš„è™šæ‹Ÿç¯å¢ƒconda info -econda env list\n\nåˆ é™¤è™šæ‹Ÿç¯å¢ƒï¼šconda remove -n [env_name] --all\n\né‡å‘½åç¯å¢ƒ\ncondaæ²¡æœ‰ç›´æ¥é‡å‘½åç¯å¢ƒçš„åŠŸèƒ½ï¼Œä½†å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤å®Œæˆï¼š\nå…‹éš†è¦é‡å‘½åçš„ç¯å¢ƒ\nå°†åŸç¯å¢ƒåˆ é™¤\n\n\n\nconda create --name [newname] --clone [oldname]conda remove --name [oldname] --all\n\n\n\nåŒ…ç®¡ç†å®‰è£…åŒ…conda install [pac_name]=[åŒ…çš„ç‰ˆæœ¬å·]\n\næŸ¥çœ‹å·²ç»å®‰è£…çš„åŒ…\næŸ¥çœ‹å½“å‰ç¯å¢ƒï¼š\n\nconda list\n\n\næŸ¥çœ‹æŒ‡å®šç¯å¢ƒï¼š\n\nconda list -n [env_name]\n\nåˆ é™¤åŒ…conda uninstall [pac_name]\n\næ›´æ–°æŒ‡å®šåŒ…conda update [pac_name]\n\næ¸…ç†åŒ…\né€šè¿‡ä»¥ä¸‹æŒ‡ä»¤æ¥åˆ é™¤ä¸€äº›æ²¡ç”¨çš„åŒ…ï¼Œè¿™ä¸ªå‘½ä»¤ä¼šæ£€æŸ¥å“ªäº›åŒ…æ²¡æœ‰åœ¨åŒ…ç¼“å­˜ä¸­è¢«ç¡¬ä¾èµ–åˆ°å…¶ä»–åœ°æ–¹ï¼Œå¹¶åˆ é™¤å®ƒä»¬\n\nconda clean -p\n\n\nåˆ é™¤condaä¿å­˜ä¸‹æ¥çš„taråŒ…\n\nconda clean -t\n\n\nåˆ é™¤æ‰€æœ‰çš„å®‰è£…åŒ…åŠcache\n\nconda clean -y --all\n\næ›´æ–°condaconda update conda\n\nå®‰è£…requirements.txtæ–‡ä»¶å†…çš„åŒ…\né¦–å…ˆé€šè¿‡cdæŒ‡ä»¤è¿›å…¥requirements.txtæ–‡ä»¶æ‰€åœ¨è·¯å¾„ï¼Œç„¶åæ‰§è¡Œå¦‚ä¸‹æŒ‡ä»¤å³å¯\n\npip install -r requirements.txt\n\nåŒ…çš„æ•°æ®æºç®¡ç†\næ˜¾ç¤ºç›®å‰condaçš„æ•°æ®æºæœ‰å“ªäº›ï¼š\n\nconda config --show channels\n\n\næ·»åŠ æ•°æ®æºï¼š(æ¸…åæº)\n\nconda config --add https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\nåˆ é™¤æ•°æ®æº\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n","categories":["Tech"],"tags":["python"]},{"title":"YAML","url":"/2024/03/14/YAML%E5%9F%BA%E7%A1%80/","content":"YAMLYAML Ainâ€™t a Markup Language\nYet Another Markup Language\nYAMLæ˜¯â€YAML Ainâ€™t a Markup Languageâ€ï¼ˆYAMLä¸æ˜¯ä¸€ç§æ ‡è®°è¯­è¨€ï¼‰çš„é€’å½’ç¼©å†™ã€‚åœ¨å¼€å‘çš„è¿™ç§è¯­è¨€æ—¶ï¼ŒYAMLçš„æ„æ€å…¶å®æ˜¯ï¼šâ€Yet Another Markup Languageâ€ï¼ˆä»æ˜¯ä¸€ç§æ ‡è®°è¯­è¨€ï¼‰ï¼Œä½†ä¸ºäº†å¼ºè°ƒè¿™ç§è¯­è¨€ä»¥æ•°æ®ä¸ºä¸­å¿ƒï¼Œè€Œä¸æ˜¯ä»¥æ ‡è®°è¯­è¨€ä¸ºé‡ç‚¹ï¼Œè€Œç”¨åå‘ç¼©ç•¥è¯­é‡å‘½åã€‚YAML (wikipedia.org)\nYAMLç‰¹ç‚¹æ˜¯ä½¿ç”¨ç©ºæ ¼æ¥è¡¨è¾¾å±‚æ¬¡ç»“æ„ï¼Œç‰¹åˆ«é€‚åˆç”¨æ¥è¡¨è¾¾æˆ–ç¼–è¾‘æ•°æ®ç»“æ„ã€å„ç§é…ç½®æ–‡ä»¶ï¼Œå…¶æ–‡ä»¶ä¸€èˆ¬ä»¥ .yaml ä¸ºåç¼€ã€‚\nåŸºæœ¬è¯­æ³•\nä»¥ k: v çš„å½¢å¼æ¥è¡¨ç¤ºé”®å€¼å¯¹çš„å…³ç³»\n\nå†’å·åé¢å¿…é¡»æœ‰ä¸€ä¸ªç©ºæ ¼\n\n\nåªæ”¯æŒå•è¡Œæ³¨é‡Šï¼Œæ³¨é‡Šç¬¦å·ï¼š# \n\nå¤§å°å†™æ•æ„Ÿ\n\né€šè¿‡ç¼©è¿›æ¥è¡¨ç¤ºå±‚çº§å…³ç³»\n\nç¼©æ’ä¸­ç©ºæ ¼çš„æ•°ç›®ä¸é‡è¦ï¼Œåªè¦ç›¸åŒé˜¶å±‚çš„å…ƒç´ å·¦ä¾§å¯¹é½å°±å¯ä»¥äº†\nç¼©è¿›åªèƒ½ä½¿ç”¨ç©ºæ ¼ï¼Œä¸èƒ½ä½¿ç”¨ tab ç¼©è¿›\n\n\nå­—ç¬¦ä¸²å¯ä»¥ä¸ç”¨åŒå¼•å·\n\nä¸€ä¸ªæ–‡ä»¶ä¸­å¯ä»¥åŒ…å«å¤šä¸ªæ–‡ä»¶çš„å†…å®¹\n\nç”¨--- å³ä¸‰ä¸ªç ´æŠ˜å·è¡¨ç¤ºä¸€ä»½å†…å®¹çš„å¼€å§‹\nç”¨...å³ä¸‰ä¸ªå°æ•°ç‚¹è¡¨ç¤ºä¸€ä»½å†…å®¹çš„ç»“æŸï¼ˆéå¿…éœ€ï¼‰\n\n\n\næ•°æ®ç»“æ„ä¸ç±»å‹å¯¹è±¡ä»¥é”®å€¼å¯¹ key: value å½¢å¼ç»„ç»‡æ•°æ®\n1. ä½¿ç”¨**å†’å·+ç©ºæ ¼**æ¥åˆ†å¼€é”®ä¸å€¼\n1. æ”¯æŒå¤šå±‚åµŒå¥—ï¼ˆ**ç”¨ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»**ï¼‰\n\nmodel:  base_learning_rate: 4.5e-6  target: ldm.models.autoencoder.AutoencoderKL  params:    monitor: &quot;val/rec_loss&quot;    embed_dim: 64    lossconfig:      target: ldm.modules.losses.LPIPSWithDiscriminator      params:        disc_start: 50001        kl_weight: 0.000001        disc_weight: 0.5\n\n\næ”¯æŒæµå¼é£æ ¼ï¼ˆFlow styleï¼‰çš„è¯­æ³•ï¼šç”¨èŠ±æ‹¬å·åŒ…è£¹ï¼Œç”¨é€—å·åŠ ç©ºæ ¼åˆ†éš”\n\nkey: &#123;child-key1: value1, child-key2: value2 &#125;\n\n\n\næ•°ç»„\nä¸€ç»„ä»¥åŒºå—æ ¼å¼ï¼ˆâ€œç ´æŠ˜å·+ç©ºæ ¼â€ï¼‰å¼€å¤´çš„æ•°æ®ç»„æˆä¸€ä¸ªæ•°ç»„\n\nunet_config:  target: ldm.modules.diffusionmodules.openaimodel.UNetModel  params:    image_size: 64    in_channels: 3    out_channels: 3    model_channels: 224    attention_resolutions:    - 8    - 4    - 2    num_res_blocks: 2    channel_mult:    - 1    - 2    - 3    - 4    num_head_channels: 32\n\n\nä¹Ÿæ”¯æŒå†…è”æ ¼å¼æ¥è¡¨è¾¾ï¼ˆç”¨æ–¹æ‹¬å·åŒ…è£¹ï¼Œé€—å·åŠ ç©ºæ ¼åˆ†éš”ï¼‰\n\nddconfig:  double_z: True  z_channels: 64  resolution: 256  in_channels: 3  out_ch: 3  ch: 128  ch_mult: [1, 1, 2, 2, 4, 4]    num_res_blocks: 2  attn_resolutions: [16, 8]  dropout: 0.0\n\n\næ”¯æŒå¤šç»´æ•°ç»„ï¼ˆç”¨ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»ï¼‰\n\nvalues:  - - 1    - 2  - - 3    - 4# ç­‰ä»·ï¼š    values: [[1, 2], [3, 4]]\n\n\n\nå­—ç¬¦ä¸²\nå­—ç¬¦ä¸²ä¸€èˆ¬ä¸éœ€è¦ç”¨å¼•å·åŒ…è£¹\nå­—ç¬¦ä¸²æ¢è¡Œè§†ä¸ºä¸€ä¸ªç©ºæ ¼\nå•å¼•å·å¯ä»¥å±è”½è½¬ä¹‰\nå­—ç¬¦ä¸²ä¸­éœ€è¦ä½¿ç”¨è½¬ä¹‰å­—ç¬¦\\å°±å¿…é¡»ä½¿ç”¨åŒå¼•å·åŒ…è£¹\n\nstrings:  - Hello world # ä¸ç”¨å¼•å·åŒ…è£¹  - Hello     world # æ¢è¡Œè§†ä¸ºä¸€ä¸ªç©ºæ ¼  - &#x27;å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º&#x27;  # å•å¼•å·å¯ä»¥å±è”½è½¬ä¹‰  - &quot;å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º&quot;  # åŒå¼•å·ä½¿ç”¨è½¬ç§»ç¬¦å·# ç»“æœï¼š- Hello world- Hello world- å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º- &#x27;å­—ç¬¦ä¸²  æ¢è¡Œ  æ¼”ç¤º&#x27;\n\n\nä¿ç•™æ¢è¡Œï¼šä½¿ç”¨ç«–çº¿ç¬¦â€œ | â€æ¥è¡¨ç¤ºè¯¥è¯­æ³•ï¼Œæ¯è¡Œçš„ç¼©è¿›å’Œè¡Œå°¾ç©ºç™½éƒ½ä¼šè¢«å»æ‰ï¼Œè€Œé¢å¤–çš„ç¼©è¿›ä¼šè¢«ä¿ç•™\n\nlines: |  æˆ‘æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ    æˆ‘æ˜¯å´å½¦ç¥–      æˆ‘æ˜¯ç¬¬å››è¡Œ  æˆ‘æ˜¯ç¬¬äº”è¡Œ  # ç»“æœ&quot;æˆ‘æ˜¯ç¬¬ä¸€è¡Œ\\næˆ‘æ˜¯ç¬¬äºŒè¡Œ\\n  æˆ‘æ˜¯å´å½¦ç¥–\\n    æˆ‘æ˜¯ç¬¬å››è¡Œ\\næˆ‘æ˜¯ç¬¬äº”è¡Œ\\n&quot;\n\n\næŠ˜å æ¢è¡Œï¼šä½¿ç”¨å³å°–æ‹¬å·â€œ &gt; â€æ¥è¡¨ç¤ºè¯¥è¯­æ³•ï¼Œåªæœ‰ç©ºç™½è¡Œæ‰ä¼šè¢«è¯†åˆ«ä¸ºæ¢è¡Œï¼ŒåŸæ¥çš„æ¢è¡Œç¬¦éƒ½ä¼šè¢«è½¬æ¢æˆç©ºæ ¼\n\nlines: &gt;  æˆ‘æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä»æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä¾æ—§æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ  è¿™ä¹ˆå·§æˆ‘ä¹Ÿæ˜¯ç¬¬äºŒè¡Œ# ç»“æœlines2: &#x27;æˆ‘æ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä»æ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä¾æ—§æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ è¿™ä¹ˆå·§æˆ‘ä¹Ÿæ˜¯ç¬¬äºŒè¡Œ  &#x27;\n\n\n\nå¸ƒå°”å€¼\nâ€œtrueâ€ã€â€œTrueâ€ã€â€œTRUEâ€ã€â€œyesâ€ã€â€œYesâ€å’Œâ€œYESâ€çš†ä¸ºçœŸ\nâ€œfalseâ€ã€â€œFalseâ€ã€â€œFALSEâ€ã€â€œnoâ€ã€â€œNoâ€å’Œâ€œNOâ€çš†ä¸ºå‡\n\næ•´æ•°\næ”¯æŒäºŒè¿›åˆ¶è¡¨ç¤º\n\nint:  - 666  - 0001_0000# ç»“æœint:- 666- 4096\n\n\n\næµ®ç‚¹æ•°\næ”¯æŒç§‘å­¦è®¡æ•°æ³•\n\nfloat:  - 3.14  - 6.8523015e+5 # ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•# ç»“æœfloat:- 3.14- 685230.15\n\nç©º Nullnullã€Nullã€~ å’Œä¸æŒ‡å®šå€¼éƒ½è¡¨ç¤ºç©º\nnulls:  - null  - Null  - ~  -# ç»“æœnulls:- null- null- null- null\n\n\n\nå¼ºåˆ¶ç±»å‹è½¬æ¢åŒæ„Ÿå¹å·+ç›®æ ‡ç±»å‹æ¥å¼ºåˆ¶è½¬æ¢ç±»å‹\na: !!float &#x27;666&#x27; # !! ä¸ºä¸¥æ ¼ç±»å‹æ ‡ç­¾b: !!int &#x27;666&#x27;   # å­—ç¬¦ä¸²è½¬ä¸ºæ•´å‹c: !!str 666     # æ•´æ•°è½¬ä¸ºå­—ç¬¦ä¸²d: !!str 666.66  # æµ®ç‚¹æ•°è½¬ä¸ºå­—ç¬¦ä¸²e: !!str true    # å¸ƒå°”å€¼è½¬ä¸ºå­—ç¬¦ä¸²f: !!bool &#x27;yes&#x27;  # å­—ç¬¦ä¸²è½¬ä¸ºå¸ƒå°”å€¼# ç»“æœa: 666.0b: 666c: &#x27;666&#x27;d: &#x27;666.66&#x27;e: &#x27;true&#x27;f: true\n\n\n\næ•°æ®å¤ç”¨ä¸åˆå¹¶æ•°æ®å¤ç”¨åœ¨keyçš„å†’å·åï¼Œä½¿ç”¨é”šç‚¹ç¬¦å·&amp;è®¾å®šé”šç‚¹ï¼Œä½¿ç”¨å¼•ç”¨ç¬¦å·*å¼•ç”¨é”šç‚¹\nmodel: &amp;all_parm  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params: &amp;model_parm    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model: *all_parmnew_params: *model_parm# ç»“æœnew_model:  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_params:  linear_start: 0.0015  linear_end: 0.0195  num_timesteps_cond: 1  log_every_t: 200  timesteps: 1000  first_stage_key: image  image_size: 64  channels: 3  monitor: val/loss_simple_ema\n\n\n\næ•°æ®åˆå¹¶åˆå¹¶æ ‡ç­¾ç¬¦å·â€œ&lt;&lt;â€é…åˆé”šç‚¹ç¬¦å·å’Œå¼•ç”¨ç¬¦å·ä½¿ç”¨å¯ä»¥ä¸ä»»æ„æ•°æ®è¿›è¡Œåˆå¹¶ï¼Œå¯ä»¥è§†ä¸ºé¢å‘å¯¹è±¡ä¸­çš„ç»§æ‰¿\nmodel_location: &amp;loc  target: ldm.models.diffusion.ddpm.LatentDiffusionmodel_params: &amp;params  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model:  base_learning_rate: 2.0e-06  &lt;&lt;: *loc  &lt;&lt;: *params  # ç»“æœnew_model:  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_ema  base_learning_rate: 2.0e-06\n\n\n\nå‚è€ƒä¸€æ–‡çœ‹æ‡‚ YAML - çŸ¥ä¹ (zhihu.com)\n","categories":["Tech"],"tags":["python"]},{"title":"Hello World","url":"/2024/02/02/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"AutoPipelineåŸºæœ¬æ•™ç¨‹","url":"/2024/03/09/2.%20Auto%20Pipeline/","content":"AutoPipelineåŸºæœ¬æ•™ç¨‹AutoPipeline çš„è®¾è®¡ç›®çš„æ˜¯ï¼š \n\nè½»æ¾åŠ è½½ checkpointï¼Œè€Œæ— éœ€çŸ¥é“è¦ä½¿ç”¨çš„ç‰¹å®špipelineçš„ç±»å‹ \nåœ¨å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨å¤šä¸ªpipeline\n\nAutoPipeline ç±»æ—¨åœ¨ç®€åŒ– Diffusers ä¸­çš„å„ç§Pipelineã€‚å®ƒæ˜¯ä¸€ä¸ªé€šç”¨çš„ã€ä»»åŠ¡ä¼˜å…ˆçš„ç®¡é“ã€‚ AutoPipeline ä¼šè‡ªåŠ¨æ£€æµ‹è¦ä½¿ç”¨çš„æ­£ç¡® Pipeline ç±»ï¼Œè¿™ä½¿å¾—åœ¨ä¸çŸ¥é“ç‰¹å®šPipelineç±»åç§°çš„æƒ…å†µä¸‹è½»æ¾åŠ è½½ä»»åŠ¡çš„æ£€æŸ¥ç‚¹\n\nAPI: AutoPipeline (huggingface.co)\n\nDiffusers èƒ½å¤Ÿå®Œæˆè®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ‚¨é€šå¸¸å¯ä»¥å°†ç›¸åŒçš„é¢„è®­ç»ƒæƒé‡é‡å¤ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®å¤ã€‚å¦‚æœæ‚¨å¯¹åº“å’Œæ‰©æ•£æ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œå¯èƒ½å¾ˆéš¾çŸ¥é“è¦ä½¿ç”¨å“ªä¸ªç®¡é“æ¥å®Œæˆä»»åŠ¡ã€‚\nä¸ºæ‚¨çš„ä»»åŠ¡é€‰æ‹© AutoPipelineé¦–å…ˆé€‰æ‹©ä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæƒ³è¦ä½¿ç”¨ runwayml&#x2F;stable-diffusion-v1-5 æ£€æŸ¥ç‚¹æ¥è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ä»»åŠ¡ï¼Œè¯·ä½¿ç”¨ AutoPipelineForText2Imageï¼š\nfrom diffusers import AutoPipelineForText2Imageimport torch, ospipeline = AutoPipelineForText2Image.from_pretrained(    &quot;sd-v1.5&quot;,                 # æƒé‡è·¯å¾„    torch_dtype=torch.float16, # æ•°æ®ç±»å‹    use_safetensors=True,      # ä½¿ç”¨safetensorç±»å‹çš„æƒé‡    variant=&#x27;fp16&#x27;,            # åŠ è½½æƒé‡æ—¶é€‰æ‹©æ–‡ä»¶åä¸­å¸¦æœ‰â€˜fp16â€™çš„).to(&quot;cuda&quot;)prompt = &quot;peasant and dragon combat, wood cutting style, viking era, bevel with rune&quot;image = pipeline(prompt, num_inference_steps=25).images[0]num = len(os.listdir(&#x27;./results&#x27;))image.save(f&#x27;./results/Auto_tur_&#123;num&#125;.png&#x27;)\n\næ·±å…¥å±‚æ¬¡æ¢è®¨ AutoPipelineForText2Image \n\nAutoPipelineå°†è‡ªåŠ¨ä» model_index.json æ–‡ä»¶ä¸­æ£€æµ‹ StableDiffusionPipeline ç±»\næ ¹æ®ç±»ååŠ è½½ç›¸åº”çš„æ–‡æœ¬åˆ°å›¾åƒçš„StableDiffusionPipeline\n\nmodel_index.jsonæ–‡ä»¶å†…å®¹ï¼š\n&#123;  &quot;_class_name&quot;: &quot;StableDiffusionPipeline&quot;,  &quot;_diffusers_version&quot;: &quot;0.6.0&quot;,  &quot;feature_extractor&quot;: [    &quot;transformers&quot;,    &quot;CLIPImageProcessor&quot;  ],  &quot;safety_checker&quot;: [    &quot;stable_diffusion&quot;,    &quot;StableDiffusionSafetyChecker&quot;  ],  &quot;scheduler&quot;: [    &quot;diffusers&quot;,    &quot;PNDMScheduler&quot;  ],  &quot;text_encoder&quot;: [    &quot;transformers&quot;,    &quot;CLIPTextModel&quot;  ],  &quot;tokenizer&quot;: [    &quot;transformers&quot;,    &quot;CLIPTokenizer&quot;  ],  &quot;unet&quot;: [    &quot;diffusers&quot;,    &quot;UNet2DConditionModel&quot;  ],  &quot;vae&quot;: [    &quot;diffusers&quot;,    &quot;AutoencoderKL&quot;  ]&#125;\n\n\n\nåŒæ ·ï¼Œå¯¹äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡çš„AutoPipelineï¼ŒAutoPipelineForImage2Image ä» model_index.json æ–‡ä»¶ä¸­æ£€æµ‹åˆ° â€œStableDiffusionâ€ æ£€æŸ¥ç‚¹ï¼Œå¹¶å°†åœ¨å¹•ååŠ è½½ç›¸åº”çš„ StableDiffusionImg2ImgPipelineã€‚\nè¿˜å¯ä»¥ä¼ é€’ç‰¹å®šäº Pipeline ç±»çš„ä»»ä½•å…¶ä»–å‚æ•°ï¼Œå¦‚guidance_scaleã€strength\nimport osos.environ[&#x27;HTTP_PROXY&#x27;] = &#x27;http://127.0.0.1:33210&#x27;os.environ[&#x27;HTTPS_PROXY&#x27;] = &#x27;http://127.0.0.1:33210&#x27;from diffusers import AutoPipelineForImage2Imageimport torchimport requestsfrom PIL import Imagefrom io import BytesIOpipeline = AutoPipelineForImage2Image.from_pretrained(    &quot;sd-v1.5&quot;,    torch_dtype=torch.float16,    use_safetensors=True,    variant=&#x27;fp16&#x27;,).to(&quot;cuda&quot;)prompt = &quot;a portrait of a dog wearing a pearl earring&quot;url = &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg&quot;response = requests.get(url)image = Image.open(BytesIO(response.content)).convert(&quot;RGB&quot;)image.thumbnail((768, 768))image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]num = len(os.listdir(&#x27;./results&#x27;))image.save(f&#x27;./results/Auto_tur2_&#123;num&#125;.png&#x27;)\n\n\n\nå¦‚æœæ‚¨æƒ³è¿›è¡Œå›¾åƒä¿®å¤ï¼Œåˆ™ AutoPipelineForInpainting ä»¥ç›¸åŒçš„æ–¹å¼åŠ è½½åº•å±‚çš„ StableDiffusionInpaintPipeline ç±»ï¼š\nimport osos.environ[&#x27;HTTP_PROXY&#x27;] = &#x27;http://127.0.0.1:33210&#x27;os.environ[&#x27;HTTPS_PROXY&#x27;] = &#x27;http://127.0.0.1:33210&#x27;from diffusers import AutoPipelineForInpaintingfrom diffusers.utils import load_imageimport torchpipeline = AutoPipelineForInpainting.from_pretrained(    r&quot;D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0&quot;,     torch_dtype=torch.float16,     use_safetensors=True,     variant=&#x27;fp16&#x27;,).to(&quot;cuda&quot;)img_url = &quot;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png&quot;mask_url = &quot;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png&quot;init_image = load_image(img_url).convert(&quot;RGB&quot;)mask_image = load_image(mask_url).convert(&quot;RGB&quot;)prompt = &quot;A majestic tiger sitting on a bench&quot;image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]num = len(os.listdir(&#x27;./results&#x27;))image.save(f&#x27;./results/Auto_tur3_&#123;num&#125;.png&#x27;)\n\nå¦‚æœæ‚¨å°è¯•åŠ è½½ä¸å—æ”¯æŒçš„æ£€æŸ¥ç‚¹ï¼Œåˆ™ä¼šæŠ›å‡ºé”™è¯¯\nä½¿ç”¨å¤šä¸ªPipelineå¯¹äºæŸäº›å·¥ä½œæµç¨‹æˆ–å¦‚æœæ‚¨è¦åŠ è½½è®¸å¤šPipelineï¼Œä»æ£€æŸ¥ç‚¹é‡ç”¨ç›¸åŒçš„ç»„ä»¶ä¼šæ›´èŠ‚çœå†…å­˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”æƒ³è¦å†æ¬¡å°†å…¶ç”¨äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡ï¼Œè¯·ä½¿ç”¨ from_pipe() æ–¹æ³•ã€‚æ­¤æ–¹æ³•ä»å…ˆå‰åŠ è½½çš„Pipelineçš„ç»„ä»¶åˆ›å»ºæ–°Pipelineï¼Œæ— éœ€é¢å¤–çš„å†…å­˜æˆæœ¬ã€‚\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Imageimport torchpipeline_text2img = AutoPipelineForText2Image.from_pretrained(    &quot;sd-v1.5&quot;,     torch_dtype=torch.float16,     use_safetensors=True,     variant=&#x27;fp16&#x27;,)print(type(pipeline_text2img))# &quot;&lt;class &#x27;diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline&#x27;&gt;&quot;\n\nç„¶å from_pipe() å°†åŸå§‹çš„ StableDiffusionInpaintPipeline ç±»æ˜ å°„åˆ° StableDiffusionImg2ImgPipelineï¼š\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(type(pipeline_img2img))# &quot;&lt;class &#x27;diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline&#x27;&gt;&quot;\n\n\n\nå¦‚æœæ‚¨å°†å¯é€‰å‚æ•°ï¼ˆä¾‹å¦‚ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ï¼‰ä¼ é€’ç»™åŸå§‹ Pipelineï¼Œåˆ™è¯¥å‚æ•°ä¹Ÿä¼šä¼ é€’ç»™æ–°ç®¡é“ï¼š\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(    &quot;sd-v1.5&quot;,    torch_dtype=torch.float16,    use_safetensors=True,    requires_safety_checker=False,    variant=&#x27;fp16&#x27;).to(&quot;cuda&quot;)pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(pipeline_img2img.config.requires_safety_checker)&quot;False&quot;\n\nå¦‚æœæ‚¨æƒ³æ›´æ”¹æ–°ç®¡é“çš„è¡Œä¸ºï¼Œæ‚¨å¯ä»¥è¦†ç›–åŸå§‹ç®¡é“ä¸­çš„ä»»ä½•å‚æ•°ç”šè‡³é…ç½®ã€‚ä¾‹å¦‚ï¼Œè¦é‡æ–°æ‰“å¼€å®‰å…¨æ£€æŸ¥å™¨å¹¶æ·»åŠ å¼ºåº¦å‚æ•°ï¼š\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(    pipeline_text2img,     requires_safety_checker=True,     strength=0.3)print(pipeline_img2img.config.requires_safety_checker)&quot;True&quot;\n\n\nNextAPI: AutoPipeline (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"PPTæ³¨æ„äº‹é¡¹","url":"/2023/12/25/ppt%E5%88%B6%E4%BD%9C/","content":"PPTæ³¨æ„äº‹é¡¹\næ— è¡¬çº¿å­—ä½“\nè‹±æ–‡é¦–å­—æ¯å¤§å†™å³å¯\nå¸¸è§„å†…å®¹ï¼š18-36å­—å·ï¼Œåº•éƒ¨å’Œå¼•ç”¨å­—å·&lt;12\nç©ºç™½ç®€å•èƒŒæ™¯\nå¾½æ ‡ä¸è¦åœ¨å†…å®¹é¡µå‡ºç°ï¼Œç”¨äºé¦–é¡µã€è¿‡æ¸¡é¡µã€å°¾é¡µ\né¿å…é«˜é¥±å’Œé¢œè‰²çš„æ’è‰²ï¼Œé»‘ç™½æ°¸ä¸è¿‡æ—¶\nç»™é¡µé¢ç•™ç™½ï¼šä¾§é¢ç•™å‡ºç©ºé—´ï¼Œåº•éƒ¨ä¸è¦æ”¾å¤ªå¤šå†…å®¹\næ ‡é¢˜ï¼šæ¯é¡µéƒ½è¦æœ‰æ ‡é¢˜ï¼Œä¸€ä¸ªç®€å•å¥ã€ä¸è¦è¶…è¿‡ä¸¤è¡Œ\nä¸èƒ½å‡ºç°å¤§æ®µæ–‡å­—\nå•é¡µä¸èƒ½æœ‰è¿‡å¤šå†…å®¹ï¼Œç‹¬ç«‹å†…å®¹æ”¾åœ¨ä¸åŒé¡µï¼Œé¿å…å¤±å»ç„¦ç‚¹\nåŒä¸€é¡µåˆ—è¡¨ä¸è¦è¶…è¿‡3ä¸ªæ¡ç›®\nå›¾è¡¨çš„å…¨éƒ¨è¦ç´ éƒ½è¦è§£é‡Šæ¸…æ¥š\nè€ƒè™‘ä¸åŒæ—¶é—´é™åˆ¶çš„æƒ…å†µä¸‹å†…å®¹çš„å®‰æ’\nåŠ¨ç”»ï¼šå°‘å°±æ˜¯å¤šï¼Œç®€å•ä¸ºä¸»ã€‚ç”¨æ¥è¡¨è¾¾é€’è¿›ã€æ”¾å¤§ã€è¿›ä¸€æ­¥ã€å˜åŒ–ç­‰é€»è¾‘\né¡µé¢åˆ‡æ¢ï¼šå¹³æ»‘\næ€»ç»“ï¼šå¼ºè°ƒé‡è¦å†…å®¹ï¼Œå¢åŠ é¡µé¢çš„æ€»ç»“å’Œé¡µé¢ä¹‹é—´çš„ä¸²è”è®²è§£ï¼Œè®©å¬ä¼—æ˜ç™½å½“å‰çš„æ¼”è®²å¤„äºä»€ä¹ˆé˜¶æ®µ\n\nåŸºæœ¬åŸåˆ™\nå§‹ç»ˆè€ƒè™‘å¬ä¼—å¦‚ä½•æ›´å®¹æ˜“çš„æ¥å—å†…å®¹\nä¸æ‰“ç®—èŠçš„å†…å®¹åˆ é™¤\nå¥½çš„æ¼”è®²å§‹äºä¸€ä¸ªå¥½é—®é¢˜\nä¸€é¡µä¸­ä¿æŒä¸€ä¸ªå†…å®¹\n\n","categories":["Note"]}]