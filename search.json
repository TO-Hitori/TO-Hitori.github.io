[{"title":"0.解构基本pipeline","url":"/2024/04/06/0.%20%E8%A7%A3%E6%9E%84%E5%9F%BA%E6%9C%ACpipeline/","content":"解构基本pipeline借助pipeline，仅需四行代码即可生成图像：\nfrom diffusers import DDPMPipelineddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")image = ddpm(num_inference_steps=25).images[0]\n\n在上面的示例中，pipeline包含：\n\n去噪模型：UNet2DModel\n采样器：DDPMScheduler\n\npipeline通过获取所需输出大小的随机噪声并将其多次通过去噪模型来对图像进行去噪。在每个时间步，去噪模型预测噪声残差，调采样器使用它来预测噪声较小的图像。管道重复此过程，直到到达指定数量的推理步骤的图像。\n要分别使用去噪模型和采样器重新创建pipeline，让我们编写自己的去噪过程：\n\n载入去噪模型和采样器：\nfrom diffusers import DDPMScheduler, UNet2DModelscheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n\n通过采样器设置去噪过程的时间步数：\nscheduler.set_timesteps(50)\n\n设置采样器的时间步数会创建一个包含均匀间隔元素的张量，在本示例中为 50。每个元素对应于模型对图像进行去噪的时间步。在进行去噪循环时，将迭代该张量以对图像进行去噪：\nscheduler.timestepstensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,    \t700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,    \t420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,    \t140, 120, 100,  80,  60,  40,  20,   0])\n\n创建与所需输出图像形状相同的随机噪声：\nimport torchsample_size = model.config.sample_sizenoise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n\n现在编写一个循环来迭代。\n\n在每个时间步，都会执行 UNet2DModel.forward() ，根据输入的噪声图像(input)和时间步(t)返回噪声残差(noisy_residual)。\n采样器的 step() 方法利用噪声残差(noisy_residual)、时间步(t))和噪声图像(input)预测前一个时间步长的图像。该输出成为去噪循环的下一个输入。\n\ninput = noisefor t in scheduler.timesteps:    with torch.no_grad():        noisy_residual = model(input, t).sample    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample    input = previous_noisy_sample\n\n最后一步是将去噪输出转换为图像：\nfrom PIL import Imageimport numpy as np# input的数据范围为[-1, 1], 转为[0, 1]# shape: [1, 3, sample_size, sample_size] -&gt; [3, sample_size, sample_size]image = (input / 2 + 0.5).clamp(0, 1).squeeze()# shape: [3, sample_size, sample_size] -&gt; [sample_size, sample_size, 3]# data range: (float32)[0, 1] -&gt; (uint8)[0, 255]# device: cuda -&gt; cpu# type: torch.tensor -&gt; numpy.arrayimage = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()# 输出为图像image = Image.fromarray(image)image\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"1.解构Stable Diffusion","url":"/2024/04/07/1.%20%E8%A7%A3%E6%9E%84%20Stable%20Diffusion/","content":"解构Stable DiffusionStable Diffusion是一种文本到图像的潜在扩散模型（LDM）。它使用图像的低维表示而不是实际的像素空间，这使得它的内存效率更高。\n\nVAE编码器将图像压缩为低维表示，VAE解码器将压缩的低维表示转换回图像。\n\n对于文本到图像模型，需要一个分词器（tokenizer）和一个文本编码器（text encoder）来生成文本嵌入（text embeddings）。\n\n\n如上所述，SD pipeline比仅包含 UNet 模型的 DDPM pipeline更复杂。Stable Diffusion具有三个独立的预训练模型。\n\n运作机制：Stable Diffusion with 🧨 Diffusers (huggingface.co)\n\n\nThe autoencoder (VAE): AutoencoderKL\nThe U-Net: UNet2DConditionModel\nThe Text-encoder: CLIPTextModel\n\nSD pipeline 运作机制载入所有组件使用 from_pretrained() 方法加载所有组件。可以在预训练的 runwayml/stable-diffusion-v1-5 中找到它们，每个组件都存储在单独的子文件夹中：\nfrom PIL import Imageimport torchfrom transformers import CLIPTextModel, CLIPTokenizerfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler# 自编码器，使用fp16半精度权重vae = AutoencoderKL.from_pretrained(\"sd-v1.5\", subfolder=\"vae\", variant='fp16', use_safetensors=True)# 分词器tokenizer = CLIPTokenizer.from_pretrained(\"sd-v1.5\", subfolder=\"tokenizer\")# 文本编码器，使用fp16半精度权重text_encoder = CLIPTextModel.from_pretrained(\"sd-v1.5\", subfolder=\"text_encoder\", variant='fp16', use_safetensors=True)# 去噪器Unet，使用fp16半精度权重unet = UNet2DConditionModel.from_pretrained(\"sd-v1.5\", subfolder=\"unet\", variant='fp16', use_safetensors=True)\n\n将采样器替换为 UniPCMultistepScheduler，而不是 sd1.5 默认的 PNDMScheduler，通过以下代码实现：\nfrom diffusers import UniPCMultistepScheduler# 采样器，修改为UniPCMultistepScheduler 而不是默认的PNDMSchedulerscheduler = UniPCMultistepScheduler.from_pretrained(\"sd-v1.5\", subfolder=\"scheduler\")\n\n为了加速推理，将具有可训练权重的模型移至 GPU：\ntorch_device = torch.device(\"cuda\")vae.to(torch_device)text_encoder.to(torch_device)unet.to(torch_device)\n\n\n\n创建文本嵌入对文本进行标记以生成文本嵌入。该文本用于为 UNet 模型输入文本条件，在扩散过程中引导生成内容。\n\nguidance_scale: 该参数决定生成图像时应给予文本提示的权重，即文本引导的强度\n\n随意选择你喜欢的任何文本提示！设定基本参数如下：\nprompt = [\"a photograph of an astronaut riding a horse\"]batch_size = len(prompt)          # 生成的批量大小height = 512                      # 目标生成的图像的高width = 512                       # 目标生成的图像的宽num_inference_steps = 25          # 去噪总步数guidance_scale = 7.5              # classifier-free guidance 条件引导强度generator = torch.manual_seed(0)  # 用于生成随机噪声的随机数种子\n\n使用分词器（tokenizer）对文本进行标记并生成嵌入：\n# 分词text_input = tokenizer(    prompt,                                # 文本    padding=\"max_length\",                  # 填充文本以达到最大长度    max_length=tokenizer.model_max_length, # 设置了文本分词的最大长度 max_length = 77    truncation=True,                       # 如果文本超过最大长度，将截断它。    return_tensors=\"pt\"                    # 表示返回 PyTorch 张量格式的结果。).to(torch_device)# 将分词结果转为文本嵌入with torch.no_grad():    text_embeddings = text_encoder(text_input.input_ids)[0]\n\n部分变量细节：\ntext_input:&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;text_input.input_ids:  - type: &lt;class 'torch.Tensor'&gt;  - shape: torch.Size([1, 77]) [batch, max_length]  - dtype: torch.int64  - device: cuda:0    text_embeddings:  - &lt;class 'torch.Tensor'&gt;  - torch.Size([1, 77, 768])  - torch.float32  - cuda:0\n\n接下来还需要生成无条件文本嵌入，即填充标记的嵌入。这些需要与条件 text_embeddings 具有相同的形状（batch_size 和 seq_length）：\nmax_length = text_input.input_ids.shape[-1] # 77# 用空文本获取idsuncond_input = tokenizer(    [\"\"] * batch_size,     padding=\"max_length\",     max_length=max_length,     return_tensors=\"pt\")uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\n让我们将条件嵌入和无条件嵌入连接为一个批量，以避免进行两次前向传递：\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])  - &lt;class 'torch.Tensor'&gt;  - torch.Size([2, 77, 768])  - torch.float32  - cuda:0\n\n\n\n创建随机噪声接下来，生成一些初始随机噪声作为扩散过程的起点。它将逐渐去噪来生成图像的潜在表示。此时，潜在表示小于最终图像尺寸，VAE解码器后会将其转换为最终的  图像。\nprint(vae.config.block_out_channels) # [128, 256, 512, 512]print(2 ** (len(vae.config.block_out_channels) - 1))VAE的层数为4，每两层之间一次下采样，下采样倍率为8\n\n生成初始噪声：\n# shape: [1, 4, 64, 64]latents = torch.randn(    (batch_size, unet.config.in_channels, height // 8, width // 8),     generator=generator                                             ).to(torch_device)\n\n\n\n对图像进行去噪首先使用 sigma（噪声缩放值）缩放初始噪声分布，这是 UniPCMultistepScheduler 等改进采样器所需的：\nlatents = latents * scheduler.init_noise_sigma# init_noise_sigma = 1.0\n\n最后一步是创建去噪循环，该循环将逐步将纯噪声转换为文本提示所描述的图像的潜在表示。去噪循环需要做三件事：\n\n设置去噪期间采样器使用的时间步长。\n迭代时间步。\n在每个时间步 ，调用 UNet 模型来根据  预测噪声残差 ，并将其传递给采样器以计算上一个时间步的噪声样本 。from tqdm.auto import tqdm# 设定采样器的迭代步数scheduler.set_timesteps(num_inference_steps) # 25# 迭代时间步：scheduler.timesteps#   - &lt;class 'torch.Tensor'&gt;#   - torch.Size([25])#   - torch.int64#   - cpufor t in tqdm(scheduler.timesteps):    # 如果使用CFG，为了避免进行两次前向传递，则可以在batch维度扩展输入。    latent_model_input = torch.cat([latents] * 2)    # 预测噪声前，根据时间步t缩放Unet的输入x_t    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)    # 预测噪声残差    with torch.no_grad():        # unet的输出类型：&lt;class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput'&gt;        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)        noise_pred = noise_pred.sample        # &lt;class 'torch.Tensor'&gt;        # torch.Size([2, 4, 64, 64])        # torch.float32        # cuda: 0        # 使用CFG计算新的噪声残差    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)        # 使用采样器计算上一个时间步的样本：x_t -&gt; x_t-1    # 采样器的输出类型：&lt;class 'diffusers.schedulers.scheduling_utils.SchedulerOutput'&gt;    latents = scheduler.step(noise_pred, t, latents)    latents = latents.prev_sample\n\n解码潜在表示为图像最后一步是使用 vae 将潜在表示解码为图像，并通过样本获取解码输出：\n# 在将潜在表示输入VAE解码器前进行缩放latents = 1 / 0.18215 * latentswith torch.no_grad():    # VAE解码器输出类型：&lt;class 'diffusers.models.autoencoders.vae.DecoderOutput'&gt;    image = vae.decode(latents)    image = image.sample\n\n最后，将图像转换为 PIL.Image 以查看生成的图像！\n# 调整图像的范围和数据类型num = len(os.listdir('./results'))image = (image / 2 + 0.5).clamp(0, 1).squeeze()image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()image = Image.fromarray(image)image.save(f'./results/learn_sp{num}.png')\n\n\n\n从基本管道到复杂管道，您已经看到编写自己的扩散系统真正需要的只是一个降噪循环。该循环应设置调度程序的时间步长，对其进行迭代，并交替调用 UNet 模型来预测噪声残差，并将其传递给调度程序以计算先前的噪声样本。\n这就是Diffusers 的设计目的：让使用模型和调度程序直观、轻松地编写自己的扩散系统。\nNext:\n\nContribute a community pipeline (huggingface.co)\nPipelines (huggingface.co)\n运作机制：Stable Diffusion with 🧨 Diffusers (huggingface.co)\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"3.训练diffusion model","url":"/2024/04/09/3.%20%E8%AE%AD%E7%BB%83Diffusion%20Model/","content":"Train a diffusion model无条件图像生成是扩散模型的一种流行应用，它生成的图像与用于训练的数据集中的图像相似。通常，最好的结果是通过在特定数据集上微调预训练模型来获得的。\n您可以在 Hub 上找到许多这样的检查点，但如果您找不到您喜欢的检查点，您可以随时训练自己的检查点！\n本教程将教您如何在 Smithsonian Butterflies 数据集的子集上从头开始训练 UNet2DModel，以生成您自己的Butterflies\n\ndiffusers_training_example.ipynb - Colaboratory (google.com)\n\n训练配置为了方便起见，创建一个包含训练超参数的 TrainingConfig 类：\nfrom dataclasses import dataclass@dataclassclass TrainingConfig:    image_size = 128                       # 生成图像的分辨率    train_batch_size = 16                  # 训练batch    eval_batch_size = 16                   # 验证阶段的图像数量    num_epochs = 50    gradient_accumulation_steps = 1        #    learning_rate = 1e-4    lr_warmup_steps = 500    save_image_epochs = 10    save_model_epochs = 30    mixed_precision = \"fp16\"               # `no` for float32, `fp16` for automatic mixed precision    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub    push_to_hub = False  # whether to upload the saved model to the HF Hub    hub_model_id = \"TO-Hitori/my-awesome-model\"  # the name of the repository to create on the HF Hub    hub_private_repo = False    overwrite_output_dir = True  # overwrite the old model when re-running the notebook    seed = 0config = TrainingConfig()\n\n\n\n载入数据集您可以使用datasets库轻松加载Smithsonian Butterflies数据集：\nfrom datasets import load_dataset# 数据集路径：本地路径或hugging-face路径config.dataset_name = r\"D:\\MyData\\smithsonian_butterflies_subset\"dataset = load_dataset(config.dataset_name, split=\"train\")\n\n\n在此查找其他数据集：huggan (HugGAN Community) (huggingface.co)\n\nDatasets 使用 Image 功能自动解码图像数据并将其加载为我们可以可视化的 PIL.Image：\nimport matplotlib.pyplot as pltfig, axs = plt.subplots(1, 4, figsize=(16, 4))for i, image in enumerate(dataset[:4][\"image\"]):    axs[i].imshow(image)    axs[i].set_axis_off()fig.show()\n\n这些图像的尺寸各不相同，因此需要先对它们进行预处理：\n\nResize 将图像大小更改为 config.image_size 中定义的大小。 \nRandomHorizontalFlip 通过随机镜像翻转图像来增强数据集。 \nNormalize对于将像素值重新缩放到 [-1, 1] 范围非常重要，这是模型所期望的输入范围。\n\nfrom torchvision import transformspreprocess = transforms.Compose(    [        transforms.Resize((config.image_size, config.image_size)),        transforms.RandomHorizontalFlip(),        transforms.ToTensor(),        transforms.Normalize([0.5], [0.5]),    ])\n\n使用Datasets的 set_transform 方法在训练期间动态应用预处理函数：\ndef transform(examples):    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]    return {\"images\": images}dataset.set_transform(transform)\n\n将数据集包装在torch的 DataLoader 中进行训练！\nimport torchtrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n\n\n\n创建 UNet2D 模型 Diffusers 中的预训练模型可以使用您想要的参数轻松地从其模型类创建。例如，要创建 UNet2DModel：\n# 创建U-Netfrom diffusers import UNet2DModelmodel = UNet2DModel(    sample_size=config.image_size,  # 图像分辨率    in_channels=3,                  # 输入图像的通道数量    out_channels=3,                 # 输出图像的通道数量    layers_per_block=2,             # 每层使用的残差块个数    block_out_channels=(128, 128, 256, 256, 512, 512),  # 每一层的输出通道数量    down_block_types=(        \"DownBlock2D\",              # 残差下采样模块        \"DownBlock2D\",        \"DownBlock2D\",        \"DownBlock2D\",        \"AttnDownBlock2D\",          # 有spatial self-attention的下采样残差模块        \"DownBlock2D\",    ),    up_block_types=(        \"UpBlock2D\",                # 残差上采样模块        \"AttnUpBlock2D\",            # 有spatial self-attention的上采样残差模块        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",    ),)print(\"UNet2DModel have {} paramerters in total\".format(sum(x.numel() for x in model.parameters())))# UNet2DModel have 113673219 paramerters in total\n\n检查样本图像形状与模型输出形状是否匹配：\nsample_image = dataset[0][\"images\"].unsqueeze(0)print(\"Input shape:\", sample_image.shape)print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)'''Input shape: torch.Size([1, 3, 128, 128])Output shape: torch.Size([1, 3, 128, 128])'''\n\n\n\n创建采样器根据您使用模型进行训练还是推理，采样器的行为会有所不同。\n\n在推理过程中，采样器根据噪声生成图像。\n在训练期间，采样器从扩散过程中的特定点获取模型输出（或样本），并根据噪声调度noise schedule和更新规则update rule将噪声注入图像。\n\n让我们看一下 DDPMScheduler 并使用 add_noise 方法向sample_image 添加一些随机噪声：\nimport torchfrom PIL import Imagefrom diffusers import DDPMSchedulerfrom torchvision.utils import save_image, make_grid# 用总步数来初始化采样器noise_scheduler = DDPMScheduler(num_train_timesteps=1000)# 设定加噪序列，这里选择了7个依次增大的时间步timesteps = torch.LongTensor([50, 150, 250, 450, 650, 850, 990])# 时间步的数量为batch，采样图片的形状后三个维度，构建采样噪声noise = torch.randn(timesteps.shape + sample_image.shape[1:])# 利用采样器的add_noise方法将噪声注入采样图片noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)# 保存图片观测采样器的加噪效果save_to_show = make_grid(torch.cat([sample_image, noisy_image], dim=0))save_image(save_to_show, './test_scheduler.png')\n\n模型的训练目标是预测添加到图像中的噪声。这一步的损失可以通过下式计算：\n# 损失函数：最简单的MSE损失函数import torch.nn.functional as Fprint('test loss func')noise_pred = model(noisy_image, timesteps).sampleloss = F.mse_loss(noise_pred, noise)print('loss value = ', loss.item())\n\n\n\n训练模型到目前为止，您已经掌握了开始训练模型的大部分内容，剩下的就是将所有内容组合在一起。 首先，您需要一个优化器和一个学习率调度器：\n# 开始训练：设置调度器from diffusers.optimization import get_cosine_schedule_with_warmupoptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)lr_scheduler = get_cosine_schedule_with_warmup(    optimizer=optimizer,    num_warmup_steps=config.lr_warmup_steps,    num_training_steps=(len(train_dataloader) * config.num_epochs),)\n\n然后，您需要一种评估模型的方法。为了进行评估，您可以使用 DDPMPipeline 生成一批样本图像并将其保存为网格：\nfrom diffusers import DDPMPipelinefrom diffusers.utils import make_image_gridimport osdef evaluate(config, epoch, pipeline):    # Sample some images from random noise (this is the backward diffusion process).    # The default pipeline output type is `List[PIL.Image]`    # 使用pipeline生成图像    images = pipeline(        batch_size=config.eval_batch_size,        generator=torch.manual_seed(config.seed),    ).images    # 将图像拼接为网格    image_grid = make_image_grid(images, rows=4, cols=4)    # 保存验证图像    test_dir = os.path.join(config.output_dir, \"samples\")    os.makedirs(test_dir, exist_ok=True)    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n\n现在，您可以使用 Accelerate 将所有这些组件包装在一个训练循环中，以轻松进行：\n\nTensorBoard 日志记录\n梯度累积和混合精度训练。\n要将模型上传到 Hub，请编写一个函数来获取存储库名称和信息，然后将其推送到 Hub。\n\n接下来是训练核心部分：\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):    # 初始化 accelerator 和 tensorboard 日志记录    accelerator = Accelerator(        mixed_precision=config.mixed_precision,                          # 是否混合精度训练        gradient_accumulation_steps=config.gradient_accumulation_steps,  # 梯度累积步数        log_with=\"tensorboard\",                                          # 使用tensorboard记录日志        project_dir=os.path.join(config.output_dir, \"logs\"),             # 日志路径    )    # 如果在主进程    if accelerator.is_main_process:        # 创建输出文件夹        if config.output_dir is not None:            os.makedirs(config.output_dir, exist_ok=True)        # 上传到HF的设置        if config.push_to_hub:            repo_id = create_repo(                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True            ).repo_id        # 初始化追踪器        accelerator.init_trackers(\"train_example\")    # 用accelerate包装：模型、优化器、数据加载器和学习率调度器    # 保证输入和输出的顺序一致即可    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(        model, optimizer, train_dataloader, lr_scheduler    )    # 初始化全局步数    global_step = 0    # 开始训练模型    for epoch in range(config.num_epochs):        # 创建进度条        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)        # 设置进度条描述        progress_bar.set_description(f\"Epoch {epoch}\")        # 对数据加载器中的每个批次进行循环        for step, batch in enumerate(train_dataloader):            # 从数据集获取图像            clean_images = batch[\"images\"]            # 生成即将加入图像的噪声            noise = torch.randn(clean_images.shape, device=clean_images.device)            # 获取当前batch-size            bs = clean_images.shape[0]            # 为当前batch中每个图像随机采样一个时间步            timesteps = torch.randint(                0,                                          # 起点                noise_scheduler.config.num_train_timesteps, # 终点                (bs,),                                      # 形状/数量                device=clean_images.device,                dtype=torch.int64            )            # 使用采样器，根据每个时间步的噪声幅度向干净的图像添加噪声            # 这是前向扩散过程            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)            # 使用accelerator累积模型梯度            with accelerator.accumulate(model):                # 预测噪声残差                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]                # 计算损失函数                loss = F.mse_loss(noise_pred, noise)                # 反向传播梯度                accelerator.backward(loss)                # 梯度裁剪                accelerator.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()      # 迭代优化器                lr_scheduler.step()   # 迭代学习率调度器                optimizer.zero_grad() # 清零梯度            # 更新基督徒            progress_bar.update(1)            # 记录日志：损失函数，学习率变化            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}            progress_bar.set_postfix(**logs)            accelerator.log(logs, step=global_step)            # 全局步数增加            global_step += 1        # 在每个epoch后，你可以选择使用evaluate()采样一些演示图像并保存模型        if accelerator.is_main_process:            # 初始化一个pipeline，传入当前训练的模型和调度器            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)            # 根据验证频率保存研究结果            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:                evaluate(config, epoch, pipeline)            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:                if config.push_to_hub:                    upload_folder(                        repo_id=repo_id,                        folder_path=config.output_dir,                        commit_message=f\"Epoch {epoch}\",                        ignore_patterns=[\"step_*\", \"epoch_*\"],                    )                else:                    pipeline.save_pretrained(config.output_dir)train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextdiffusers_training_example.ipynb - Colaboratory (google.com)\nTextual Inversion (huggingface.co)\nDreamBooth (huggingface.co)\nText-to-image (huggingface.co)\nLoRA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"2.AutoPipeline基本教程","url":"/2024/04/09/2.%20Auto%20Pipeline/","content":"AutoPipeline基本教程AutoPipeline 的设计目的是： \n\n轻松加载 checkpoint，而无需知道要使用的特定pipeline的类型 \n在工作流程中使用多个pipeline\n\nAutoPipeline 类旨在简化 Diffusers 中的各种Pipeline。它是一个通用的、任务优先的管道。 AutoPipeline 会自动检测要使用的正确 Pipeline 类，这使得在不知道特定Pipeline类名称的情况下轻松加载任务的检查点\n\nAPI: AutoPipeline (huggingface.co)\n\nDiffusers 能够完成许多不同的任务，并且您通常可以将相同的预训练权重重复用于多个任务，例如文本到图像、图像到图像和修复。如果您对库和扩散模型不熟悉，可能很难知道要使用哪个管道来完成任务。\n为您的任务选择 AutoPipeline首先选择一个检查点。例如，如果想要使用 runwayml/stable-diffusion-v1-5 检查点来进行文本到图像（T2I）任务，请使用 AutoPipelineForText2Image：\nfrom diffusers import AutoPipelineForText2Imageimport torch, ospipeline = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",                 # 权重路径    torch_dtype=torch.float16, # 数据类型    use_safetensors=True,      # 使用safetensor类型的权重    variant='fp16',            # 加载权重时选择文件名中带有‘fp16’的).to(\"cuda\")prompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"image = pipeline(prompt, num_inference_steps=25).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur_{num}.png')\n\n深入层次探讨 AutoPipelineForText2Image \n\nAutoPipeline将自动从 model_index.json 文件中检测 StableDiffusionPipeline 类\n根据类名加载相应的文本到图像的StableDiffusionPipeline\n\nmodel_index.json文件内容：\n{  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.6.0\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n\n\n同样，对于图像到图像任务的AutoPipeline，AutoPipelineForImage2Image 从 model_index.json 文件中检测到 “StableDiffusion” 检查点，并将在幕后加载相应的 StableDiffusionImg2ImgPipeline。\n还可以传递特定于 Pipeline 类的任何其他参数，如guidance_scale、strength\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForImage2Imageimport torchimport requestsfrom PIL import Imagefrom io import BytesIOpipeline = AutoPipelineForImage2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    variant='fp16',).to(\"cuda\")prompt = \"a portrait of a dog wearing a pearl earring\"url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"response = requests.get(url)image = Image.open(BytesIO(response.content)).convert(\"RGB\")image.thumbnail((768, 768))image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur2_{num}.png')\n\n\n\n如果您想进行图像修复，则 AutoPipelineForInpainting 以相同的方式加载底层的 StableDiffusionInpaintPipeline 类：\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForInpaintingfrom diffusers.utils import load_imageimport torchpipeline = AutoPipelineForInpainting.from_pretrained(    r\"D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',).to(\"cuda\")img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"init_image = load_image(img_url).convert(\"RGB\")mask_image = load_image(mask_url).convert(\"RGB\")prompt = \"A majestic tiger sitting on a bench\"image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur3_{num}.png')\n\n如果您尝试加载不受支持的检查点，则会抛出错误\n使用多个Pipeline对于某些工作流程或如果您要加载许多Pipeline，从检查点重用相同的组件会更节省内存。例如，如果您正在使用文本到图像任务的检查点，并且想要再次将其用于图像到图像任务，请使用 from_pipe() 方法。此方法从先前加载的Pipeline的组件创建新Pipeline，无需额外的内存成本。\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Imageimport torchpipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',)print(type(pipeline_text2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'&gt;\"\n\n然后 from_pipe() 将原始的 StableDiffusionInpaintPipeline 类映射到 StableDiffusionImg2ImgPipeline：\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(type(pipeline_img2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'&gt;\"\n\n\n\n如果您将可选参数（例如禁用安全检查器）传递给原始 Pipeline，则该参数也会传递给新管道：\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    requires_safety_checker=False,    variant='fp16').to(\"cuda\")pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(pipeline_img2img.config.requires_safety_checker)\"False\"\n\n如果您想更改新管道的行为，您可以覆盖原始管道中的任何参数甚至配置。例如，要重新打开安全检查器并添加强度参数：\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(    pipeline_text2img,     requires_safety_checker=True,     strength=0.3)print(pipeline_img2img.config.requires_safety_checker)\"True\"\n\n\nNextAPI: AutoPipeline (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"4.载入 LoRA 进行推理","url":"/2024/04/10/4.%20%E8%BD%BD%E5%85%A5%20LoRA%20%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86/","content":"\n      \n        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\n      \n      \n        \n          \n          \n            请输入与 Rhodes Island™ 取得弱神经连接时的口令：\n          \n        \n        \n      \n    \n    ","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"5.加速 T2I 扩散模型的推理","url":"/2024/04/10/5.%20%E5%8A%A0%E9%80%9F%20T2I%20%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86/","content":"加速 T2I 扩散模型的推理由于迭代和反向扩散过程，扩散模型比 GAN 模型慢。有多种技术可以解决此限制：\n\n渐进时间步蒸馏 (LCM LoRA)\n模型压缩 (SSD-1B) \n重用降噪器的相邻特征 (DeepCache)。\n\n\n\nPerforming inference with LCM-LoRA (huggingface.co)\n\nsegmind/SSD-1B · Hugging Face\n\nDeepCache (huggingface.co)\n\n\n\n但是，不一定需要使用这些技术来加速推理。仅使用 PyTorch 2，您就可以将文本到图像扩散模型的推理加速最多 3 倍。\n降低精度-bfloat16启用第一个优化：降低精度或更具体地说 bfloat16。使用降低的精度有几个好处：\n\n使用降低的数值精度（例如 float16 或 bfloat16）进行推理不会影响生成质量，但会显著改善推理延迟。\n与 float16 相比，使用 bfloat16 的优势取决于硬件，现代 GPU 倾向于使用 bfloat16。\n与 float16 相比，bfloat16 在与量化一起使用时更具弹性，但我们使用的最新版本的量化库 torchao 不存在 float16 的数值问题。\n\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # 使用float16    variant=\"fp16\").to(\"cuda\")# Run the attention ops without SDPA.pipe.unet.set_default_attn_processor()pipe.vae.set_default_attn_processor()prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\n\nSDPAAttention模块运行耗时长。使用 PyTorch 的 scaled_dot_product_attention 函数后，它的效率要高很多。 Diffusers 中默认使用此函数，因此您无需对代码进行任何更改。\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # 使用float16    variant=\"fp16\").to(\"cuda\")prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\nSDPA (huggingface.co)\n\ntorch.compile\nWindows 不支持：\nRuntimeError: Windows not yet supported for torch.compile\n\nPyTorch 2 包含 torch.compile，它使用快速且优化的内核。在 Diffusers 中，通常编译 UNet 和 VAE模块，因为它们是计算最耗时的模块。\nfrom diffusers import StableDiffusionXLPipelineimport torchtorch._inductor.config.conv_1x1_as_mm = Truetorch._inductor.config.coordinate_descent_tuning = Truetorch._inductor.config.epilogue_fusion = Falsetorch._inductor.config.coordinate_descent_check_all_directions = True\n\n在编译 UNet 和 VAE 时将其内存布局更改为“channels_last”也很重要，以确保最大速度。\npipe.unet.to(memory_format=torch.channels_last)pipe.vae.to(memory_format=torch.channels_last)\n\n现在编译 UNet 和 VAE 并执行推理：\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)pipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\ntorch.compile 提供不同的后端和模式。为了获得最大推理速度，请对 inductor backend 使用“max-autotune”。 \n“max-autotune”使用 CUDA 图并专门针对延迟优化编译图。 CUDA 图通过使用通过单个 CPU 操作启动多个 GPU 操作的机制，大大减少了启动 GPU 操作的开销。\nPrevent graph breaks指定 fullgraph=True 可确保底层模型中没有图形中断，以充分利用 torch.compile，而不会降低任何性能。对于 UNet 和 VAE，这意味着更改访问返回变量的方式。\n- latents = unet(    latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds-).sample+ latents = unet(+   latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds, +   return_dict=False+)[0]\n\n\n\n\n\n组合Attention模块的投影矩阵SDXL 中的 UNet 和 VAE 使用类似 Transformer 的模块，由Attention模块和feed-forward模块组成。\n在Attention模块中，使用三个不同的投影矩阵（Q、K 和 V）将输入投影到三个子空间中。这些投影在输入上单独执行。但是我们可以将投影矩阵水平组合成一个矩阵并一步执行投影。这增加了输入投影的矩阵乘法的大小并改善了量化的影响。\n只需一行代码即可组合投影矩阵：\npipe.fuse_qkv_projections()\n\n\n这种方法效果优先，且仅适用于部分扩散模型\n\n动态量化还可以使用超轻量级 PyTorch 量化库 torchao（commit SHA 54bcd5a10d0abbe7b0c045052029257099f83fd9）将动态 int8 量化应用于 UNet 和 VAE。量化给模型增加了额外的转换开销，可以通过更快的 matmuls（动态量化）来弥补。\n如果矩阵相乘太小，这些技术可能会降低性能。\nNextSpeed up inference (huggingface.co)\nSDPA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"YAML","url":"/2024/03/14/YAML%E5%9F%BA%E7%A1%80/","content":"YAMLYAML Ain’t a Markup Language\nYet Another Markup Language\nYAML是”YAML Ain’t a Markup Language”（YAML不是一种标记语言）的递归缩写。在开发的这种语言时，YAML的意思其实是：”Yet Another Markup Language”（仍是一种标记语言），但为了强调这种语言以数据为中心，而不是以标记语言为重点，而用反向缩略语重命名。YAML (wikipedia.org)\nYAML特点是使用空格来表达层次结构，特别适合用来表达或编辑数据结构、各种配置文件，其文件一般以 .yaml 为后缀。\n基本语法\n以 k: v 的形式来表示键值对的关系\n\n冒号后面必须有一个空格\n\n\n只支持单行注释，注释符号：# \n\n大小写敏感\n\n通过缩进来表示层级关系\n\n缩排中空格的数目不重要，只要相同阶层的元素左侧对齐就可以了\n缩进只能使用空格，不能使用 tab 缩进\n\n\n字符串可以不用双引号\n\n一个文件中可以包含多个文件的内容\n\n用--- 即三个破折号表示一份内容的开始\n用...即三个小数点表示一份内容的结束（非必需）\n\n\n\n数据结构与类型对象以键值对 key: value 形式组织数据\n1. 使用**冒号+空格**来分开键与值\n1. 支持多层嵌套（**用缩进表示层级关系**）\n\nmodel:  base_learning_rate: 4.5e-6  target: ldm.models.autoencoder.AutoencoderKL  params:    monitor: &quot;val/rec_loss&quot;    embed_dim: 64    lossconfig:      target: ldm.modules.losses.LPIPSWithDiscriminator      params:        disc_start: 50001        kl_weight: 0.000001        disc_weight: 0.5\n\n\n支持流式风格（Flow style）的语法：用花括号包裹，用逗号加空格分隔\n\nkey: &#123;child-key1: value1, child-key2: value2 &#125;\n\n\n\n数组\n一组以区块格式（“破折号+空格”）开头的数据组成一个数组\n\nunet_config:  target: ldm.modules.diffusionmodules.openaimodel.UNetModel  params:    image_size: 64    in_channels: 3    out_channels: 3    model_channels: 224    attention_resolutions:    - 8    - 4    - 2    num_res_blocks: 2    channel_mult:    - 1    - 2    - 3    - 4    num_head_channels: 32\n\n\n也支持内联格式来表达（用方括号包裹，逗号加空格分隔）\n\nddconfig:  double_z: True  z_channels: 64  resolution: 256  in_channels: 3  out_ch: 3  ch: 128  ch_mult: [1, 1, 2, 2, 4, 4]    num_res_blocks: 2  attn_resolutions: [16, 8]  dropout: 0.0\n\n\n支持多维数组（用缩进表示层级关系）\n\nvalues:  - - 1    - 2  - - 3    - 4# 等价：    values: [[1, 2], [3, 4]]\n\n\n\n字符串\n字符串一般不需要用引号包裹\n字符串换行视为一个空格\n单引号可以屏蔽转义\n字符串中需要使用转义字符\\就必须使用双引号包裹\n\nstrings:  - Hello world # 不用引号包裹  - Hello     world # 换行视为一个空格  - &#x27;字符串\\n换行\\n演示&#x27;  # 单引号可以屏蔽转义  - &quot;字符串\\n换行\\n演示&quot;  # 双引号使用转移符号# 结果：- Hello world- Hello world- 字符串\\n换行\\n演示- &#x27;字符串  换行  演示&#x27;\n\n\n保留换行：使用竖线符“ | ”来表示该语法，每行的缩进和行尾空白都会被去掉，而额外的缩进会被保留\n\nlines: |  我是第一行  我是第二行    我是吴彦祖      我是第四行  我是第五行  # 结果&quot;我是第一行\\n我是第二行\\n  我是吴彦祖\\n    我是第四行\\n我是第五行\\n&quot;\n\n\n折叠换行：使用右尖括号“ &gt; ”来表示该语法，只有空白行才会被识别为换行，原来的换行符都会被转换成空格\n\nlines: &gt;  我是第一行  我也是第一行  我仍是第一行  我依旧是第一行  我是第二行  这么巧我也是第二行# 结果lines2: &#x27;我是第一行 我也是第一行 我仍是第一行 我依旧是第一行  我是第二行 这么巧我也是第二行  &#x27;\n\n\n\n布尔值\n“true”、“True”、“TRUE”、“yes”、“Yes”和“YES”皆为真\n“false”、“False”、“FALSE”、“no”、“No”和“NO”皆为假\n\n整数\n支持二进制表示\n\nint:  - 666  - 0001_0000# 结果int:- 666- 4096\n\n\n\n浮点数\n支持科学计数法\n\nfloat:  - 3.14  - 6.8523015e+5 # 使用科学计数法# 结果float:- 3.14- 685230.15\n\n空 Nullnull、Null、~ 和不指定值都表示空\nnulls:  - null  - Null  - ~  -# 结果nulls:- null- null- null- null\n\n\n\n强制类型转换双感叹号+目标类型来强制转换类型\na: !!float &#x27;666&#x27; # !! 为严格类型标签b: !!int &#x27;666&#x27;   # 字符串转为整型c: !!str 666     # 整数转为字符串d: !!str 666.66  # 浮点数转为字符串e: !!str true    # 布尔值转为字符串f: !!bool &#x27;yes&#x27;  # 字符串转为布尔值# 结果a: 666.0b: 666c: &#x27;666&#x27;d: &#x27;666.66&#x27;e: &#x27;true&#x27;f: true\n\n\n\n数据复用与合并数据复用在key的冒号后，使用锚点符号&amp;设定锚点，使用引用符号*引用锚点\nmodel: &amp;all_parm  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params: &amp;model_parm    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model: *all_parmnew_params: *model_parm# 结果new_model:  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_params:  linear_start: 0.0015  linear_end: 0.0195  num_timesteps_cond: 1  log_every_t: 200  timesteps: 1000  first_stage_key: image  image_size: 64  channels: 3  monitor: val/loss_simple_ema\n\n\n\n数据合并合并标签符号“&lt;&lt;”配合锚点符号和引用符号使用可以与任意数据进行合并，可以视为面向对象中的继承\nmodel_location: &amp;loc  target: ldm.models.diffusion.ddpm.LatentDiffusionmodel_params: &amp;params  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model:  base_learning_rate: 2.0e-06  &lt;&lt;: *loc  &lt;&lt;: *params  # 结果new_model:  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_ema  base_learning_rate: 2.0e-06\n\n\n\n参考一文看懂 YAML - 知乎 (zhihu.com)\n","categories":["Tech"],"tags":["python"]},{"title":"Hello World","url":"/2024/02/02/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"conda常用命令","url":"/2021/11/25/conda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/","content":"conda常用命令在windows的cmd下使用如下指令进入conda：\nactivate\n\n环境管理创建虚拟环境：conda create -n [env_name] python=[X.X]\n\n\nenv_name：要创建的环境的名字\nX.X：要创建的环境的python的版本，如3.7\n\n激活虚拟环境conda activate [env_name]\n\n停用当前环境conda deactivate\n\n查看当前环境的python版本python --version\n\n查看所有存在的虚拟环境conda info -econda env list\n\n删除虚拟环境：conda remove -n [env_name] --all\n\n重命名环境\nconda没有直接重命名环境的功能，但可以通过以下两个步骤完成：\n克隆要重命名的环境\n将原环境删除\n\n\n\nconda create --name [newname] --clone [oldname]conda remove --name [oldname] --all\n\n\n\n包管理安装包conda install [pac_name]=[包的版本号]\n\n查看已经安装的包\n查看当前环境：\n\nconda list\n\n\n查看指定环境：\n\nconda list -n [env_name]\n\n删除包conda uninstall [pac_name]\n\n更新指定包conda update [pac_name]\n\n清理包\n通过以下指令来删除一些没用的包，这个命令会检查哪些包没有在包缓存中被硬依赖到其他地方，并删除它们\n\nconda clean -p\n\n\n删除conda保存下来的tar包\n\nconda clean -t\n\n\n删除所有的安装包及cache\n\nconda clean -y --all\n\n更新condaconda update conda\n\n安装requirements.txt文件内的包\n首先通过cd指令进入requirements.txt文件所在路径，然后执行如下指令即可\n\npip install -r requirements.txt\n\n包的数据源管理\n显示目前conda的数据源有哪些：\n\nconda config --show channels\n\n\n添加数据源：(清华源)\n\nconda config --add https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\n删除数据源\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n","categories":["Tech"],"tags":["python"]},{"title":"PPT注意事项","url":"/2023/12/25/ppt%E5%88%B6%E4%BD%9C/","content":"PPT注意事项\n无衬线字体\n英文首字母大写即可\n常规内容：18-36字号，底部和引用字号&lt;12\n空白简单背景\n徽标不要在内容页出现，用于首页、过渡页、尾页\n避免高饱和颜色的撞色，黑白永不过时\n给页面留白：侧面留出空间，底部不要放太多内容\n标题：每页都要有标题，一个简单句、不要超过两行\n不能出现大段文字\n单页不能有过多内容，独立内容放在不同页，避免失去焦点\n同一页列表不要超过3个条目\n图表的全部要素都要解释清楚\n考虑不同时间限制的情况下内容的安排\n动画：少就是多，简单为主。用来表达递进、放大、进一步、变化等逻辑\n页面切换：平滑\n总结：强调重要内容，增加页面的总结和页面之间的串联讲解，让听众明白当前的演讲处于什么阶段\n\n基本原则\n始终考虑听众如何更容易的接受内容\n不打算聊的内容删除\n好的演讲始于一个好问题\n一页中保持一个内容\n\n","categories":["Note"]}]