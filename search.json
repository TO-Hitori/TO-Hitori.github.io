[{"title":"0.解构基本pipeline","url":"/2024/04/06/0.%20%E8%A7%A3%E6%9E%84%E5%9F%BA%E6%9C%ACpipeline/","content":"解构基本pipeline借助pipeline，仅需四行代码即可生成图像：\nfrom diffusers import DDPMPipelineddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")image = ddpm(num_inference_steps=25).images[0]\n\n在上面的示例中，pipeline包含：\n\n去噪模型：UNet2DModel\n采样器：DDPMScheduler\n\npipeline通过获取所需输出大小的随机噪声并将其多次通过去噪模型来对图像进行去噪。在每个时间步，去噪模型预测噪声残差，调采样器使用它来预测噪声较小的图像。管道重复此过程，直到到达指定数量的推理步骤的图像。\n要分别使用去噪模型和采样器重新创建pipeline，让我们编写自己的去噪过程：\n\n载入去噪模型和采样器：\nfrom diffusers import DDPMScheduler, UNet2DModelscheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n\n通过采样器设置去噪过程的时间步数：\nscheduler.set_timesteps(50)\n\n设置采样器的时间步数会创建一个包含均匀间隔元素的张量，在本示例中为 50。每个元素对应于模型对图像进行去噪的时间步。在进行去噪循环时，将迭代该张量以对图像进行去噪：\nscheduler.timestepstensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,    \t700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,    \t420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,    \t140, 120, 100,  80,  60,  40,  20,   0])\n\n创建与所需输出图像形状相同的随机噪声：\nimport torchsample_size = model.config.sample_sizenoise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n\n现在编写一个循环来迭代。\n\n在每个时间步，都会执行 UNet2DModel.forward() ，根据输入的噪声图像(input)和时间步(t)返回噪声残差(noisy_residual)。\n采样器的 step() 方法利用噪声残差(noisy_residual)、时间步(t))和噪声图像(input)预测前一个时间步长的图像。该输出成为去噪循环的下一个输入。\n\ninput = noisefor t in scheduler.timesteps:    with torch.no_grad():        noisy_residual = model(input, t).sample    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample    input = previous_noisy_sample\n\n最后一步是将去噪输出转换为图像：\nfrom PIL import Imageimport numpy as np# input的数据范围为[-1, 1], 转为[0, 1]# shape: [1, 3, sample_size, sample_size] -&gt; [3, sample_size, sample_size]image = (input / 2 + 0.5).clamp(0, 1).squeeze()# shape: [3, sample_size, sample_size] -&gt; [sample_size, sample_size, 3]# data range: (float32)[0, 1] -&gt; (uint8)[0, 255]# device: cuda -&gt; cpu# type: torch.tensor -&gt; numpy.arrayimage = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()# 输出为图像image = Image.fromarray(image)image\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"1.解构Stable Diffusion","url":"/2024/04/07/1.%20%E8%A7%A3%E6%9E%84%20Stable%20Diffusion/","content":"解构Stable DiffusionStable Diffusion是一种文本到图像的潜在扩散模型（LDM）。它使用图像的低维表示而不是实际的像素空间，这使得它的内存效率更高。\n\nVAE编码器将图像压缩为低维表示，VAE解码器将压缩的低维表示转换回图像。\n\n对于文本到图像模型，需要一个分词器（tokenizer）和一个文本编码器（text encoder）来生成文本嵌入（text embeddings）。\n\n\n如上所述，SD pipeline比仅包含 UNet 模型的 DDPM pipeline更复杂。Stable Diffusion具有三个独立的预训练模型。\n\n运作机制：Stable Diffusion with 🧨 Diffusers (huggingface.co)\n\n\nThe autoencoder (VAE): AutoencoderKL\nThe U-Net: UNet2DConditionModel\nThe Text-encoder: CLIPTextModel\n\nSD pipeline 运作机制载入所有组件使用 from_pretrained() 方法加载所有组件。可以在预训练的 runwayml/stable-diffusion-v1-5 中找到它们，每个组件都存储在单独的子文件夹中：\nfrom PIL import Imageimport torchfrom transformers import CLIPTextModel, CLIPTokenizerfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler# 自编码器，使用fp16半精度权重vae = AutoencoderKL.from_pretrained(\"sd-v1.5\", subfolder=\"vae\", variant='fp16', use_safetensors=True)# 分词器tokenizer = CLIPTokenizer.from_pretrained(\"sd-v1.5\", subfolder=\"tokenizer\")# 文本编码器，使用fp16半精度权重text_encoder = CLIPTextModel.from_pretrained(\"sd-v1.5\", subfolder=\"text_encoder\", variant='fp16', use_safetensors=True)# 去噪器Unet，使用fp16半精度权重unet = UNet2DConditionModel.from_pretrained(\"sd-v1.5\", subfolder=\"unet\", variant='fp16', use_safetensors=True)\n\n将采样器替换为 UniPCMultistepScheduler，而不是 sd1.5 默认的 PNDMScheduler，通过以下代码实现：\nfrom diffusers import UniPCMultistepScheduler# 采样器，修改为UniPCMultistepScheduler 而不是默认的PNDMSchedulerscheduler = UniPCMultistepScheduler.from_pretrained(\"sd-v1.5\", subfolder=\"scheduler\")\n\n为了加速推理，将具有可训练权重的模型移至 GPU：\ntorch_device = torch.device(\"cuda\")vae.to(torch_device)text_encoder.to(torch_device)unet.to(torch_device)\n\n\n\n创建文本嵌入对文本进行标记以生成文本嵌入。该文本用于为 UNet 模型输入文本条件，在扩散过程中引导生成内容。\n\nguidance_scale: 该参数决定生成图像时应给予文本提示的权重，即文本引导的强度\n\n随意选择你喜欢的任何文本提示！设定基本参数如下：\nprompt = [\"a photograph of an astronaut riding a horse\"]batch_size = len(prompt)          # 生成的批量大小height = 512                      # 目标生成的图像的高width = 512                       # 目标生成的图像的宽num_inference_steps = 25          # 去噪总步数guidance_scale = 7.5              # classifier-free guidance 条件引导强度generator = torch.manual_seed(0)  # 用于生成随机噪声的随机数种子\n\n使用分词器（tokenizer）对文本进行标记并生成嵌入：\n# 分词text_input = tokenizer(    prompt,                                # 文本    padding=\"max_length\",                  # 填充文本以达到最大长度    max_length=tokenizer.model_max_length, # 设置了文本分词的最大长度 max_length = 77    truncation=True,                       # 如果文本超过最大长度，将截断它。    return_tensors=\"pt\"                    # 表示返回 PyTorch 张量格式的结果。).to(torch_device)# 将分词结果转为文本嵌入with torch.no_grad():    text_embeddings = text_encoder(text_input.input_ids)[0]\n\n部分变量细节：\ntext_input:&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;text_input.input_ids:  - type: &lt;class 'torch.Tensor'&gt;  - shape: torch.Size([1, 77]) [batch, max_length]  - dtype: torch.int64  - device: cuda:0    text_embeddings:  - &lt;class 'torch.Tensor'&gt;  - torch.Size([1, 77, 768])  - torch.float32  - cuda:0\n\n接下来还需要生成无条件文本嵌入，即填充标记的嵌入。这些需要与条件 text_embeddings 具有相同的形状（batch_size 和 seq_length）：\nmax_length = text_input.input_ids.shape[-1] # 77# 用空文本获取idsuncond_input = tokenizer(    [\"\"] * batch_size,     padding=\"max_length\",     max_length=max_length,     return_tensors=\"pt\")uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\n让我们将条件嵌入和无条件嵌入连接为一个批量，以避免进行两次前向传递：\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])  - &lt;class 'torch.Tensor'&gt;  - torch.Size([2, 77, 768])  - torch.float32  - cuda:0\n\n\n\n创建随机噪声接下来，生成一些初始随机噪声作为扩散过程的起点。它将逐渐去噪来生成图像的潜在表示。此时，潜在表示小于最终图像尺寸，VAE解码器后会将其转换为最终的  图像。\nprint(vae.config.block_out_channels) # [128, 256, 512, 512]print(2 ** (len(vae.config.block_out_channels) - 1))VAE的层数为4，每两层之间一次下采样，下采样倍率为8\n\n生成初始噪声：\n# shape: [1, 4, 64, 64]latents = torch.randn(    (batch_size, unet.config.in_channels, height // 8, width // 8),     generator=generator                                             ).to(torch_device)\n\n\n\n对图像进行去噪首先使用 sigma（噪声缩放值）缩放初始噪声分布，这是 UniPCMultistepScheduler 等改进采样器所需的：\nlatents = latents * scheduler.init_noise_sigma# init_noise_sigma = 1.0\n\n最后一步是创建去噪循环，该循环将逐步将纯噪声转换为文本提示所描述的图像的潜在表示。去噪循环需要做三件事：\n\n设置去噪期间采样器使用的时间步长。\n迭代时间步。\n在每个时间步 ，调用 UNet 模型来根据  预测噪声残差 ，并将其传递给采样器以计算上一个时间步的噪声样本 。from tqdm.auto import tqdm# 设定采样器的迭代步数scheduler.set_timesteps(num_inference_steps) # 25# 迭代时间步：scheduler.timesteps#   - &lt;class 'torch.Tensor'&gt;#   - torch.Size([25])#   - torch.int64#   - cpufor t in tqdm(scheduler.timesteps):    # 如果使用CFG，为了避免进行两次前向传递，则可以在batch维度扩展输入。    latent_model_input = torch.cat([latents] * 2)    # 预测噪声前，根据时间步t缩放Unet的输入x_t    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)    # 预测噪声残差    with torch.no_grad():        # unet的输出类型：&lt;class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput'&gt;        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)        noise_pred = noise_pred.sample        # &lt;class 'torch.Tensor'&gt;        # torch.Size([2, 4, 64, 64])        # torch.float32        # cuda: 0        # 使用CFG计算新的噪声残差    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)        # 使用采样器计算上一个时间步的样本：x_t -&gt; x_t-1    # 采样器的输出类型：&lt;class 'diffusers.schedulers.scheduling_utils.SchedulerOutput'&gt;    latents = scheduler.step(noise_pred, t, latents)    latents = latents.prev_sample\n\n解码潜在表示为图像最后一步是使用 vae 将潜在表示解码为图像，并通过样本获取解码输出：\n# 在将潜在表示输入VAE解码器前进行缩放latents = 1 / 0.18215 * latentswith torch.no_grad():    # VAE解码器输出类型：&lt;class 'diffusers.models.autoencoders.vae.DecoderOutput'&gt;    image = vae.decode(latents)    image = image.sample\n\n最后，将图像转换为 PIL.Image 以查看生成的图像！\n# 调整图像的范围和数据类型num = len(os.listdir('./results'))image = (image / 2 + 0.5).clamp(0, 1).squeeze()image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()image = Image.fromarray(image)image.save(f'./results/learn_sp{num}.png')\n\n\n\n从基本管道到复杂管道，您已经看到编写自己的扩散系统真正需要的只是一个降噪循环。该循环应设置调度程序的时间步长，对其进行迭代，并交替调用 UNet 模型来预测噪声残差，并将其传递给调度程序以计算先前的噪声样本。\n这就是Diffusers 的设计目的：让使用模型和调度程序直观、轻松地编写自己的扩散系统。\nNext:\n\nContribute a community pipeline (huggingface.co)\nPipelines (huggingface.co)\n运作机制：Stable Diffusion with 🧨 Diffusers (huggingface.co)\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"3.训练diffusion model","url":"/2024/04/09/3.%20%E8%AE%AD%E7%BB%83Diffusion%20Model/","content":"Train a diffusion model无条件图像生成是扩散模型的一种流行应用，它生成的图像与用于训练的数据集中的图像相似。通常，最好的结果是通过在特定数据集上微调预训练模型来获得的。\n您可以在 Hub 上找到许多这样的检查点，但如果您找不到您喜欢的检查点，您可以随时训练自己的检查点！\n本教程将教您如何在 Smithsonian Butterflies 数据集的子集上从头开始训练 UNet2DModel，以生成您自己的Butterflies\n\ndiffusers_training_example.ipynb - Colaboratory (google.com)\n\n训练配置为了方便起见，创建一个包含训练超参数的 TrainingConfig 类：\nfrom dataclasses import dataclass@dataclassclass TrainingConfig:    image_size = 128                       # 生成图像的分辨率    train_batch_size = 16                  # 训练batch    eval_batch_size = 16                   # 验证阶段的图像数量    num_epochs = 50    gradient_accumulation_steps = 1        #    learning_rate = 1e-4    lr_warmup_steps = 500    save_image_epochs = 10    save_model_epochs = 30    mixed_precision = \"fp16\"               # `no` for float32, `fp16` for automatic mixed precision    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub    push_to_hub = False  # whether to upload the saved model to the HF Hub    hub_model_id = \"TO-Hitori/my-awesome-model\"  # the name of the repository to create on the HF Hub    hub_private_repo = False    overwrite_output_dir = True  # overwrite the old model when re-running the notebook    seed = 0config = TrainingConfig()\n\n\n\n载入数据集您可以使用datasets库轻松加载Smithsonian Butterflies数据集：\nfrom datasets import load_dataset# 数据集路径：本地路径或hugging-face路径config.dataset_name = r\"D:\\MyData\\smithsonian_butterflies_subset\"dataset = load_dataset(config.dataset_name, split=\"train\")\n\n\n在此查找其他数据集：huggan (HugGAN Community) (huggingface.co)\n\nDatasets 使用 Image 功能自动解码图像数据并将其加载为我们可以可视化的 PIL.Image：\nimport matplotlib.pyplot as pltfig, axs = plt.subplots(1, 4, figsize=(16, 4))for i, image in enumerate(dataset[:4][\"image\"]):    axs[i].imshow(image)    axs[i].set_axis_off()fig.show()\n\n这些图像的尺寸各不相同，因此需要先对它们进行预处理：\n\nResize 将图像大小更改为 config.image_size 中定义的大小。 \nRandomHorizontalFlip 通过随机镜像翻转图像来增强数据集。 \nNormalize对于将像素值重新缩放到 [-1, 1] 范围非常重要，这是模型所期望的输入范围。\n\nfrom torchvision import transformspreprocess = transforms.Compose(    [        transforms.Resize((config.image_size, config.image_size)),        transforms.RandomHorizontalFlip(),        transforms.ToTensor(),        transforms.Normalize([0.5], [0.5]),    ])\n\n使用Datasets的 set_transform 方法在训练期间动态应用预处理函数：\ndef transform(examples):    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]    return {\"images\": images}dataset.set_transform(transform)\n\n将数据集包装在torch的 DataLoader 中进行训练！\nimport torchtrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n\n\n\n创建 UNet2D 模型 Diffusers 中的预训练模型可以使用您想要的参数轻松地从其模型类创建。例如，要创建 UNet2DModel：\n# 创建U-Netfrom diffusers import UNet2DModelmodel = UNet2DModel(    sample_size=config.image_size,  # 图像分辨率    in_channels=3,                  # 输入图像的通道数量    out_channels=3,                 # 输出图像的通道数量    layers_per_block=2,             # 每层使用的残差块个数    block_out_channels=(128, 128, 256, 256, 512, 512),  # 每一层的输出通道数量    down_block_types=(        \"DownBlock2D\",              # 残差下采样模块        \"DownBlock2D\",        \"DownBlock2D\",        \"DownBlock2D\",        \"AttnDownBlock2D\",          # 有spatial self-attention的下采样残差模块        \"DownBlock2D\",    ),    up_block_types=(        \"UpBlock2D\",                # 残差上采样模块        \"AttnUpBlock2D\",            # 有spatial self-attention的上采样残差模块        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",    ),)print(\"UNet2DModel have {} paramerters in total\".format(sum(x.numel() for x in model.parameters())))# UNet2DModel have 113673219 paramerters in total\n\n检查样本图像形状与模型输出形状是否匹配：\nsample_image = dataset[0][\"images\"].unsqueeze(0)print(\"Input shape:\", sample_image.shape)print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)'''Input shape: torch.Size([1, 3, 128, 128])Output shape: torch.Size([1, 3, 128, 128])'''\n\n\n\n创建采样器根据您使用模型进行训练还是推理，采样器的行为会有所不同。\n\n在推理过程中，采样器根据噪声生成图像。\n在训练期间，采样器从扩散过程中的特定点获取模型输出（或样本），并根据噪声调度noise schedule和更新规则update rule将噪声注入图像。\n\n让我们看一下 DDPMScheduler 并使用 add_noise 方法向sample_image 添加一些随机噪声：\nimport torchfrom PIL import Imagefrom diffusers import DDPMSchedulerfrom torchvision.utils import save_image, make_grid# 用总步数来初始化采样器noise_scheduler = DDPMScheduler(num_train_timesteps=1000)# 设定加噪序列，这里选择了7个依次增大的时间步timesteps = torch.LongTensor([50, 150, 250, 450, 650, 850, 990])# 时间步的数量为batch，采样图片的形状后三个维度，构建采样噪声noise = torch.randn(timesteps.shape + sample_image.shape[1:])# 利用采样器的add_noise方法将噪声注入采样图片noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)# 保存图片观测采样器的加噪效果save_to_show = make_grid(torch.cat([sample_image, noisy_image], dim=0))save_image(save_to_show, './test_scheduler.png')\n\n模型的训练目标是预测添加到图像中的噪声。这一步的损失可以通过下式计算：\n# 损失函数：最简单的MSE损失函数import torch.nn.functional as Fprint('test loss func')noise_pred = model(noisy_image, timesteps).sampleloss = F.mse_loss(noise_pred, noise)print('loss value = ', loss.item())\n\n\n\n训练模型到目前为止，您已经掌握了开始训练模型的大部分内容，剩下的就是将所有内容组合在一起。 首先，您需要一个优化器和一个学习率调度器：\n# 开始训练：设置调度器from diffusers.optimization import get_cosine_schedule_with_warmupoptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)lr_scheduler = get_cosine_schedule_with_warmup(    optimizer=optimizer,    num_warmup_steps=config.lr_warmup_steps,    num_training_steps=(len(train_dataloader) * config.num_epochs),)\n\n然后，您需要一种评估模型的方法。为了进行评估，您可以使用 DDPMPipeline 生成一批样本图像并将其保存为网格：\nfrom diffusers import DDPMPipelinefrom diffusers.utils import make_image_gridimport osdef evaluate(config, epoch, pipeline):    # Sample some images from random noise (this is the backward diffusion process).    # The default pipeline output type is `List[PIL.Image]`    # 使用pipeline生成图像    images = pipeline(        batch_size=config.eval_batch_size,        generator=torch.manual_seed(config.seed),    ).images    # 将图像拼接为网格    image_grid = make_image_grid(images, rows=4, cols=4)    # 保存验证图像    test_dir = os.path.join(config.output_dir, \"samples\")    os.makedirs(test_dir, exist_ok=True)    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n\n现在，您可以使用 Accelerate 将所有这些组件包装在一个训练循环中，以轻松进行：\n\nTensorBoard 日志记录\n梯度累积和混合精度训练。\n要将模型上传到 Hub，请编写一个函数来获取存储库名称和信息，然后将其推送到 Hub。\n\n接下来是训练核心部分：\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):    # 初始化 accelerator 和 tensorboard 日志记录    accelerator = Accelerator(        mixed_precision=config.mixed_precision,                          # 是否混合精度训练        gradient_accumulation_steps=config.gradient_accumulation_steps,  # 梯度累积步数        log_with=\"tensorboard\",                                          # 使用tensorboard记录日志        project_dir=os.path.join(config.output_dir, \"logs\"),             # 日志路径    )    # 如果在主进程    if accelerator.is_main_process:        # 创建输出文件夹        if config.output_dir is not None:            os.makedirs(config.output_dir, exist_ok=True)        # 上传到HF的设置        if config.push_to_hub:            repo_id = create_repo(                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True            ).repo_id        # 初始化追踪器        accelerator.init_trackers(\"train_example\")    # 用accelerate包装：模型、优化器、数据加载器和学习率调度器    # 保证输入和输出的顺序一致即可    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(        model, optimizer, train_dataloader, lr_scheduler    )    # 初始化全局步数    global_step = 0    # 开始训练模型    for epoch in range(config.num_epochs):        # 创建进度条        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)        # 设置进度条描述        progress_bar.set_description(f\"Epoch {epoch}\")        # 对数据加载器中的每个批次进行循环        for step, batch in enumerate(train_dataloader):            # 从数据集获取图像            clean_images = batch[\"images\"]            # 生成即将加入图像的噪声            noise = torch.randn(clean_images.shape, device=clean_images.device)            # 获取当前batch-size            bs = clean_images.shape[0]            # 为当前batch中每个图像随机采样一个时间步            timesteps = torch.randint(                0,                                          # 起点                noise_scheduler.config.num_train_timesteps, # 终点                (bs,),                                      # 形状/数量                device=clean_images.device,                dtype=torch.int64            )            # 使用采样器，根据每个时间步的噪声幅度向干净的图像添加噪声            # 这是前向扩散过程            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)            # 使用accelerator累积模型梯度            with accelerator.accumulate(model):                # 预测噪声残差                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]                # 计算损失函数                loss = F.mse_loss(noise_pred, noise)                # 反向传播梯度                accelerator.backward(loss)                # 梯度裁剪                accelerator.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()      # 迭代优化器                lr_scheduler.step()   # 迭代学习率调度器                optimizer.zero_grad() # 清零梯度            # 更新基督徒            progress_bar.update(1)            # 记录日志：损失函数，学习率变化            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}            progress_bar.set_postfix(**logs)            accelerator.log(logs, step=global_step)            # 全局步数增加            global_step += 1        # 在每个epoch后，你可以选择使用evaluate()采样一些演示图像并保存模型        if accelerator.is_main_process:            # 初始化一个pipeline，传入当前训练的模型和调度器            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)            # 根据验证频率保存研究结果            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:                evaluate(config, epoch, pipeline)            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:                if config.push_to_hub:                    upload_folder(                        repo_id=repo_id,                        folder_path=config.output_dir,                        commit_message=f\"Epoch {epoch}\",                        ignore_patterns=[\"step_*\", \"epoch_*\"],                    )                else:                    pipeline.save_pretrained(config.output_dir)train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextdiffusers_training_example.ipynb - Colaboratory (google.com)\nTextual Inversion (huggingface.co)\nDreamBooth (huggingface.co)\nText-to-image (huggingface.co)\nLoRA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"2.AutoPipeline基本教程","url":"/2024/04/09/2.%20Auto%20Pipeline/","content":"AutoPipeline基本教程AutoPipeline 的设计目的是： \n\n轻松加载 checkpoint，而无需知道要使用的特定pipeline的类型 \n在工作流程中使用多个pipeline\n\nAutoPipeline 类旨在简化 Diffusers 中的各种Pipeline。它是一个通用的、任务优先的管道。 AutoPipeline 会自动检测要使用的正确 Pipeline 类，这使得在不知道特定Pipeline类名称的情况下轻松加载任务的检查点\n\nAPI: AutoPipeline (huggingface.co)\n\nDiffusers 能够完成许多不同的任务，并且您通常可以将相同的预训练权重重复用于多个任务，例如文本到图像、图像到图像和修复。如果您对库和扩散模型不熟悉，可能很难知道要使用哪个管道来完成任务。\n为您的任务选择 AutoPipeline首先选择一个检查点。例如，如果想要使用 runwayml/stable-diffusion-v1-5 检查点来进行文本到图像（T2I）任务，请使用 AutoPipelineForText2Image：\nfrom diffusers import AutoPipelineForText2Imageimport torch, ospipeline = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",                 # 权重路径    torch_dtype=torch.float16, # 数据类型    use_safetensors=True,      # 使用safetensor类型的权重    variant='fp16',            # 加载权重时选择文件名中带有‘fp16’的).to(\"cuda\")prompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"image = pipeline(prompt, num_inference_steps=25).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur_{num}.png')\n\n深入层次探讨 AutoPipelineForText2Image \n\nAutoPipeline将自动从 model_index.json 文件中检测 StableDiffusionPipeline 类\n根据类名加载相应的文本到图像的StableDiffusionPipeline\n\nmodel_index.json文件内容：\n{  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.6.0\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n\n\n同样，对于图像到图像任务的AutoPipeline，AutoPipelineForImage2Image 从 model_index.json 文件中检测到 “StableDiffusion” 检查点，并将在幕后加载相应的 StableDiffusionImg2ImgPipeline。\n还可以传递特定于 Pipeline 类的任何其他参数，如guidance_scale、strength\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForImage2Imageimport torchimport requestsfrom PIL import Imagefrom io import BytesIOpipeline = AutoPipelineForImage2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    variant='fp16',).to(\"cuda\")prompt = \"a portrait of a dog wearing a pearl earring\"url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"response = requests.get(url)image = Image.open(BytesIO(response.content)).convert(\"RGB\")image.thumbnail((768, 768))image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur2_{num}.png')\n\n\n\n如果您想进行图像修复，则 AutoPipelineForInpainting 以相同的方式加载底层的 StableDiffusionInpaintPipeline 类：\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForInpaintingfrom diffusers.utils import load_imageimport torchpipeline = AutoPipelineForInpainting.from_pretrained(    r\"D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',).to(\"cuda\")img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"init_image = load_image(img_url).convert(\"RGB\")mask_image = load_image(mask_url).convert(\"RGB\")prompt = \"A majestic tiger sitting on a bench\"image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur3_{num}.png')\n\n如果您尝试加载不受支持的检查点，则会抛出错误\n使用多个Pipeline对于某些工作流程或如果您要加载许多Pipeline，从检查点重用相同的组件会更节省内存。例如，如果您正在使用文本到图像任务的检查点，并且想要再次将其用于图像到图像任务，请使用 from_pipe() 方法。此方法从先前加载的Pipeline的组件创建新Pipeline，无需额外的内存成本。\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Imageimport torchpipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',)print(type(pipeline_text2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'&gt;\"\n\n然后 from_pipe() 将原始的 StableDiffusionInpaintPipeline 类映射到 StableDiffusionImg2ImgPipeline：\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(type(pipeline_img2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'&gt;\"\n\n\n\n如果您将可选参数（例如禁用安全检查器）传递给原始 Pipeline，则该参数也会传递给新管道：\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    requires_safety_checker=False,    variant='fp16').to(\"cuda\")pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(pipeline_img2img.config.requires_safety_checker)\"False\"\n\n如果您想更改新管道的行为，您可以覆盖原始管道中的任何参数甚至配置。例如，要重新打开安全检查器并添加强度参数：\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(    pipeline_text2img,     requires_safety_checker=True,     strength=0.3)print(pipeline_img2img.config.requires_safety_checker)\"True\"\n\n\nNextAPI: AutoPipeline (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"4.载入 LoRA 进行推理","url":"/2024/04/10/4.%20%E8%BD%BD%E5%85%A5%20LoRA%20%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86/","content":"\n      \n        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\n      \n      \n        \n          \n          \n            请输入与 Rhodes Island™ 取得弱神经连接时的口令：\n          \n        \n        \n      \n    \n    ","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"5.加速 T2I 扩散模型的推理","url":"/2024/04/10/5.%20%E5%8A%A0%E9%80%9F%20T2I%20%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86/","content":"加速 T2I 扩散模型的推理由于迭代和反向扩散过程，扩散模型比 GAN 模型慢。有多种技术可以解决此限制：\n\n渐进时间步蒸馏 (LCM LoRA)\n模型压缩 (SSD-1B) \n重用降噪器的相邻特征 (DeepCache)。\n\n\n\nPerforming inference with LCM-LoRA (huggingface.co)\n\nsegmind/SSD-1B · Hugging Face\n\nDeepCache (huggingface.co)\n\n\n\n但是，不一定需要使用这些技术来加速推理。仅使用 PyTorch 2，您就可以将文本到图像扩散模型的推理加速最多 3 倍。\n降低精度-bfloat16启用第一个优化：降低精度或更具体地说 bfloat16。使用降低的精度有几个好处：\n\n使用降低的数值精度（例如 float16 或 bfloat16）进行推理不会影响生成质量，但会显著改善推理延迟。\n与 float16 相比，使用 bfloat16 的优势取决于硬件，现代 GPU 倾向于使用 bfloat16。\n与 float16 相比，bfloat16 在与量化一起使用时更具弹性，但我们使用的最新版本的量化库 torchao 不存在 float16 的数值问题。\n\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # 使用float16    variant=\"fp16\").to(\"cuda\")# Run the attention ops without SDPA.pipe.unet.set_default_attn_processor()pipe.vae.set_default_attn_processor()prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\n\nSDPAAttention模块运行耗时长。使用 PyTorch 的 scaled_dot_product_attention 函数后，它的效率要高很多。 Diffusers 中默认使用此函数，因此您无需对代码进行任何更改。\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # 使用float16    variant=\"fp16\").to(\"cuda\")prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\nSDPA (huggingface.co)\n\ntorch.compile\nWindows 不支持：\nRuntimeError: Windows not yet supported for torch.compile\n\nPyTorch 2 包含 torch.compile，它使用快速且优化的内核。在 Diffusers 中，通常编译 UNet 和 VAE模块，因为它们是计算最耗时的模块。\nfrom diffusers import StableDiffusionXLPipelineimport torchtorch._inductor.config.conv_1x1_as_mm = Truetorch._inductor.config.coordinate_descent_tuning = Truetorch._inductor.config.epilogue_fusion = Falsetorch._inductor.config.coordinate_descent_check_all_directions = True\n\n在编译 UNet 和 VAE 时将其内存布局更改为“channels_last”也很重要，以确保最大速度。\npipe.unet.to(memory_format=torch.channels_last)pipe.vae.to(memory_format=torch.channels_last)\n\n现在编译 UNet 和 VAE 并执行推理：\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)pipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\ntorch.compile 提供不同的后端和模式。为了获得最大推理速度，请对 inductor backend 使用“max-autotune”。 \n“max-autotune”使用 CUDA 图并专门针对延迟优化编译图。 CUDA 图通过使用通过单个 CPU 操作启动多个 GPU 操作的机制，大大减少了启动 GPU 操作的开销。\nPrevent graph breaks指定 fullgraph=True 可确保底层模型中没有图形中断，以充分利用 torch.compile，而不会降低任何性能。对于 UNet 和 VAE，这意味着更改访问返回变量的方式。\n- latents = unet(    latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds-).sample+ latents = unet(+   latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds, +   return_dict=False+)[0]\n\n\n\n\n\n组合Attention模块的投影矩阵SDXL 中的 UNet 和 VAE 使用类似 Transformer 的模块，由Attention模块和feed-forward模块组成。\n在Attention模块中，使用三个不同的投影矩阵（Q、K 和 V）将输入投影到三个子空间中。这些投影在输入上单独执行。但是我们可以将投影矩阵水平组合成一个矩阵并一步执行投影。这增加了输入投影的矩阵乘法的大小并改善了量化的影响。\n只需一行代码即可组合投影矩阵：\npipe.fuse_qkv_projections()\n\n\n这种方法效果优先，且仅适用于部分扩散模型\n\n动态量化还可以使用超轻量级 PyTorch 量化库 torchao（commit SHA 54bcd5a10d0abbe7b0c045052029257099f83fd9）将动态 int8 量化应用于 UNet 和 VAE。量化给模型增加了额外的转换开销，可以通过更快的 matmuls（动态量化）来弥补。\n如果矩阵相乘太小，这些技术可能会降低性能。\nNextSpeed up inference (huggingface.co)\nSDPA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"YAML","url":"/2024/03/14/YAML%E5%9F%BA%E7%A1%80/","content":"YAMLYAML Ain’t a Markup Language\nYet Another Markup Language\nYAML是”YAML Ain’t a Markup Language”（YAML不是一种标记语言）的递归缩写。在开发的这种语言时，YAML的意思其实是：”Yet Another Markup Language”（仍是一种标记语言），但为了强调这种语言以数据为中心，而不是以标记语言为重点，而用反向缩略语重命名。YAML (wikipedia.org)\nYAML特点是使用空格来表达层次结构，特别适合用来表达或编辑数据结构、各种配置文件，其文件一般以 .yaml 为后缀。\n基本语法\n以 k: v 的形式来表示键值对的关系\n\n冒号后面必须有一个空格\n\n\n只支持单行注释，注释符号：# \n\n大小写敏感\n\n通过缩进来表示层级关系\n\n缩排中空格的数目不重要，只要相同阶层的元素左侧对齐就可以了\n缩进只能使用空格，不能使用 tab 缩进\n\n\n字符串可以不用双引号\n\n一个文件中可以包含多个文件的内容\n\n用--- 即三个破折号表示一份内容的开始\n用...即三个小数点表示一份内容的结束（非必需）\n\n\n\n数据结构与类型对象以键值对 key: value 形式组织数据\n1. 使用**冒号+空格**来分开键与值\n1. 支持多层嵌套（**用缩进表示层级关系**）\n\nmodel:  base_learning_rate: 4.5e-6  target: ldm.models.autoencoder.AutoencoderKL  params:    monitor: &quot;val/rec_loss&quot;    embed_dim: 64    lossconfig:      target: ldm.modules.losses.LPIPSWithDiscriminator      params:        disc_start: 50001        kl_weight: 0.000001        disc_weight: 0.5\n\n\n支持流式风格（Flow style）的语法：用花括号包裹，用逗号加空格分隔\n\nkey: &#123;child-key1: value1, child-key2: value2 &#125;\n\n\n\n数组\n一组以区块格式（“破折号+空格”）开头的数据组成一个数组\n\nunet_config:  target: ldm.modules.diffusionmodules.openaimodel.UNetModel  params:    image_size: 64    in_channels: 3    out_channels: 3    model_channels: 224    attention_resolutions:    - 8    - 4    - 2    num_res_blocks: 2    channel_mult:    - 1    - 2    - 3    - 4    num_head_channels: 32\n\n\n也支持内联格式来表达（用方括号包裹，逗号加空格分隔）\n\nddconfig:  double_z: True  z_channels: 64  resolution: 256  in_channels: 3  out_ch: 3  ch: 128  ch_mult: [1, 1, 2, 2, 4, 4]    num_res_blocks: 2  attn_resolutions: [16, 8]  dropout: 0.0\n\n\n支持多维数组（用缩进表示层级关系）\n\nvalues:  - - 1    - 2  - - 3    - 4# 等价：    values: [[1, 2], [3, 4]]\n\n\n\n字符串\n字符串一般不需要用引号包裹\n字符串换行视为一个空格\n单引号可以屏蔽转义\n字符串中需要使用转义字符\\就必须使用双引号包裹\n\nstrings:  - Hello world # 不用引号包裹  - Hello     world # 换行视为一个空格  - &#x27;字符串\\n换行\\n演示&#x27;  # 单引号可以屏蔽转义  - &quot;字符串\\n换行\\n演示&quot;  # 双引号使用转移符号# 结果：- Hello world- Hello world- 字符串\\n换行\\n演示- &#x27;字符串  换行  演示&#x27;\n\n\n保留换行：使用竖线符“ | ”来表示该语法，每行的缩进和行尾空白都会被去掉，而额外的缩进会被保留\n\nlines: |  我是第一行  我是第二行    我是吴彦祖      我是第四行  我是第五行  # 结果&quot;我是第一行\\n我是第二行\\n  我是吴彦祖\\n    我是第四行\\n我是第五行\\n&quot;\n\n\n折叠换行：使用右尖括号“ &gt; ”来表示该语法，只有空白行才会被识别为换行，原来的换行符都会被转换成空格\n\nlines: &gt;  我是第一行  我也是第一行  我仍是第一行  我依旧是第一行  我是第二行  这么巧我也是第二行# 结果lines2: &#x27;我是第一行 我也是第一行 我仍是第一行 我依旧是第一行  我是第二行 这么巧我也是第二行  &#x27;\n\n\n\n布尔值\n“true”、“True”、“TRUE”、“yes”、“Yes”和“YES”皆为真\n“false”、“False”、“FALSE”、“no”、“No”和“NO”皆为假\n\n整数\n支持二进制表示\n\nint:  - 666  - 0001_0000# 结果int:- 666- 4096\n\n\n\n浮点数\n支持科学计数法\n\nfloat:  - 3.14  - 6.8523015e+5 # 使用科学计数法# 结果float:- 3.14- 685230.15\n\n空 Nullnull、Null、~ 和不指定值都表示空\nnulls:  - null  - Null  - ~  -# 结果nulls:- null- null- null- null\n\n\n\n强制类型转换双感叹号+目标类型来强制转换类型\na: !!float &#x27;666&#x27; # !! 为严格类型标签b: !!int &#x27;666&#x27;   # 字符串转为整型c: !!str 666     # 整数转为字符串d: !!str 666.66  # 浮点数转为字符串e: !!str true    # 布尔值转为字符串f: !!bool &#x27;yes&#x27;  # 字符串转为布尔值# 结果a: 666.0b: 666c: &#x27;666&#x27;d: &#x27;666.66&#x27;e: &#x27;true&#x27;f: true\n\n\n\n数据复用与合并数据复用在key的冒号后，使用锚点符号&amp;设定锚点，使用引用符号*引用锚点\nmodel: &amp;all_parm  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params: &amp;model_parm    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model: *all_parmnew_params: *model_parm# 结果new_model:  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_params:  linear_start: 0.0015  linear_end: 0.0195  num_timesteps_cond: 1  log_every_t: 200  timesteps: 1000  first_stage_key: image  image_size: 64  channels: 3  monitor: val/loss_simple_ema\n\n\n\n数据合并合并标签符号“&lt;&lt;”配合锚点符号和引用符号使用可以与任意数据进行合并，可以视为面向对象中的继承\nmodel_location: &amp;loc  target: ldm.models.diffusion.ddpm.LatentDiffusionmodel_params: &amp;params  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model:  base_learning_rate: 2.0e-06  &lt;&lt;: *loc  &lt;&lt;: *params  # 结果new_model:  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_ema  base_learning_rate: 2.0e-06\n\n\n\n参考一文看懂 YAML - 知乎 (zhihu.com)\n","categories":["Tech"],"tags":["python"]},{"title":"Hello World","url":"/2024/02/02/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"conda常用命令","url":"/2021/11/25/conda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/","content":"conda常用命令在windows的cmd下使用如下指令进入conda：\nactivate\n\n环境管理创建虚拟环境：conda create -n [env_name] python=[X.X]\n\n\nenv_name：要创建的环境的名字\nX.X：要创建的环境的python的版本，如3.7\n\n激活虚拟环境conda activate [env_name]\n\n停用当前环境conda deactivate\n\n查看当前环境的python版本python --version\n\n查看所有存在的虚拟环境conda info -econda env list\n\n删除虚拟环境：conda remove -n [env_name] --all\n\n重命名环境\nconda没有直接重命名环境的功能，但可以通过以下两个步骤完成：\n克隆要重命名的环境\n将原环境删除\n\n\n\nconda create --name [newname] --clone [oldname]conda remove --name [oldname] --all\n\n\n\n包管理安装包conda install [pac_name]=[包的版本号]\n\n查看已经安装的包\n查看当前环境：\n\nconda listpip list\n\n\n查看指定环境：\n\nconda list -n [env_name]\n\n删除包conda uninstall [pac_name]\n\n更新指定包conda update [pac_name]pip install [pac_name] -U\n\n清理包\n通过以下指令来删除一些没用的包，这个命令会检查哪些包没有在包缓存中被硬依赖到其他地方，并删除它们\n\nconda clean -p\n\n\n删除conda保存下来的tar包\n\nconda clean -t\n\n\n删除所有的安装包及cache\n\nconda clean -y --all\n\n更新condaconda update conda\n\n安装requirements.txt文件内的包\n首先通过cd指令进入requirements.txt文件所在路径，然后执行如下指令即可\n\npip install -r requirements.txt\n\n包的数据源管理\n显示目前conda的数据源有哪些：\n\nconda config --show channels\n\n\n添加数据源：(清华源)\n\nconda config --add https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\n删除数据源\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\n安装出现异常：创建文件：C:\\Users\\[user name]\\AppData\\Roaming\\pip\\pip.ini写入：[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host = mirrors.aliyun.com\n\n","categories":["Tech"],"tags":["python"]},{"title":"PPT注意事项","url":"/2023/12/25/ppt%E5%88%B6%E4%BD%9C/","content":"PPT注意事项\n无衬线字体\n英文首字母大写即可\n常规内容：18-36字号，底部和引用字号&lt;12\n空白简单背景\n徽标不要在内容页出现，用于首页、过渡页、尾页\n避免高饱和颜色的撞色，黑白永不过时\n给页面留白：侧面留出空间，底部不要放太多内容\n标题：每页都要有标题，一个简单句、不要超过两行\n不能出现大段文字\n单页不能有过多内容，独立内容放在不同页，避免失去焦点\n同一页列表不要超过3个条目\n图表的全部要素都要解释清楚\n考虑不同时间限制的情况下内容的安排\n动画：少就是多，简单为主。用来表达递进、放大、进一步、变化等逻辑\n页面切换：平滑\n总结：强调重要内容，增加页面的总结和页面之间的串联讲解，让听众明白当前的演讲处于什么阶段\n\n基本原则\n始终考虑听众如何更容易的接受内容\n不打算聊的内容删除\n好的演讲始于一个好问题\n一页中保持一个内容\n\n","categories":["Note"]},{"title":"【Using-Diffusers-01】Loading & Hub","url":"/2024/04/11/Loading%20&%20Hub/","content":"[TOC]\nLoading &amp; HubOverviewDiffusers 为生成任务提供了许多 Pipeline、模型和采样器。为了使加载这些组件尽可能简单，我们提供了一个统一的方法 from_pretrained() 从 Hugging Face Hub 或本地路径加载任何组件。\n从 Hugging Face Hub 加载模型需要配置代理：\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'\n\n本节的主要内容包括：\n\n加载 Pipeline\n加载 Pipeline 中的不同组件\n加载检查点变体\n加载社区 Pipeline 所需了解的所有信息。\n加载采样器并比较使用不同采样器的速度和质量权衡。\n转换和加载 KerasCV 检查点，以便在 PyTorch 中通过 Diffusers 使用它们。\n\nLoad pipelines, models, and schedulersDiffusionPipeline 将整个扩散系统的复杂组成包装到一个易于使用的 API 中，同时保持足够的灵活性，例如将每个组件单独加载来组装您自己的扩散系统。\n推理或训练所需的一切都可以通过 from_pretrained() 方法访问。\n\n来自 Hub 和本地的 Pipeline \n将不同的组件加入 Pipeline 中\n检查点变体，例如不同的浮点类型或非指数平均 (EMA) 权重 \n模型和采样器\n\n通过Diffusion Pipeline加载检查点DiffusionPipeline 类是从 Hub 加载最新扩散模型的最简单、最通用的方法。\nDiffusionPipeline.from_pretrained() 方法自动从检查点检测正确的Pipeline类，并返回用于推理的 Pipeline 实例。\nfrom diffusers import DiffusionPipelineimport torch# 从HF Hub加载，会自动下载需要使用的文件（需要VPN，配置代理）# repo_id = \"runwayml/stable-diffusion-v1-5\"# 从本地路径加载checkpoint_path = r'D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0'pipe = DiffusionPipeline.from_pretrained(    checkpoint_path,            # 或 repo_id    use_safetensors=True,       # 使用后缀为.safetensors的权重文件    torch_dtype=torch.float16,  # 加载16位的半精度模型    variant=\"fp16\"              # 16位权重文件中包含'fp16'字段)\n\n出现如下信息表示成功加载各个组件：\nLoading pipeline components...: 100%|██████████| 7/7 [00:00&lt;00:00,  9.67it/s]\n\n还可以加载具有特定 Pipeline 类的检查点。要获得相同的结果，请使用 StableDiffusionXLPipeline 类：\nfrom diffusers import StableDiffusionXLPipelineimport torch# repo_id = \"runwayml/stable-diffusion-v1-5\"checkpoint_path = r'D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0'pipe = StableDiffusionXLPipeline.from_pretrained(    checkpoint_path,                use_safetensors=True,           torch_dtype=torch.float16,      variant=\"fp16\"              )\n\n检查点（例如 runwayml/stable-diffusion-v1-5 可以用于多个任务，例如文本到图像生成或图像到图像生成。为了区分特定的任务，必须直接使用其相应任务的 Pipeline 类来加载它：\nfrom diffusers import StableDiffusionXLInpaintPipelineStableDiffusionXLImg2ImgPipelineStableDiffusionXLControlNetImg2ImgPipeline...\n\n\n\n替换Pipeline中的组件可以使用其他兼容的组件自定义任何管道的默认组件。这种定制很重要，因为：\n\n更改采样器对于探索生成速度和质量之间的权衡非常重要。\n模型的不同组件通常是独立训练的，您可以用性能更好的组件替换原有组件。\n在微调过程中，通常只训练一些组件（例如 UNet 或文本编码器）。\n\n以 SD1.5 为例，要找出哪些采样器兼容自定义，可以使用 compatibles 方法：\nfrom diffusers import DiffusionPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16')# 查看能够兼容当前pipeline的采样器print(pipeline.scheduler.compatibles)\n\n输出可供替换的采样器类组成的列表：\n[&lt;class 'diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_pndm.PNDMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_edm_euler.EDMEulerScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler'&gt;]\n\n使用 SchedulerMixin.from_pretrained() 方法将默认的 PNDMScheduler 替换为性能更高的采样器 EulerDiscreteScheduler。\n需要 subfolder=\"scheduler\" 参数才能从 Pipeline 检查点存储的正确子文件夹加载采样器配置文件。\n然后，就可以将新的 EulerDiscreteScheduler 实例传递给 DiffusionPipeline 中的采样器参数：\nfrom diffusers import DiffusionPipelinefrom diffusers import EulerDiscreteSchedulerimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'scheduler = EulerDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    scheduler=scheduler,    torch_dtype=torch.float16,    variant='fp16')print(pipeline.scheduler)\n\n如下所示，当前采样器被修改为 EulerDiscreteScheduler\nEulerDiscreteScheduler {  \"_class_name\": \"EulerDiscreteScheduler\",  \"_diffusers_version\": \"0.27.2\",  \"beta_end\": 0.012,  \"beta_schedule\": \"scaled_linear\",  \"beta_start\": 0.00085,  \"clip_sample\": false,  \"interpolation_type\": \"linear\",  \"num_train_timesteps\": 1000,  \"prediction_type\": \"epsilon\",  \"rescale_betas_zero_snr\": false,  \"set_alpha_to_one\": false,  \"sigma_max\": null,  \"sigma_min\": null,  \"skip_prk_steps\": true,  \"steps_offset\": 1,  \"timestep_spacing\": \"linspace\",  \"timestep_type\": \"discrete\",  \"trained_betas\": null,  \"use_karras_sigmas\": false}\n\n\n\nSafety checker像 Stable Diffusion 这样的扩散模型可以生成有害内容，这就是为什么Diffusers 有一个安全检查器（Safety checker）来根据已知的 NSFW（Not Safe For Work） 内容检查生成的输出。\n如果想要禁用安全检查器，请将 None 传递给 safety_checker 参数：\npipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    scheduler=scheduler,    safety_checker = None,    torch_dtype=torch.float16,    variant='fp16')\n\n加载组件的数量变为6个：\nLoading pipeline components...: 100%|██████████| 6/6 [00:00&lt;00:00, 15.77it/s]\n\n\n\n跨Pipeline重用组件您还可以在多个 Pipeline 中重复使用相同的组件，以避免将权重加载到 RAM 中两次。使用 components 方法保存组件：\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'stable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(    checkpoint_path,    safety_checker=None,    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')components = stable_diffusion_txt2img.components\n\n然后，您可以将组件通过 components 变量传递到另一个 Pipeline ，而无需将权重重新加载到 RAM 中：\nstable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n\n如果希望更灵活地重用或禁用哪些组件，还可以将组件单独传递到新的Pipeline。\n例如，要在图像到图像 Pipeline 中重用文本到图像管道中的相同组件（安全检查器和特征提取器除外）：\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'stable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(    checkpoint_path,    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')stable_diffusion_img2img = StableDiffusionImg2ImgPipeline(    vae=stable_diffusion_txt2img.vae,    text_encoder=stable_diffusion_txt2img.text_encoder,    tokenizer=stable_diffusion_txt2img.tokenizer,    unet=stable_diffusion_txt2img.unet,    scheduler=stable_diffusion_txt2img.scheduler,    safety_checker=None,    feature_extractor=None,    requires_safety_checker=False,)\n\n\n\n检查点变体 Checkpoint variants检查点变体通常是权重为：\n\n存储在不同的浮点类型中，以实现较低的精度和较低的存储，例如torch.float16，因为它只需要一半的带宽和存储来下载。如果您正在继续训练或使用 CPU，则无法使用此变体。 \n非指数平均 (EMA) 权重，这种权重不适用于推理，用于继续微调模型。\n\n当检查点具有相同的模型结构，但它们在不同的数据集和不同的训练设置上进行训练时，它们应该存储在单独的存储库中而不是变体中\n变体与原始检查点完全相同。它们具有完全相同的序列化格式（如 Safetensors）、模型结构和具有相同形状张量的权重。\n\n\n\ncheckpoint类型\n权重文件名\n加载权重文件的参数\n\n\n\noriginal\ndiffusion_pytorch_model.bin\n\n\n\nfloating point（fp16）\ndiffusion_pytorch_model.fp16.bin\nvariant='fp16', torch_dtype=torch.float16\n\n\nnon-EMA\ndiffusion_pytorch_model.non_ema.bin\nvariant='non_ema'\n\n\n加载变体对于加载检查点变体，有两个重要的参数：\n\ntorch_dtype： 定义加载检查点的浮点精度。\n如果想通过加载 fp16 变体来节省带宽，则应指定 torch_dtype=torch.float16 将权重转换为 fp16。否则，fp16 权重将转换为默认的 fp32 精度。\n还可以在不定义该参数的情况下加载原始检查点，并使用 torch_dtype=torch.float16 将其转换为 fp16。在这种情况下，首先下载默认的 fp32 权重，然后在加载后将其转换为 fp16。\n\n\nvariant：定义应从存储库加载哪些文件。\n如果想加载 non_ema 变体，则应指定variant=\"non_ema 来下载 non_ema 文件。\n如果想加载 fp16 变体，则应指定variant=\"fp16 来下载半精度文件。\n\n\n\n保存变体要将检查点保存为不同浮点类型或作为非 EMA 变体，请使用 DiffusionPipeline.save_pretrained() 方法并指定 variant 参数。\nsave_path = './save_variant'# save as fp16 variantstable_diffusion.save_pretrained(save_path, variant=\"fp16\")# save as non-ema variantstable_diffusion.save_pretrained(save_path, variant=\"non_ema\")\n\n保存的文件结构与原始检查点的结构一致\nModels模型从 ModelMixin.from_pretrained() 方法加载。\n可以使用 subfolder 参数从子文件夹加载模型。例如，stable-diffusion-v1-5 的U-Net\n模型权重存储在 unet 子文件夹中：\nfrom diffusers import UNet2DConditionModelimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'model = UNet2DConditionModel.from_pretrained(    checkpoint_path,    subfolder='unet',    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')\n\n或者直接从存储库的目录加载：\nfrom diffusers import UNet2DModelrepo_id = \"google/ddpm-cifar10-32\"model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n\n还可以通过在 ModelMixin.from_pretrained() 和 ModelMixin.save_pretrained() 中指定变量参数来加载和保存变体模型：\nfrom diffusers import UNet2DConditionModelmodel = UNet2DConditionModel.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",     subfolder=\"unet\",     variant=\"non_ema\",     use_safetensors=True)model.save_pretrained(    \"./local-unet\",     variant=\"non_ema\")\n\n\n\n采样器采样器是从 SchedulerMixin.from_pretrained() 方法加载的，与模型不同，调度程序没有参数；它们由配置文件定义。\n加载采样器不会消耗任何大量的内存，并且相同的配置文件可以用于各种不同的采样器。\n例如，以下调度程序与 StableDiffusionPipeline 兼容，这意味着您可以在这些类中的任何一个中加载相同的调度程序配置文件：\nfrom diffusers import StableDiffusionPipelinefrom diffusers import (    DDPMScheduler,    DDIMScheduler,    PNDMScheduler,    LMSDiscreteScheduler,    EulerAncestralDiscreteScheduler,    EulerDiscreteScheduler,    DPMSolverMultistepScheduler,)import torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'ddpm = DDPMScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")ddim = DDIMScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")pndm = PNDMScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")lms = LMSDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")euler_anc = EulerAncestralDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")euler = EulerDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")dpm = DPMSolverMultistepScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")# replace `dpm` with any of `ddpm`, `ddim`, `pndm`, `lms`, `euler_anc`, `euler`pipeline = StableDiffusionPipeline.from_pretrained(    checkpoint_path,    scheduler=dpm,    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')\n\n\n\nDiffusionPipeline详解DiffusionPipeline.from_pretrained() 负责两件事：\n\n下载推理所需的最新版本的文件夹结构并将其缓存。如果本地缓存中有最新的文件夹结构，则重用缓存且不会重复下载文件。\n将缓存的权重加载到正确的 Pipeline 类中（从 model_index.json 文件中检索）并返回实例。\n\n Pipeline 的底层文件夹结构与其类实例直接对应。例如，StableDiffusionPipeline 对应于 runwayml/stable-diffusion-v1-5 中的文件夹结构。\nfrom diffusers import DiffusionPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16')print(pipeline)\n\n\n\n你会看到 pipeline 是 StableDiffusionPipeline 的一个实例，它由七个组件组成：\n\n\n\n组件\n类\n库\n\n\n\nfeature_extractor\nCLIPImageProcessor\nTransformers\n\n\nsafety_checker\nCLIPImageProcessor\ndiffusers\n\n\nscheduler\nPNDMScheduler\ndiffusers\n\n\ntext_encoder\nCLIPTextModel\nTransformers\n\n\ntokenizer\nCLIPTokenizer\nTransformers\n\n\nunet\nUNet2DConditionModel\ndiffusers\n\n\nvae\nAutoencoderKL\ndiffusers\n\n\nStableDiffusionPipeline {  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.27.2\",  \"_name_or_path\": \"D:\\\\MyCode\\\\Torch_Deom\\\\sd-v1.5\\\\sd-v1.5\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"image_encoder\": [    null,    null  ],  \"requires_safety_checker\": true,  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n将实例的组件与检查点文件夹结构进行比较，您将看到存储库中的每个组件都有一个单独的文件夹\n可以将管道的每个组件作为属性访问以查看其配置：\npipeline.tokenizer\n\n每个管道都需要一个 model_index.json 文件来告诉 DiffusionPipeline：\n\n从 _class_name 加载哪个管道类\n使用哪个版本的Diffusers 在 _diffusers_version 中创建模型\n子文件夹中存储了哪个库中的哪些组件\n\n{  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.6.0\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n\n\n\nLoad and compare different schedulers采样器定义整个去噪过程：\n\n去噪步数\n随机性还是确定性\n使用什么算法\n\n通常需要在去噪速度和去噪质量之间做出权衡。定量测量哪个采样器最适合给定的扩散Pipeline是极其困难的，因此通常建议简单地尝试哪个最适合。\n访问采样器以stable diffusion v1.5 为例，加载检查点并访问调度器\nfrom diffusers import DiffusionPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16')print(pipeline.scheduler)\n\n输出为：\nPNDMScheduler {  \"_class_name\": \"PNDMScheduler\",  \"_diffusers_version\": \"0.27.2\",  \"beta_end\": 0.012,  \"beta_schedule\": \"scaled_linear\",  \"beta_start\": 0.00085,  \"clip_sample\": false,  \"num_train_timesteps\": 1000,  \"prediction_type\": \"epsilon\",  \"set_alpha_to_one\": false,  \"skip_prk_steps\": true,  \"steps_offset\": 1,  \"timestep_spacing\": \"leading\",  \"trained_betas\": null}\n\n可以看到采样器的类为 PNDMScheduler。现在与其他采样器进行比较。首先，我们定义一个 prompt，我们将使用它测试所有不同的采样器：\nprompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\n\n接下来，我们从随机种子 4 中创建一个生成器，确保可以生成类似的图像：\ngenerator = torch.Generator(device=\"cuda\").manual_seed(4)image = pipeline(    prompt,    generator=generator).images[0]save_name = 'PNDMS-50.png'save_path = './results/schedulers'if(not os.path.exists(save_path)):    os.makedirs(save_path)image.save(os.path.join(save_path, save_name))\n\n\n更改采样器通过 compatibles 属性查看兼容的其他采样器：\nprint(pipeline.scheduler.compatibles)\n\n输出为：\n[&lt;class 'diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_pndm.PNDMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_edm_euler.EDMEulerScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler'&gt;, &lt;class 'diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler'&gt;]\n\n\n\n要更改管道的调度程序，可以结合使用方便的 config 属性和 from_config() 函数。\nprint(pipeline.scheduler.config)\n\nFrozenDict([    ('num_train_timesteps', 1000),     ('beta_start', 0.00085),     ('beta_end', 0.012),     ('beta_schedule', 'scaled_linear'),     ('trained_betas', None),     ('skip_prk_steps', True),     ('set_alpha_to_one', False),     ('prediction_type', 'epsilon'),     ('timestep_spacing', 'leading'),     ('steps_offset', 1),     ('_use_default_values', ['prediction_type', 'timestep_spacing']),     ('_class_name', 'PNDMScheduler'),     ('_diffusers_version', '0.27.2'),     ('clip_sample', False)])\n\n然后，可以使用此配置实例化一个与 Pipeline 兼容的不同的采样器。在这里，我们将调度程序更改为 DDIMScheduler。\nfrom diffusers import DDIMSchedulerpipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n\n\n比较采样器现在已经发布了一些更好的采样器，它们可以用更少的步骤运行；让我们在这里对它们进行比较：\n\nLMSDiscreteScheduler通常会带来更好的结果\n\nfrom diffusers import LMSDiscreteSchedulerpipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n\nLMSDiscreteScheduler 50步：\n\n\nEulerDiscreteScheduler 和 EulerAncestralDiscreteScheduler 仅需 30 个步骤即可生成高质量结果。\n\nfrom diffusers import EulerDiscreteSchedulerpipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\n1. `EulerDiscreteScheduler` 50步：\n\n\n\nEulerDiscreteScheduler 30步：\n\n\nfrom diffusers import EulerAncestralDiscreteSchedulerpipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\n 1. `EulerAncestralDiscreteScheduler` 50步：\n\n\n\nEulerAncestralDiscreteScheduler 30步：\n\n\n\nDPMSolverMultistepScheduler 给出了合理的速度和质量的权衡，并且可以用最少 20 个步骤运行。\n\nfrom diffusers import DPMSolverMultistepSchedulerpipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\n\nDPMSolverMultistepScheduler 50步：\n\n\n\nDPMSolverMultistepScheduler 20步：\n\n\n正如您所看到的，大多数图像看起来都非常相似，质量也可以说非常接近。选择哪种通常取决于具体的使用情况。最好的方法是运行多个不同的采样器来比较结果。\nLoad community pipelines and components社区pipeline是指不同于其论文中指定的原始实现的任何 DiffusionPipeline 类（例如，StableDiffusionControlNetPipeline 与 ControlNet 论文中的文本到图像生成相对应）。它们提供了额外的功能或扩展了pipeline的原始实现。\n\n社区pipeline示例：diffusers/examples/community at main · huggingface/diffusers (github.com)\n\n要在 Hub 上加载任何社区管道，请在 custom_pipeline 参数中输入社区pipeline的资源库 id，以及要加载管道权重和组件的模型资源库。例如，下面的示例从hf-internal-testing/diffusers-dummy-pipeline以及Google/DDPM-CIFAR10-32的管道权重和组件加载了一个 pipeline：\nfrom diffusers import DiffusionPipelinepipeline = DiffusionPipeline.from_pretrained(    \"google/ddpm-cifar10-32\",     custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\",     use_safetensors=True)\n\n\n\n加载官方社区pipeline可以混合加载来自官方资源库的权重，并直接传递管道组件。下面的示例加载了社区pipeline CLIP Guided Stable Diffusion ，你可以直接将 CLIP 模型组件传递给它：\nfrom diffusers import DiffusionPipelinefrom transformers import CLIPImageProcessor, CLIPModelclip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)clip_model = CLIPModel.from_pretrained(clip_model_id)pipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",    custom_pipeline=\"clip_guided_stable_diffusion\",    clip_model=clip_model,    feature_extractor=feature_extractor,    use_safetensors=True,)\n\n\n\n从本地文件加载如果您通过文件路径，也可以从本地文件加载社区 pipeline。文件路径必须包含一个pipeline.py文件，该文件包含pipeline类。\npipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",    custom_pipeline=\"./path/to/pipeline_directory/\",    clip_model=clip_model,    feature_extractor=feature_extractor,    use_safetensors=True,)\n\n从特定版本加载默认情况下，社区 pipeline 是从最新稳定版本的 Diffusers 中加载的。要从另一个版本加载社区管道，请设置 custom_revision 参数。\npipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",    custom_pipeline=\"clip_guided_stable_diffusion\",    custom_revision=\"main\", # 或\"v0.25.0\"等特定版本    clip_model=clip_model,    feature_extractor=feature_extractor,    use_safetensors=True,)\n\n\n\nLoad safetensors\nhuggingface/safetensors: Simple, safe way to store and distribute tensors (github.com)\n\n是一种安全、快速的文件格式，用于存储和加载张量。通常情况下，PyTorch 模型权重会通过 Python 的 pickle 工具保存到 .bin 文件中。然而，pickle 并不安全，文件可能包含可被执行的恶意代码。safetensors 是 pickle 的安全替代品，是共享模型权重的理想选择。\n接下来介绍如何加载 .safetensor 文件，以及如何将以其他格式存储的Stable Diffusion模型权重转换为 .safetensor 格式。开始之前，请确保已安装 safetensors：\npip install safetensors\n\n默认情况下（如runwayml/stable-diffusion-v1-5），如果模型库中有这些 .safetensors 文件，Diffusers 会自动从它们的子文件夹中加载这些文件。\n要实现更明确的控制，可以选择设置 use_safetensors=True（如果未安装 safetensors，则会收到要求安装的错误信息）：\nfrom diffusers import DiffusionPipelinepipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",     use_safetensors=True)\n\n不过，模型权重并不一定像上例那样存储在单独的子文件夹中。有时，所有权重都存储在一个 .safetensors 文件中。在这种情况下，如果权重是稳定扩散权重，则可以使用 from_single_file() 方法直接加载该文件：\nfrom diffusers import StableDiffusionPipelinepipeline = StableDiffusionPipeline.from_single_file( \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\")\n\n转换为safetensors并非 Hub 上的所有权重都是 .safetensors 格式，您可能会遇到以 .bin 格式存储的权重。在这种情况下，请使用 Convert Space 将权重转换为 .safetensors 格式。\nConvert Space会下载权重并进行转换，然后打开一个Pull Request，将新转换的 .safetensors 文件上传到集线器上。这样，如果腌制文件中包含任何恶意代码，它们就会被上传到Hub，而不是你的电脑，因为Hub有一个security scanner来检测不安全的文件。\n为什么要使用safetensors？使用 safetensors 有几个原因：\n\n安全是使用 safetensors 的首要原因。随着开源和模型分发的发展，确保您下载的模型权重不包含任何恶意代码非常重要。目前 safetensors 中标头的 current size 会阻止解析超大 JSON 文件。\nsafetensors 也支持延迟加载（Lazy loading），这在分布式设置中非常有用，可以只加载部分张量。通过这种格式，在 8 个 GPU 上加载 BLOOM 模型只需 45 秒，而使用普通 PyTorch 权重则需要 10 分钟。\n切换模型之间的加载速度是使用 safetensors 的另一个原因，它会对张量执行零拷贝。如果将权重加载到 CPU（默认情况），它的速度比 pickle 更快，而直接将权重加载到 GPU 时，也同样快。\n\n加载整个pipeline所需的时间：\nfrom diffusers import StableDiffusionPipelinepipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", use_safetensors=True)\"Loaded in safetensors 0:00:02.033658\"\"Loaded in PyTorch 0:00:02.663379\"\n\n但加载 500MB 模型权重的实际时间仅为：\nsafetensors: 3.4873msPyTorch: 172.7537ms\n\n\n\nLoad different Stable Diffusion formatsStable Diffusion 模型有不同的格式，这取决于它们是用什么框架训练和保存的，以及从哪里下载的。转换这些格式以便在 Diffusers 中使用，可以让您使用该库支持的所有功能，例如：\n\n使用不同的采样器进行推理\n构建自定义管道\n优化推理速度\n\n接下来介绍如何转换其他格式以与 Diffusers 兼容。\nPyTorch .ckpt在 PyTorch 中 checkpoint（或 .ckpt）格式通常用于保存模型。.ckpt 文件包含整个模型。虽然可以使用 from_single_file() 方法直接加载和使用，但通常最好将 .ckpt 文件转换为 Diffusers 推荐的 safetensors 格式。\n转换 .ckpt 文件有两种方式：\n\n使用 Space 转换检查点\n使用脚本转换 .ckpt 文件\n\n用Space转换\nSD To Diffusers - a Hugging Face Space by diffusers\n\n转换 .ckpt 文件最简单方便的方法是使用 SD To Diffusers Space。\n这种方法对基本模型很有效，但对更多定制模型可能会有困难。如果 Space 返回 empty pull request或erroe，可以尝试用脚本转换 .ckpt 文件。\n用脚本转换\ndiffusers/scripts/convert_original_stable_diffusion_to_diffusers.py at main · huggingface/diffusers (github.com)\n\n这种方法比上面的Space更可靠，需要设定如下参数：\n\ncheckpoint_path：要转换的 .ckpt 文件的路径。\noriginal_config_file：定义原始架构配置的 YAML 文件。如果找不到此文件，请尝试在找到 .ckpt 文件的 GitHub 存储库中搜索 YAML 文件。\ndump_path：转换后模型的保存路径\n\nA1111 LoRA filesAutomatic1111 (A1111) 是一种流行的Stable Diffusion Web UI，支持Civitai等模型共享平台。\nDiffusers 支持使用 load_lora_weights() 将 LoRA 检查点加载到 pipeline 中：\npipeline.load_lora_weights(    \"LoRA/civitai_LoRA\",     weight_name=\"bl3uprint.safetensors\")save_name = 'lora-blueprint.png'prompt = \"bl3uprint, a highly detailed blueprint of the empire state building, explaining how to build all parts, many txt, blueprint grid backdrop\"negative_prompt = \"lowres, cropped, worst quality, low quality, normal quality, artifacts, signature, watermark, username, blurry, more than one bridge, bad architecture\"image = pipeline(    prompt=prompt,    negative_prompt=negative_prompt,    generator=torch.manual_seed(0),).images[0]save_path = './results/LoRA'image.save(os.path.join(save_path, save_name))\n\n\nLoad adapters有多种训练技术可用于个性化扩散模型以生成特定主题的图像或某些风格的图像。每种训练方法都依赖不同类型的 Adapter。有些适配器会生成一个全新的模型，而其他适配器则只修改较小的嵌入或权重集。这意味着每个适配器的加载过程也不尽相同。\n接下来介绍如何加载 DreamBooth、Textual inversion和 LoRA 权重。\nDreamBooth\nsd-dreambooth-library (Stable Diffusion Dreambooth Concepts Library) (huggingface.co)\n\nDreamBooth 只需在几张同一主题或风格的图像上微调整个扩散模型，就能生成新的同一主题或风格的图像。\n这种方法的原理是在提示中使用一个特殊的提示词，让模型学会将该单词与主题图像联系起来。在所有训练方法中，DreamBooth 产生的文件尺寸最大（通常为几 GB），因为它是一个完整的检查点。\n加载方式和普通pipeline类似：\n让我们加载 sd-dreambooth-library/pikachu 检查点，它只对 9 张由 皮卡丘 的图像进行训练。为了让它正常工作，需要在提示中加入 pikachu 这个特殊的单词来触发检查点：\nfrom diffusers import AutoPipelineForText2Imageimport torchcheckpoint_path = 'DreamBooth/pikachu'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16).to(\"cuda\")prompt = \"A pikachu is playing the guitar, masterpiece\"image = pipeline(prompt).images[0]image.save('./result/pikachu_DB.png')\n\n\n\n\n\nTextual inversion\nsd-concepts-library (Stable Diffusion concepts library) (huggingface.co)\n\n文本反转与 DreamBooth 非常相似，从几张图片中学习生成特定的概念（风格、对象）。这种方法的工作原理是通过训练和寻找新的 embedding，用提示中的一个特殊单词来代表你提供的图片。因此，扩散模型的权重保持不变，训练过程产生的文件也相对较小（几 KB）。\n由于文本反转会创建 embedding，因此它不能像 DreamBooth 那样单独使用，而需要依附于另一个扩散模型，这里使用 SD1.5：\nfrom diffusers import AutoPipelineForText2Imageimport torch, oscheckpoint_path = 'sd-v1.5/sd-v1.5'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16',    use_safetensors=True,    safety_checker=None,).to(\"cuda\")\n\n现在，你可以使用 load_textual_inversion() 方法加载文本反转embedding来生成图像。让我们加载 sd-concepts-library/gta5-artwork 嵌入，然后在提示中包含特殊字 &lt;gta5-artwork&gt; 来触发它：\nseed = 0save_name = (f'gta5-artwork-TI-{seed}.png')pipeline.load_textual_inversion(\"Textual_inversion/gta5-artwork\")prompt = \"a man is playing the guitar, masterpiece, best quality, &lt;gta5-artwork&gt; style\"image = pipeline(    prompt,    num_inference_steps=25,    generator=torch.manual_seed(seed)).images[0]save_path = './results/Textual_inversion'if not os.path.exists(save_path):    os.makedirs(save_path)image.save(os.path.join(save_path, save_name))\n\n文本反转也可以针对不需要的事物进行训练，以创建negative embeddings，阻止模型生成含有我们不希望出现内容的图像，如带有模糊的图像或人物手指数量异常的图像。\n这是快速改进 prompt 的简单方法。可以使用 load_textual_inversion() 加载embedding，设定这两个参数：\n\nweight_name：要加载的权重文件\ntoken：在 prompt 中的触发词\n\nseed = 0save_name = (f'gta5-artwork-TI-EN-{seed}.png')pipeline.load_textual_inversion(\"Textual_inversion/gta5-artwork\")# 载入 EasyNegativepipeline.load_textual_inversion(    \"Textual_inversion\",    weight_name=\"EasyNegative.safetensors\",    token=\"EasyNegative\")prompt = \"A man is playing the guitar, masterpiece, best quality, &lt;gta5-artwork&gt; style, EasyNegative\"negative_prompt = \"EasyNegative\"image = pipeline(    prompt,    negative_prompt=negative_prompt,    num_inference_steps=25,    generator=torch.manual_seed(seed),).images[0]\n\n\n\nLoRALow-Rank Adaptation（LoRA）是一种流行的微调技术，因为它速度快，生成的文件较小（几百 MB）。从几张图像中学习新的风格。\n它的工作原理是在扩散模型中插入新的权重，然后只训练新权重而不是整个模型。这使得 LoRA 的训练速度更快，也更易于存储。\n\nLoRA 是一种非常通用的训练技术，可以与其他训练方法一起使用。\n使用 DreamBooth 和 LoRA 训练模型就很常见。\n\nLoRA 也需要依附于一个扩散模型使用，这里使用SDXL：\nfrom diffusers import AutoPipelineForText2Imageimport torch, oscheckpoint_path = 'SDXL/stable-diffusion-xl-base-1.0'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16',    use_safetensors=True,    safety_checker=None,).to(\"cuda\")\n\n然后使用 load_lora_weights() 方法加载 Pixel Art LoRA | Civitai 权重并指定权重文件名，命名这个 LoRA 为 pixel：\npipeline.load_lora_weights(    \"LoRA/civitai_LoRA\",    weight_name=\"pixel_stormXL.safetensors\",    adapter_name=\"pixel\")seed = 0save_name = (f'pixel-LoRA-{seed}.png')prompt = \"A man is playing the guitar, masterpiece, best quality\"image = pipeline(    prompt,    num_inference_steps=30,    generator=torch.manual_seed(seed),).images[0]save_path = './results/LoRA'if not os.path.exists(save_path):    os.makedirs(save_path)image.save(os.path.join(save_path, save_name))\n\nload_lora_weights() 方法可将 LoRA 权重加载到 UNet 和文本编码器中。通过 get_list_adapters() 方法查看加载位置：\nlist_adapters_component_wise = pipeline.get_list_adapters()print(list_adapters_component_wise){'text_encoder': ['pixel'], 'text_encoder_2': ['pixel'], 'unet': ['pixel']}\n\n这是加载 LoRA 的首选方法，因为它可以处理以下情况\n\nLoRA 权重在 UNet 和文本编码器中没有单独的标识符\nLoRA 权重在 UNet 和文本编码器中有单独的标识符\n\n但如果你只需要将LoRA权重加载到UNet中，那么你可以使用 load_attn_procs()方法。\npipeline.unet.load_attn_procs(\t\"LoRA/civitai_LoRA\",    weight_name=\"pixel_stormXL.safetensors\",)\n\n要卸载 LoRA 权重，请使用 unload_lora_weights() 方法丢弃 LoRA 权重并将模型恢复为其原始权重：\npipeline.unload_lora_weights()\n\n\n\nIP-Adapter\nh94/IP-Adapter · Hugging Face\n\nIP-Adapter 是一种轻量级Adapter，可为任何扩散模型提供图像 prompt。该适配器通过解耦图像和文本特征的交叉注意层来工作。所有其他模型组件都被冻结，只有 UNet 中的embedded image features会被训练。因此，IP-Adapter 文件大小通常只有约 100MB。\n首先，加载 Stable Diffusion 检查点。\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForText2Image, DPMSolverMultistepSchedulerimport torchfrom diffusers.utils import load_imagecheckpoint_path = 'sd-v1.5/sd-v1.5'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16',    use_safetensors=True,    safety_checker=None,).to(\"cuda\")\n\n然后使用 load_ip_adapter() 方法将其添加到 pipeline 中。\npipeline.load_ip_adapter(\"LoRA/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.safetensors\")\n\n加载后，您可以使用带有图像和文本提示的管道来指导图像生成过程。\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")generator = torch.Generator(device=\"cpu\").manual_seed(33)images = pipeline(    prompt='best quality, high quality, wearing sunglasses',    ip_adapter_image=image,    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",    num_inference_steps=50,    generator=generator,).images[0]images.save('./results/IP-Adapter.png')\n\n\nIP-Adapter PlusIP-Adapter 依靠图像编码器生成图像特征。如果 IP-Adapter 资源库包含 image_encoder 子文件夹，图像编码器就会自动加载并注册到 pipeline 中。否则，就需要使用 CLIPVisionModelWithProjection 模型显式加载图像编码器，并将其传递给 pipeline。\nfrom transformers import CLIPVisionModelWithProjection# 显式加载图像编码器image_encoder = CLIPVisionModelWithProjection.from_pretrained(    \"h94/IP-Adapter\",    subfolder=\"models/image_encoder\",    torch_dtype=torch.float16)pipeline = AutoPipelineForText2Image.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    image_encoder=image_encoder,    torch_dtype=torch.float16).to(\"cuda\")pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\")\n\n\n\nPush files to the Hub待定\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"Accelerate基本使用方法","url":"/2024/04/19/Accelerate%20base/","content":"Accelerate-NoteAccelerate 是一个由 Hugging Face 开发的 Python 库，它允许开发者将相同的 PyTorch 代码运行在任何分布式配置上，只需添加四行代码即可。这个库简化了在多种环境（包括单机、多 GPU、TPU 和各种分布式训练环境）中进行深度学习训练的过程。它基于 torch_xla 和 torch.distributed 构建，可以轻松地将现有代码库转换为使用 DeepSpeed 进行全分片数据并行处理，并自动支持混合精度训练。\n将Accelerate添加到PyTorch代码中Accelerate 提供了一种友好的方式来适应各类分布式框架，而无需学习每个框架的具体细节。\n接下来将从一个基本的 PyTorch 训练循环开始（假设模型和优化器等所有训练对象都已设置完毕），然后逐步将 Accelerate 集成到其中。\n# 初始化model = ...                # torch.nn.Moduletraining_dataloader = ...  # torch.utils.data.DataLoaderoptimizer = ...            # torch.optimscheduler = ...            # torch.optim.lr_schedulerdevice = \"cuda\"model.to(device)for batch in training_dataloader:    optimizer.zero_grad()    inputs, targets = batch    inputs = inputs.to(device)    targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)    loss.backward()    optimizer.step()    scheduler.step()\n\n完整实例如下：\nimport torchfrom torch import nnfrom torch import optimimport torch.nn.functional as Ffrom torch.optim import lr_schedulerfrom torch.utils.data import DataLoaderfrom torch.utils.data import random_splitimport torchvision.datasets as datasetsimport torchvision.transforms as transformsfrom torch.utils.tensorboard import SummaryWriterfrom tqdm import tqdmdef check_accuracy(loader, model):    num_correct = 0    num_samples = 0    model.eval()    with torch.no_grad():        # Loop through the data        for x, y in loader:            # Move data to device            x = x.to(device=device)            y = y.to(device=device)            # Get to correct shape            x = x.reshape(x.shape[0], -1)            # Forward pass            scores = model(x)            _, predictions = scores.max(1)            # Check how many we got correct            num_correct += (predictions == y).sum()            # Keep track of number of samples            num_samples += predictions.size(0)    model.train()    return num_correct / num_samples# 超参数设置input_size = 784num_classes = 10learning_rate = 0.001batch_size = 64num_epochs = 10sw = SummaryWriter('./mnist-logs')# 0. 设备设置device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 1. 模型class NN(nn.Module):    def __init__(self, input_size, num_classes):        super().__init__()        self.fc1 = nn.Linear(input_size, 50)        self.fc2 = nn.Linear(50, num_classes)    def forward(self, x):        x = F.relu(self.fc1(x))        x = self.fc2(x)        return xmodel = NN(input_size=input_size, num_classes=num_classes).to(device)# 2. 数据加载器entire_dataset = datasets.MNIST(    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)train_ds, val_ds = random_split(entire_dataset, [50000, 10000])test_ds = datasets.MNIST(    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)val_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)# 3.优化器和调度器optimizer = optim.Adam(    model.parameters(),    lr=learning_rate)scheduler = lr_scheduler.CosineAnnealingWarmRestarts(    optimizer,    T_0=2,    eta_min=1e-6)# 损失函数loss_func = nn.CrossEntropyLoss()# 训练循环len_per_epoch = len(train_loader)global_step = 0for epoch in range(num_epochs):    for i, (data, targets) in enumerate(tqdm(train_loader)):        # 清空梯度        optimizer.zero_grad()        # 数据移至device        data = data.to(device=device)        targets = targets.to(device=device)        # 计算损失        data = data.reshape(data.shape[0], -1)        scores = model(data)        loss = loss_func(scores, targets)        # Backward        loss.backward()        # Gradient descent or adam step        optimizer.step()        scheduler.step(epoch + i / len_per_epoch)        global_step += 1        sw.add_scalar('lr', scheduler.get_last_lr()[0], global_step=global_step)    val_accuracy = check_accuracy(val_loader, model) * 100    sw.add_scalar('val_accuracy', val_accuracy, global_step=epoch)# Check accuracy on training &amp; test to see how good our modelmodel.to(device)print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")print(f\"Accuracy on validation set: {check_accuracy(val_loader, model)*100:.2f}\")print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n\nAcceleratorAccelerator 是用于调整代码以与 Accelerate 配合使用的主类。它了解你正在使用的分布式设置，如不同进程的数量和硬件类型。使您的 PyTorch 代码能够在任何分布式训练环境中工作，并管理和执行跨设备进程。\n从导入和创建 Accelerator 实例开始：\nfrom accelerate import Acceleratoraccelerator = Accelerator()\n\nAccelerator 知道将 PyTorch 对象移动到哪个设备上，因此建议让 Accelerate 设置 device 参数：\n- device = \"cuda\"+ device = accelerator.device  model.to(device)\n\n准备 PyTorch 对象接下来，您需要为分布式训练准备好 PyTorch 对象（模型、优化器、调度器、数据加载器等）。\nprepare() 方法会根据训练设置将模型放入适当的容器（如单GPU或多GPU）中，调整优化器和调度器以使用 Accelerate 的 AcceleratedOptimizer 和 AcceleratedScheduler，并创建可跨进程分片的新数据加载器。\n\nprepare() 方法仅准备从 PyTorch 类继承的对象，例如 torch.optim.Optimizer。\n\nPyTorch 对象的返回顺序与输入顺序相同：\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(    model, optimizer, training_dataloader, scheduler)\n\n\n\n训练循环最后，删除训练循环中对输入和目标数据的 to(device) 调用，因为 Accelerate 的 DataLoader 类会自动将它们放置在正确的设备上。\n您还应该将通常的loss.backward()传递替换为 Accelerate 的backward()方法，该方法会为您缩放梯度，并根据您的分布式设置（例如，DeepSpeed或Megatron）使用适当的backward()方法。\n-   inputs = inputs.to(device)-   targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)-   loss.backward()+   accelerator.backward(loss)\n\n\n\n总结将所有内容放在一起，您的 Accelerate 训练循环现在应该如下所示！\nfrom accelerate import Acceleratoraccelerator = Accelerator()# 初始化model = ...                # torch.nn.Moduletraining_dataloader = ...  # torch.utils.data.DataLoaderoptimizer = ...            # torch.optimscheduler = ...            # torch.optim.lr_schedulerdevice = accelerator.devicemodel, optimizer, training_dataloader, scheduler = accelerator.prepare(    model, optimizer, training_dataloader, scheduler)for batch in training_dataloader:    optimizer.zero_grad()        inputs, targets = batch        outputs = model(inputs)    loss = loss_function(outputs, targets)        accelerator.backward(loss)        optimizer.step()    scheduler.step()\n\n\n\nTraining featuresAccelerate 提供了额外的功能，例如梯度累积 (gradient accumulation)、梯度裁剪 (gradient clipping)、混合精度训练 (mixed precision training)等，您可以将其添加到脚本中以改进训练。\n梯度累积梯度累积使您能够在更新权重之前通过累积多个批次的梯度来获取更大的等效 batch_size。这对于解决显存对 batch_size 的限制很有用。\n要在 Accelerate 中启用此功能，请在加速器类中指定 gradient_accumulation_steps 参数，并在脚本中添加 accumulate() 上下文管理器：\n+ accelerator = Accelerator(gradient_accumulation_steps=2)  model, optimizer, training_dataloader = accelerator.prepare(      model, optimizer, training_dataloader  )  for input, label in training_dataloader:+     with accelerator.accumulate(model):          predictions = model(input)          loss = loss_function(predictions, label)          accelerator.backward(loss)          optimizer.step()          scheduler.step()          optimizer.zero_grad()\n\n梯度裁剪梯度裁剪是一种防止“梯度爆炸”的技术，Accelerate 提供以下两种方法：\n\nclip_grad_value_：将可迭代参数的梯度裁剪为指定值。 梯度就地修改（in-place）。\nparametres：可迭代的张量或单个张量，其梯度将归一化\nclip_value：梯度的阈值。梯度被限制在范围内\n\n\nclip_grad_norm_：范数是对所有梯度一起计算的。梯度就地修改。\nparameters：可迭代的张量或单个张量，其梯度将归一化\nmax_norm：梯度的最大范数\nnorm_type：float，默认为2.0，用的 p-范数的类型。inf表示无穷范数。\n\n\n\nfrom accelerate import Acceleratoraccelerator = Accelerator(gradient_accumulation_steps=2)dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)for input, target in dataloader:     optimizer.zero_grad()     output = model(input)     loss = loss_func(output, target)     accelerator.backward(loss)     if accelerator.sync_gradients:     # 二者取其一：    \taccelerator.clip_grad_value_(model.parameters(), clip_value)        accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)             optimizer.step()\n\n\n\n混合精度训练混合精度通过使用 fp16（半精度）等较低精度的数据类型来计算梯度，从而加速训练。要想使用 Accelerate 获得最佳性能，应在模型内部计算损失（如在 Transformers 模型中），因为模型外部的计算是以全精度进行的。\n设置要在 accelerater 中使用的混合精度类型，然后使用 autocast() 上下文管理器将值自动转换为指定的数据类型。\nfrom accelerate import Accelerator+ accelerator = Accelerator(mixed_precision=\"fp16\")+ with accelerator.autocast():      loss = complex_loss_function(outputs, target):\n\n\n\n保存和加载训练完成后，加速还可以保存和加载模型，或者您还可以保存模型和优化器状态（optimizer state），这对于恢复训练很有用。\n模型所有过程完成后，在保存模型前使用 unwrap_model() 方法解除模型的封装，因为训练开始前执行的 prepare() 方法将模型封装到了适合的分布式训练接口中。如果不解除对模型的封装，保存模型状态字典的同时也会保存大模型中任何潜在的额外层，这样就无法将权重加载回基础模型中。\n使用 save_model() 方法来解包并保存模型状态字典。此方法还可以将模型保存到切片检查点 sharded checkpoints 或safetensors格式中。\naccelerator.wait_for_everyone()accelerator.save_model(model, save_directory)\n\n\n对于 Transformers 库中的模型，请使用 save_pretrained 方法保存模型，以便可以使用 from_pretrained方法重新加载。\nfrom transformers import AutoModelunwrapped_model = accelerator.unwrap_model(model)unwrapped_model.save_pretrained(    \"path/to/my_model_directory\",    is_main_process=accelerator.is_main_process,    save_function=accelerator.save,)model = AutoModel.from_pretrained(\"path/to/my_model_directory\")\n\n要加载权重，请在加载权重之前先使用 unwrap_model() 方法解包模型。所有模型参数都是对张量的引用，因此这会将您的权重加载到模型中。\nunwrapped_model = accelerator.unwrap_model(model)path_to_checkpoint = os.path.join(save_directory,\"pytorch_model.bin\")unwrapped_model.load_state_dict(torch.load(path_to_checkpoint))\n\n切片检查点设置 safe_serialization=True 将模型保存为 safetensor 格式。\naccelerator.wait_for_everyone()accelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)\n\n要加载分片检查点或 safetensor 格式的检查点，请使用 load_checkpoint_in_model() 方法。此方法允许您将检查点加载到特定设备上。\nload_checkpoint_in_model(unwrapped_model, save_directory, device_map={\"\":device})\n\n\n\n状态在训练过程中，你可能希望保存模型、优化器、随机生成器以及学习率调度器的当前状态，以便在同一个脚本中恢复它们。你应该在脚本中添加 save_state() 和 load_state() 方法来保存和加载状态。\n任何其他需要存储的有状态项目都应使用 register_for_checkpointing() 方法注册，以便保存和加载。传递给此方法的每个要存储的对象都必须具有 load_state_dict 和 state_dict 函数。\n执行进程在使用分布式训练系统时，管理跨 GPU 执行流程的方式和时间非常重要。有些进程比其他进程完成得更快，有些进程在其他进程尚未完成时就不应开始。Accelerate 提供了用于协调进程执行时间的工具，以确保所有设备上的一切保持同步。\n在一个进程上执行某些代码只需在特定机器上运行一次，如打印日志语句或只在本地主进程上显示一个进度条。\nstatement应使用 accelerator.is_local_main_process 来指示只应执行一次的代码。\n\naccelerator.is_local_main_process ：\n\n用于判断当前进程是否是本地节点（服务器）上的主进程，\n如果你的训练任务在多台服务器上运行，每台服务器都有一个主进程。is_local_main_process() 如果返回 True，表示当前进程是本地节点上的主进程。\n通常，你可以在本地节点的主进程上执行一些只需执行一次的操作，例如初始化数据、加载预训练模型等。\n\n\nfrom tqdm.auto import tqdmprogress_bar = tqdm(    range(args.max_train_steps),     disable=not accelerator.is_local_main_process)\n\n还可以使用 accelerator.is_local_main_process 包装语句。\nif accelerator.is_local_main_process:    print(\"Accelerate is the best\")\n\n\n\n还可以指示 Accelerate 在所有进程中都要执行一次的代码，而不管有多少台机器。如果您要将最终模型上传到 Hub，这将非常有用。\n\naccelerator.is_main_process：\n\n这个函数用于判断当前进程是否是整个训练任务中的主进程。\n主进程通常负责一些全局操作，例如模型保存、日志记录等。因此，你可以使用 is_main_process() 来确保这些操作只在主进程中执行一次。\n如果你的训练任务在多台服务器上运行，is_main_process() 将返回 True，只有一个服务器上的主进程会满足这个条件。\n\n\nif accelerator.is_main_process:    repo.push_to_hub()\n\n\n\nfunction对于只应执行一次的函数，请使用 on_local_main_process 装饰器。\n@accelerator.on_local_main_processdef do_my_thing():    \"Something done once per server\"    do_thing_once_per_server()\n\n对于只应在所有进程中执行一次的函数，请使用 on_main_process 装饰器。\n@accelerator.on_main_processdef do_my_thing():    \"Something done once per server\"    do_thing_once()\n\n\n\n\n\n在特定进程上执行Accelerate 还可以执行只应在特定进程或本地进程索引上执行的函数。\n使用 on_process() 装饰器指定要执行函数的进程索引。\n@accelerator.on_process(process_index=0)def do_my_thing():    \"Something done on process index 0\"    do_thing_on_index_zero()\n\n使用 on_local_process() 装饰器指定要执行函数的本地进程索引。\n@accelerator.on_local_process(local_process_idx=0)def do_my_thing():    \"Something done on process index 0 on each server\"    do_thing_on_index_zero_on_each_server()\n\n\n\n推迟执行当同时在多个 GPU 上运行脚本时，某些代码的执行速度可能会比其他代码快。在执行下一组指令之前，您可能需要等待所有进程都达到一定程度。例如，在确保每个进程都完成训练之前，您不应该保存模型。\n为此，请在代码中添加 wait_for_everyone()。这会阻止所有先完成训练的进程继续训练，直到所有剩余进程都达到相同点（如果在单个 GPU 或 CPU 上运行，则没有影响）。\naccelerator.wait_for_everyone()\n\n\n\n启动Accelerate脚本首先，将训练代码重写为函数，并使其可作为脚本调用。例如：\n  from accelerate import Accelerator  + def main():      accelerator = Accelerator()      model, optimizer, training_dataloader, scheduler = accelerator.prepare(          model, optimizer, training_dataloader, scheduler      )      for batch in training_dataloader:          optimizer.zero_grad()          inputs, targets = batch          outputs = model(inputs)          loss = loss_function(outputs, targets)          accelerator.backward(loss)          optimizer.step()          scheduler.step()+ if __name__ == \"__main__\":+     main()\n\n\n\n然后，在命令行使用 accelerate config 来配置accelerate的运行环境\n\nThe Command Line (huggingface.co)\n\n使用 accelerate launchAccelerate 有一个特殊的 CLI 命令，可帮助您通过加速启动在系统中启动代码。该命令包含在各种平台上启动脚本所需的所有不同命令\n使用以下命令快速启动脚本：\naccelerate launch --accelerate-arg {script_name.py} --script-arg1 --script-arg2 ...\n\n由于这会运行各种 torch 生成方法，因此也可以在此处修改所有环境变量。例如，以下是如何使用单个 GPU ：\nCUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...\n\n您也可以使用 accelerate launch，而无需先执行 accelerate config，但可能需要手动输入正确的配置参数。在这种情况下，Accelerate 会为你做出一些超参数决定，例如，如果 GPU 可用，它会默认使用所有 GPU，不使用混合精度。\n指定要使用的 GPU 数量：\naccelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\n使用混合精度在两个 GPU 上启动相同的脚本\naccelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\n要获取可以传入的参数的完整列表，请运行：\naccelerate launch -h\n\n从该自定义 yaml 文件启动脚本如下所示：\naccelerate launch --config_file {path/to/config/my_config_file.yaml} {script_name.py} {--arg1} {--arg2} ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["Tech"],"tags":["python","Training-Trick"]},{"title":"Log","url":"/2024/08/01/Log/","content":"\n      \n        1f455657f328e3db0faa540e3c291d18e222c6ff6344f61ef5ba21ba3cf38c2480e8920cfc8beceb13c6efdc9b51554c08464845f0298c86f5b3edbf641ce8c30e7a8941f89ac280b7861824e9b5faeb1caff0d68e52b2bd5ee0b267546636472047eabc64501582445735757460dd2ec42d987a614b294545bda32d6c6000623fbafa7f92473ed0748b00d2e87544d0\n      \n      \n        \n          \n          \n            请输入与 Rhodes Island™ 取得弱神经连接时的口令：\n          \n        \n        \n      \n    \n    "},{"url":"/2024/09/01/NTIRE%20Com/","content":"\nNight Photography Rendering Challenge (nightimaging.org)\nBlind Compressed Image Enhancement Challenge CodaLab - Competition (upsaclay.fr)\nRAW Image Super Resolution Challenge CodaLab - Competition (upsaclay.fr)\nReal-Time Image Super-Resolution CodaLab - Competition (upsaclay.fr)\n\n"},{"url":"/2024/10/07/BSR/","content":"\n\n\n\nDeg\nparm\np\n\n\n\n\nBlur1\nkernel-size&#x3D;[7,21] $\\sigma$&#x3D;[0.2, 3], $\\beta$&#x3D;[0.5, 4]\n1.0\n\n\n\n2D sinc\nNone\n0.1\n\n\n\nResize1\n\n1.0\n\n\n\nNoise1\nG[1, 30]  P[0.05, 3]\n0.5-0.5\n\n\n\nGray-Noise\n\n0.4\n\n\n\nJPEG1\n[30, 95]\n1.0\n\n\n\nBlur2\nkernel-size&#x3D;[7,21] $\\sigma$&#x3D;[0.2, 1.5]\n0.8\n\n\n\nResize2\n\n1.0\n\n\n\nNoise2\nG[1, 25]  P[0.05, 2.5]\n0.5-0.5\n\n\n\nJPEG2\n[30, 95]\n1.0\n\n\n\n2D sinc\nNone\n0.8\n\n\n\n"},{"url":"/2024/09/04/SR-Dataset&Method/","content":"超分数据集Train Set\n\n\nName\nHomepage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Set\n\n\nName\nHomepage\nInfo\nPaper\n\n\n\nRealSRSet\nBSRGAN&#x2F;testsets&#x2F;RealSRSet\n20 real low-resolution images\n[2103.14006] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution (arxiv.org)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethodSurvey[2107.03055] Blind Image Super-Resolution: A Survey and Beyond (arxiv.org)\nReal-ESRGAN:\nlink: [2107.10833] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data (arxiv.org)\ngithub: xinntao&#x2F;Real-ESRGAN: Real-ESRGAN aims at developing Practical Algorithms for General Image&#x2F;Video Restoration. (github.com)\n\n1. Classical Degradation Model真实图像 $y$ 首先与模糊核 $k$ 进行卷积。然后，执行比例因子为 $r$ 的下采样操作。添加噪声 $n$ 。最后，还采用了广泛用于现实世界的图像的 JPEG 压缩$$\\boldsymbol{x}&#x3D;\\mathcal{D}(\\boldsymbol{y})&#x3D;\\left[(\\boldsymbol{y} \\circledast \\boldsymbol{k}) \\downarrow_{r}+\\boldsymbol{n}\\right]_{\\mathrm{JPEG}}$$\n2. High orderDateset\nDIV2K\nFlickr2K\nOutdoorSceneTraining\n\nDetail\n基于 XPixelGroup&#x2F;BasicSR 开发\n引入 exponential moving average (EMA) 稳定训练\n模板：xinntao&#x2F;ProjectTemplate-Python: Python Project Template (github.com)\n\nDiffBIR\nlink: [2308.15070] DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior (arxiv.org)\ngithub: XPixelGroup&#x2F;DiffBIR: Official codes of DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior (github.com)\n\nBSRGAN\nlink:[2103.14006] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution (arxiv.org)\ngithub: cszn&#x2F;BSRGAN: Designing a Practical Degradation Model for Deep Blind Image Super-Resolution (ICCV, 2021) (PyTorch) - We released the training code! (github.com)\n\n退化方法\nhttps://huggingface.co/datasets/laion/laion-high-resolution/blob/main/part-00021-45914064-d424-4c1c-8d96-dc8125c645fb-c000.snappy.parquet\nhttps://huggingface.co/datasets/laion/laion-high-resolution/resolve/main/part-\\$(printf “%05d” $i)-5d6701c4-b238-4c0a-84e4-fe8e9daea963-c000.snappy.parquet\n45914064-d424-4c1c-8d96-dc8125c645fb-c000.snappy.parquet\n5d6701c4-b238-4c0a-84e4-fe8e9daea963-c000.snappy.parquet\n"},{"title":"NeuS笔记","url":"/2025/05/06/NeuS%E7%AC%94%E8%AE%B0/","content":"NeuS Note渲染过程场景表示要重建的物体场景由两个函数表示：\n\n$f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$：将空间位置 $x \\in \\mathbb{R}^3$ 映射到物体的符号距离\n$c: \\mathbb{R}^3 \\times S^2 \\rightarrow \\mathbb{R}^3$：将空间位置 $x \\in \\mathbb{R}^3$ 和视图方向 $v \\in \\mathbb{S}^2$ 映射为颜色\n\n这两个函数都通过 MLP 实现\n物体的表面 $S$ 由其 SDF（signed distance function） 的零等值面表示，即$$\\mathcal{S} &#x3D; \\left{ \\mathbf{x} \\in \\mathbb{R}^3 \\mid f(\\mathbf{x}) &#x3D; 0 \\right}. \\tag{1}$$\n\n符号距离函数 SDF\n具体来说，SDF是一个函数，通常表示为 $F : \\mathbb{R}^n \\rightarrow \\mathbb{R}$，其中 $n$ 是空间的维度（例如二维或三维）。对于空间中的任意一点 $p$，SDF $F(p)$ 表示从点 $p$ 到最近表面的距离，并且：\n\n如果 $F(p) &lt; 0$，则点 $p$ 在物体内部；\n如果 $F(p) &#x3D; 0$，则点 $p$ 在物体表面上；\n如果 $F(p) &gt; 0$，则点 $p$ 在物体外部。\n\n\nS-密度为了将体渲染的方法用于训练 SDF 神经网络，引入概率密度函数  $\\phi_s(f(x))$，称为 S-密度场：$$\\phi_s(x) &#x3D; \\frac{s e^{-sx}}{(1 + e^{-sx})^2} \\tag{2}$$S-密度场是 Sigmoid 函数 $\\Phi_s(x) &#x3D; (1 + e^{-sx})^{-1}$ 的导数，即 $\\phi_s(x) &#x3D; \\Phi_s’(x)$。\n\n原则上，$\\phi_s(x)$ 可以是任何以 0 为中心的单峰（即钟形）密度分布；这里我们选择逻辑密度分布是因为它的计算方便。\n注意，$\\phi_s(x)$ 的标准差由 $1&#x2F;s$ 给出，这也是一个可训练参数，即随着网络训练的收敛，$1&#x2F;s$ 接近于零。\n随着 $s$ 的增大，$\\phi_s(x)$ 的的峰值高度会增加，宽度会变窄，也就是说随着 s 的增大而变得更加尖锐和集中\n\n\nNeuS 的主要思想是，在 S-密度场 $\\phi_s(f(x))$ 的帮助下，使用体渲染通过仅以 2D 输入图像作为监督来训练 SDF 网络。在成功最小化基于这种监督的损失函数之后，期望网络编码的 SDF 的零等值面能够准确地表示重建的表面 $S$，其诱导的 S-密度 $\\phi_s(f(x))$ 在表面附近呈现出显著的高值。\n\n $\\phi_s(f(x))$ 越大 $\\rarr$ $f(x)$越趋近于0 $\\rarr$ $x$ 越趋近于物体表面\n\n渲染给定一个像素，我们将从该像素发出的射线表示为 ${p(t) &#x3D; o + tv \\mid t \\geq 0}$，其中 $o$ 是相机的中心，$v$ 是射线的单位方向向量。我们通过以下方式沿射线累积颜色：\n$$C(\\mathbf{o}, \\mathbf{v}) &#x3D; \\int_{0}^{+\\infty} w(t)c(\\mathbf{p}(t), \\mathbf{v}) \\mathrm{d}t, \\tag{3}$$\n\n$C(\\mathbf{o}, \\mathbf{v})$ 是该像素的输出颜色\n$w(t)$ 是点 $p(t)$ 的权重函数\n$c(\\mathbf{p}(t), \\mathbf{v})$ 是沿视图方向 $\\mathbf{v}$ 的点 $\\mathbf{p}(t)$ 处的颜色。\n\n对权重函数 $w(t)$ 的要求从 2D 图像中学习精确的 SDF 表示的关键是建立输出颜色 $C(\\mathbf{o}, \\mathbf{v})$ 和 SDF $f$ 之间的适当联系，即基于场景的 SDF $f$ 导出沿射线的适当权重函数 $w(t)$。\n权重函数 $w(t)$ 要满足以下条件：\n\n无偏性。给定一条相机射线 $p(t)$，$w(t)$ 在表面交点 $p(t^*)$ 处取得局部最大值，即 $f(p(t^*)) &#x3D; 0$，也就是说，点 $p(t^*)$ 位于 SDF 的零等值面上（$x$）。\n\n遮挡感知。给定任意两个深度值 $t_0$ 和 $t_1$，满足 $f(t_0) &#x3D; f(t_1)$，$w(t_0) &gt; 0$，$w(t_1) &gt; 0$，且 $t_0 &lt; t_1$，有 $w(t_0) &gt; w(t_1)$。也就是说，当两个点具有相同的 SDF 值（因此具有相同的 SDF 诱导的 S-密度值）时，更靠近视点的点应该对最终输出颜色有更大的贡献。\n\n\n一个无偏的权重函数 $w(t)$ 保证了相机射线与 SDF 的零等值面的交点对像素颜色的贡献最大。\n遮挡感知属性确保了当一条射线依次穿过多个表面时，渲染过程将正确地使用离相机最近的表面的颜色来计算输出颜色。\nNeRF的朴素解NeRF中的权重函数定义如下：$$w(t) &#x3D; T(t) \\sigma(t),\\tag{4}$$\n\n$\\sigma(t)$ 是NeRF中所谓的体密度，这里将 $\\sigma(t)$ 设置为等于 S-密度值，即 $\\sigma(t) &#x3D; \\phi_s(f(p(t)))$ \n$T(t) &#x3D; \\exp\\left(-\\int_{0}^{t} \\sigma(u) \\mathrm{d}u\\right)$ 表示沿射线的累积透射率。\n\n尽管由此产生的权重函数具有遮挡感知性，但它是有偏的，因为它在重建的表面中引入了固有的误差。如图所示，权重函数 $w(t)$ 在射线达到满足 $f(p(t^*)) &#x3D; 0$ 的表面点 $p(t^*)$ 之前的某点达到局部最大值\n\nNeuS对权重函数 $w(t)$ 的解为了介绍 NeuS 的解决方案，首先介绍一种直接使用归一化 S-密度作为权重来构建无偏权重函数的简单方法：$$w(t) &#x3D; \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_{0}^{+\\infty} \\phi_s(f(\\mathbf{p}(u))) \\mathrm{d}u}.\\tag{5}$$这种权重函数的构造是无偏的，但不具备遮挡感知性。\n\n例如，如果射线穿透两个表面，SDF 函数 $f$ 将在射线上有两个零点，这导致权重函数 $w(t)$ 上有两个峰值，并且由此产生的权重函数将平均混合这两个表面的颜色，而不考虑遮挡。\n\n为了确保权重函数 $w(t)$ 的遮挡感知属性，我们仍将遵循NeRF 的基本框架，以一种新的方式从 S-密度定义我们的权重函数 $w(t)$：$$w(t) &#x3D; T(t) \\rho(t), \\text{ where } T(t) &#x3D; \\exp\\left(-\\int_{0}^{t} \\rho(u) \\mathrm{d}u\\right).\\tag{6}$$\n\n$\\rho(t)$：不透明密度函数，是标准体积渲染中体积密度 $\\sigma$ 的对应\n\n不透明密度函数 $\\rho(t)$ 的推导首先考虑一个简单的理想情况：表面是一个远离相机的平面，且只有一个交点\n此时的符号距离函数 SDF 很明显是：$$f(p(t)) &#x3D; -|\\cos(\\theta)| \\cdot (t - t^*)$$\n\n焦点位置：$f(p(t^*)) &#x3D; 0$\n$\\theta$ 是视图方向 $v$ 和向外表面法线向量 $n$ 之间的角度\n\n在这种假设下公式 5 确实满足要求，由于$|\\cos(\\theta)|$ 是一个常数，可以得出：$$\\begin{align*}w(t) &amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_0^{+\\infty} \\phi_s(f(\\mathbf{p}(u))) \\mathrm{d}u} \\&amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_0^{+\\infty} \\phi_s(-|\\cos(\\theta)|(u - t^*)) \\mathrm{d}u} \\&amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_{-t^*}^{+\\infty} \\phi_s(-|\\cos(\\theta)|u^*) \\mathrm{d}u^*} \\tag{7} \\&amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{|\\cos(\\theta)|^{-1} \\int_{-|\\cos(\\theta)|t^*}^{+\\infty} \\phi_s(\\hat{u}) \\mathrm{d}\\hat{u}} \\&amp;&#x3D; |\\cos(\\theta)| \\phi_s(f(\\mathbf{p}(t))).\\end{align*}$$\n朴素解中的偏差权重函数定义为 $w(t) &#x3D; T(t) \\sigma(t)$，其中不透明度 $\\sigma(t) &#x3D; \\phi_s(f(\\mathbf{p}(t)))$。因此，我们有$$\\begin{equation}\\begin{split}\\frac{\\mathrm{d}w}{\\mathrm{d}t} &amp;&#x3D; \\frac{\\mathrm{d}(T(t)\\sigma(t))}{\\mathrm{d}t} \\&amp;&#x3D; \\frac{\\mathrm{d}T(t)}{\\mathrm{d}t}\\sigma(t) + T(t)\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} \\&amp;&#x3D; \\left[\\exp\\left(-\\int_0^t \\sigma(t) \\mathrm{d}t\\right)(-\\sigma(t))\\right]\\sigma(t) + T(t)\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} \\&amp;&#x3D; T(t)(-\\sigma(t))\\sigma(t) + T(t)\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} \\&amp;&#x3D; T(t)\\left(\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} - \\sigma(t)^2\\right).\\end{split}\\end{equation}$$待定\n","categories":["Note"],"tags":["三维重建"]},{"url":"/2024/11/19/%E6%9C%AA%E5%91%BD%E5%90%8D/","content":"武嘉琪邮箱: dl2wjq@gmail.com\n电话: 18153421584\n地址: 电子科技大学清水河校区\n教育经历\n本科 | 电子科技大学 | 通信工程 | 2019年9月 - 2023年6月\n\n硕士 | 电子科技大学 | 电子信息 | 2023年9月 - 现在\n\n\n研究领域\n生成式模型：扩散模型及其应用\n3D 计算机视觉和图形学：三维重建（NeRF &amp; 3DGS）\n计算机视觉：图像增强（低光图像增强），图像融合\n\n研究项目基于扩散模型的星空&amp;低光图像增强\n\n构建了第一个由355个实拍和854个半合成图像对组成的星野图像增强基准数据集，这使得不同方法对星空图像增强的比较成为可能\n开发了第一种基于去噪扩散概率模型 (DDPM, Denoising Diffusion Probabilistic Models) 的星空图像增强方法，在低光图像增强和星野图像增强任务上优于最先进的低光图像增强算法\n\n技能\n编程语言: Python, Matlab, C++\n深度学习框架：PyTorch\n\n"}]