[{"title":"0.è§£æ„åŸºæœ¬pipeline","url":"/2024/04/06/0.%20%E8%A7%A3%E6%9E%84%E5%9F%BA%E6%9C%ACpipeline/","content":"è§£æ„åŸºæœ¬pipelineå€ŸåŠ©pipelineï¼Œä»…éœ€å››è¡Œä»£ç å³å¯ç”Ÿæˆå›¾åƒï¼š\nfrom diffusers import DDPMPipelineddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")image = ddpm(num_inference_steps=25).images[0]\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼ŒpipelineåŒ…å«ï¼š\n\nå»å™ªæ¨¡å‹ï¼šUNet2DModel\né‡‡æ ·å™¨ï¼šDDPMScheduler\n\npipelineé€šè¿‡è·å–æ‰€éœ€è¾“å‡ºå¤§å°çš„éšæœºå™ªå£°å¹¶å°†å…¶å¤šæ¬¡é€šè¿‡å»å™ªæ¨¡å‹æ¥å¯¹å›¾åƒè¿›è¡Œå»å™ªã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œå»å™ªæ¨¡å‹é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œè°ƒé‡‡æ ·å™¨ä½¿ç”¨å®ƒæ¥é¢„æµ‹å™ªå£°è¾ƒå°çš„å›¾åƒã€‚ç®¡é“é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°åˆ°è¾¾æŒ‡å®šæ•°é‡çš„æ¨ç†æ­¥éª¤çš„å›¾åƒã€‚\nè¦åˆ†åˆ«ä½¿ç”¨å»å™ªæ¨¡å‹å’Œé‡‡æ ·å™¨é‡æ–°åˆ›å»ºpipelineï¼Œè®©æˆ‘ä»¬ç¼–å†™è‡ªå·±çš„å»å™ªè¿‡ç¨‹ï¼š\n\nè½½å…¥å»å™ªæ¨¡å‹å’Œé‡‡æ ·å™¨ï¼š\nfrom diffusers import DDPMScheduler, UNet2DModelscheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n\né€šè¿‡é‡‡æ ·å™¨è®¾ç½®å»å™ªè¿‡ç¨‹çš„æ—¶é—´æ­¥æ•°ï¼š\nscheduler.set_timesteps(50)\n\nè®¾ç½®é‡‡æ ·å™¨çš„æ—¶é—´æ­¥æ•°ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«å‡åŒ€é—´éš”å…ƒç´ çš„å¼ é‡ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­ä¸º 50ã€‚æ¯ä¸ªå…ƒç´ å¯¹åº”äºæ¨¡å‹å¯¹å›¾åƒè¿›è¡Œå»å™ªçš„æ—¶é—´æ­¥ã€‚åœ¨è¿›è¡Œå»å™ªå¾ªç¯æ—¶ï¼Œå°†è¿­ä»£è¯¥å¼ é‡ä»¥å¯¹å›¾åƒè¿›è¡Œå»å™ªï¼š\nscheduler.timestepstensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,    \t700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,    \t420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,    \t140, 120, 100,  80,  60,  40,  20,   0])\n\nåˆ›å»ºä¸æ‰€éœ€è¾“å‡ºå›¾åƒå½¢çŠ¶ç›¸åŒçš„éšæœºå™ªå£°ï¼š\nimport torchsample_size = model.config.sample_sizenoise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n\nç°åœ¨ç¼–å†™ä¸€ä¸ªå¾ªç¯æ¥è¿­ä»£ã€‚\n\nåœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œéƒ½ä¼šæ‰§è¡Œ UNet2DModel.forward() ï¼Œæ ¹æ®è¾“å…¥çš„å™ªå£°å›¾åƒ(input)å’Œæ—¶é—´æ­¥(t)è¿”å›å™ªå£°æ®‹å·®(noisy_residual)ã€‚\né‡‡æ ·å™¨çš„ step() æ–¹æ³•åˆ©ç”¨å™ªå£°æ®‹å·®(noisy_residual)ã€æ—¶é—´æ­¥(t))å’Œå™ªå£°å›¾åƒ(input)é¢„æµ‹å‰ä¸€ä¸ªæ—¶é—´æ­¥é•¿çš„å›¾åƒã€‚è¯¥è¾“å‡ºæˆä¸ºå»å™ªå¾ªç¯çš„ä¸‹ä¸€ä¸ªè¾“å…¥ã€‚\n\ninput = noisefor t in scheduler.timesteps:    with torch.no_grad():        noisy_residual = model(input, t).sample    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample    input = previous_noisy_sample\n\næœ€åä¸€æ­¥æ˜¯å°†å»å™ªè¾“å‡ºè½¬æ¢ä¸ºå›¾åƒï¼š\nfrom PIL import Imageimport numpy as np# inputçš„æ•°æ®èŒƒå›´ä¸º[-1, 1], è½¬ä¸º[0, 1]# shape: [1, 3, sample_size, sample_size] -&gt; [3, sample_size, sample_size]image = (input / 2 + 0.5).clamp(0, 1).squeeze()# shape: [3, sample_size, sample_size] -&gt; [sample_size, sample_size, 3]# data range: (float32)[0, 1] -&gt; (uint8)[0, 255]# device: cuda -&gt; cpu# type: torch.tensor -&gt; numpy.arrayimage = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()# è¾“å‡ºä¸ºå›¾åƒimage = Image.fromarray(image)image\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"1.è§£æ„Stable Diffusion","url":"/2024/04/07/1.%20%E8%A7%A3%E6%9E%84%20Stable%20Diffusion/","content":"è§£æ„Stable DiffusionStable Diffusionæ˜¯ä¸€ç§æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚å®ƒä½¿ç”¨å›¾åƒçš„ä½ç»´è¡¨ç¤ºè€Œä¸æ˜¯å®é™…çš„åƒç´ ç©ºé—´ï¼Œè¿™ä½¿å¾—å®ƒçš„å†…å­˜æ•ˆç‡æ›´é«˜ã€‚\n\nVAEç¼–ç å™¨å°†å›¾åƒå‹ç¼©ä¸ºä½ç»´è¡¨ç¤ºï¼ŒVAEè§£ç å™¨å°†å‹ç¼©çš„ä½ç»´è¡¨ç¤ºè½¬æ¢å›å›¾åƒã€‚\n\nå¯¹äºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œéœ€è¦ä¸€ä¸ªåˆ†è¯å™¨ï¼ˆtokenizerï¼‰å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆtext encoderï¼‰æ¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆtext embeddingsï¼‰ã€‚\n\n\nå¦‚ä¸Šæ‰€è¿°ï¼ŒSD pipelineæ¯”ä»…åŒ…å« UNet æ¨¡å‹çš„ DDPM pipelineæ›´å¤æ‚ã€‚Stable Diffusionå…·æœ‰ä¸‰ä¸ªç‹¬ç«‹çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚\n\nè¿ä½œæœºåˆ¶ï¼šStable Diffusion with ğŸ§¨ Diffusers (huggingface.co)\n\n\nThe autoencoder (VAE): AutoencoderKL\nThe U-Net: UNet2DConditionModel\nThe Text-encoder: CLIPTextModel\n\nSD pipeline è¿ä½œæœºåˆ¶è½½å…¥æ‰€æœ‰ç»„ä»¶ä½¿ç”¨ from_pretrained() æ–¹æ³•åŠ è½½æ‰€æœ‰ç»„ä»¶ã€‚å¯ä»¥åœ¨é¢„è®­ç»ƒçš„ runwayml/stable-diffusion-v1-5 ä¸­æ‰¾åˆ°å®ƒä»¬ï¼Œæ¯ä¸ªç»„ä»¶éƒ½å­˜å‚¨åœ¨å•ç‹¬çš„å­æ–‡ä»¶å¤¹ä¸­ï¼š\nfrom PIL import Imageimport torchfrom transformers import CLIPTextModel, CLIPTokenizerfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler# è‡ªç¼–ç å™¨ï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡vae = AutoencoderKL.from_pretrained(\"sd-v1.5\", subfolder=\"vae\", variant='fp16', use_safetensors=True)# åˆ†è¯å™¨tokenizer = CLIPTokenizer.from_pretrained(\"sd-v1.5\", subfolder=\"tokenizer\")# æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡text_encoder = CLIPTextModel.from_pretrained(\"sd-v1.5\", subfolder=\"text_encoder\", variant='fp16', use_safetensors=True)# å»å™ªå™¨Unetï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡unet = UNet2DConditionModel.from_pretrained(\"sd-v1.5\", subfolder=\"unet\", variant='fp16', use_safetensors=True)\n\nå°†é‡‡æ ·å™¨æ›¿æ¢ä¸º UniPCMultistepSchedulerï¼Œè€Œä¸æ˜¯ sd1.5 é»˜è®¤çš„ PNDMSchedulerï¼Œé€šè¿‡ä»¥ä¸‹ä»£ç å®ç°ï¼š\nfrom diffusers import UniPCMultistepScheduler# é‡‡æ ·å™¨ï¼Œä¿®æ”¹ä¸ºUniPCMultistepScheduler è€Œä¸æ˜¯é»˜è®¤çš„PNDMSchedulerscheduler = UniPCMultistepScheduler.from_pretrained(\"sd-v1.5\", subfolder=\"scheduler\")\n\nä¸ºäº†åŠ é€Ÿæ¨ç†ï¼Œå°†å…·æœ‰å¯è®­ç»ƒæƒé‡çš„æ¨¡å‹ç§»è‡³ GPUï¼š\ntorch_device = torch.device(\"cuda\")vae.to(torch_device)text_encoder.to(torch_device)unet.to(torch_device)\n\n\n\nåˆ›å»ºæ–‡æœ¬åµŒå…¥å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ä»¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚è¯¥æ–‡æœ¬ç”¨äºä¸º UNet æ¨¡å‹è¾“å…¥æ–‡æœ¬æ¡ä»¶ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å¯¼ç”Ÿæˆå†…å®¹ã€‚\n\nguidance_scale: è¯¥å‚æ•°å†³å®šç”Ÿæˆå›¾åƒæ—¶åº”ç»™äºˆæ–‡æœ¬æç¤ºçš„æƒé‡ï¼Œå³æ–‡æœ¬å¼•å¯¼çš„å¼ºåº¦\n\néšæ„é€‰æ‹©ä½ å–œæ¬¢çš„ä»»ä½•æ–‡æœ¬æç¤ºï¼è®¾å®šåŸºæœ¬å‚æ•°å¦‚ä¸‹ï¼š\nprompt = [\"a photograph of an astronaut riding a horse\"]batch_size = len(prompt)          # ç”Ÿæˆçš„æ‰¹é‡å¤§å°height = 512                      # ç›®æ ‡ç”Ÿæˆçš„å›¾åƒçš„é«˜width = 512                       # ç›®æ ‡ç”Ÿæˆçš„å›¾åƒçš„å®½num_inference_steps = 25          # å»å™ªæ€»æ­¥æ•°guidance_scale = 7.5              # classifier-free guidance æ¡ä»¶å¼•å¯¼å¼ºåº¦generator = torch.manual_seed(0)  # ç”¨äºç”Ÿæˆéšæœºå™ªå£°çš„éšæœºæ•°ç§å­\n\nä½¿ç”¨åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°å¹¶ç”ŸæˆåµŒå…¥ï¼š\n# åˆ†è¯text_input = tokenizer(    prompt,                                # æ–‡æœ¬    padding=\"max_length\",                  # å¡«å……æ–‡æœ¬ä»¥è¾¾åˆ°æœ€å¤§é•¿åº¦    max_length=tokenizer.model_max_length, # è®¾ç½®äº†æ–‡æœ¬åˆ†è¯çš„æœ€å¤§é•¿åº¦ max_length = 77    truncation=True,                       # å¦‚æœæ–‡æœ¬è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå°†æˆªæ–­å®ƒã€‚    return_tensors=\"pt\"                    # è¡¨ç¤ºè¿”å› PyTorch å¼ é‡æ ¼å¼çš„ç»“æœã€‚).to(torch_device)# å°†åˆ†è¯ç»“æœè½¬ä¸ºæ–‡æœ¬åµŒå…¥with torch.no_grad():    text_embeddings = text_encoder(text_input.input_ids)[0]\n\néƒ¨åˆ†å˜é‡ç»†èŠ‚ï¼š\ntext_input:&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;text_input.input_ids:  - type: &lt;class 'torch.Tensor'&gt;  - shape: torch.Size([1, 77]) [batch, max_length]  - dtype: torch.int64  - device: cuda:0    text_embeddings:  - &lt;class 'torch.Tensor'&gt;  - torch.Size([1, 77, 768])  - torch.float32  - cuda:0\n\næ¥ä¸‹æ¥è¿˜éœ€è¦ç”Ÿæˆæ— æ¡ä»¶æ–‡æœ¬åµŒå…¥ï¼Œå³å¡«å……æ ‡è®°çš„åµŒå…¥ã€‚è¿™äº›éœ€è¦ä¸æ¡ä»¶ text_embeddings å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼ˆbatch_size å’Œ seq_lengthï¼‰ï¼š\nmax_length = text_input.input_ids.shape[-1] # 77# ç”¨ç©ºæ–‡æœ¬è·å–idsuncond_input = tokenizer(    [\"\"] * batch_size,     padding=\"max_length\",     max_length=max_length,     return_tensors=\"pt\")uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\nè®©æˆ‘ä»¬å°†æ¡ä»¶åµŒå…¥å’Œæ— æ¡ä»¶åµŒå…¥è¿æ¥ä¸ºä¸€ä¸ªæ‰¹é‡ï¼Œä»¥é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼š\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])  - &lt;class 'torch.Tensor'&gt;  - torch.Size([2, 77, 768])  - torch.float32  - cuda:0\n\n\n\nåˆ›å»ºéšæœºå™ªå£°æ¥ä¸‹æ¥ï¼Œç”Ÿæˆä¸€äº›åˆå§‹éšæœºå™ªå£°ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„èµ·ç‚¹ã€‚å®ƒå°†é€æ¸å»å™ªæ¥ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚æ­¤æ—¶ï¼Œæ½œåœ¨è¡¨ç¤ºå°äºæœ€ç»ˆå›¾åƒå°ºå¯¸ï¼ŒVAEè§£ç å™¨åä¼šå°†å…¶è½¬æ¢ä¸ºæœ€ç»ˆçš„  å›¾åƒã€‚\nprint(vae.config.block_out_channels) # [128, 256, 512, 512]print(2 ** (len(vae.config.block_out_channels) - 1))VAEçš„å±‚æ•°ä¸º4ï¼Œæ¯ä¸¤å±‚ä¹‹é—´ä¸€æ¬¡ä¸‹é‡‡æ ·ï¼Œä¸‹é‡‡æ ·å€ç‡ä¸º8\n\nç”Ÿæˆåˆå§‹å™ªå£°ï¼š\n# shape: [1, 4, 64, 64]latents = torch.randn(    (batch_size, unet.config.in_channels, height // 8, width // 8),     generator=generator                                             ).to(torch_device)\n\n\n\nå¯¹å›¾åƒè¿›è¡Œå»å™ªé¦–å…ˆä½¿ç”¨ sigmaï¼ˆå™ªå£°ç¼©æ”¾å€¼ï¼‰ç¼©æ”¾åˆå§‹å™ªå£°åˆ†å¸ƒï¼Œè¿™æ˜¯ UniPCMultistepScheduler ç­‰æ”¹è¿›é‡‡æ ·å™¨æ‰€éœ€çš„ï¼š\nlatents = latents * scheduler.init_noise_sigma# init_noise_sigma = 1.0\n\næœ€åä¸€æ­¥æ˜¯åˆ›å»ºå»å™ªå¾ªç¯ï¼Œè¯¥å¾ªç¯å°†é€æ­¥å°†çº¯å™ªå£°è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºæ‰€æè¿°çš„å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚å»å™ªå¾ªç¯éœ€è¦åšä¸‰ä»¶äº‹ï¼š\n\nè®¾ç½®å»å™ªæœŸé—´é‡‡æ ·å™¨ä½¿ç”¨çš„æ—¶é—´æ­¥é•¿ã€‚\nè¿­ä»£æ—¶é—´æ­¥ã€‚\nåœ¨æ¯ä¸ªæ—¶é—´æ­¥ ï¼Œè°ƒç”¨ UNet æ¨¡å‹æ¥æ ¹æ®  é¢„æµ‹å™ªå£°æ®‹å·® ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™é‡‡æ ·å™¨ä»¥è®¡ç®—ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„å™ªå£°æ ·æœ¬ ã€‚from tqdm.auto import tqdm# è®¾å®šé‡‡æ ·å™¨çš„è¿­ä»£æ­¥æ•°scheduler.set_timesteps(num_inference_steps) # 25# è¿­ä»£æ—¶é—´æ­¥ï¼šscheduler.timesteps#   - &lt;class 'torch.Tensor'&gt;#   - torch.Size([25])#   - torch.int64#   - cpufor t in tqdm(scheduler.timesteps):    # å¦‚æœä½¿ç”¨CFGï¼Œä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼Œåˆ™å¯ä»¥åœ¨batchç»´åº¦æ‰©å±•è¾“å…¥ã€‚    latent_model_input = torch.cat([latents] * 2)    # é¢„æµ‹å™ªå£°å‰ï¼Œæ ¹æ®æ—¶é—´æ­¥tç¼©æ”¾Unetçš„è¾“å…¥x_t    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)    # é¢„æµ‹å™ªå£°æ®‹å·®    with torch.no_grad():        # unetçš„è¾“å‡ºç±»å‹ï¼š&lt;class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput'&gt;        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)        noise_pred = noise_pred.sample        # &lt;class 'torch.Tensor'&gt;        # torch.Size([2, 4, 64, 64])        # torch.float32        # cuda: 0        # ä½¿ç”¨CFGè®¡ç®—æ–°çš„å™ªå£°æ®‹å·®    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)        # ä½¿ç”¨é‡‡æ ·å™¨è®¡ç®—ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„æ ·æœ¬ï¼šx_t -&gt; x_t-1    # é‡‡æ ·å™¨çš„è¾“å‡ºç±»å‹ï¼š&lt;class 'diffusers.schedulers.scheduling_utils.SchedulerOutput'&gt;    latents = scheduler.step(noise_pred, t, latents)    latents = latents.prev_sample\n\nè§£ç æ½œåœ¨è¡¨ç¤ºä¸ºå›¾åƒæœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ vae å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒï¼Œå¹¶é€šè¿‡æ ·æœ¬è·å–è§£ç è¾“å‡ºï¼š\n# åœ¨å°†æ½œåœ¨è¡¨ç¤ºè¾“å…¥VAEè§£ç å™¨å‰è¿›è¡Œç¼©æ”¾latents = 1 / 0.18215 * latentswith torch.no_grad():    # VAEè§£ç å™¨è¾“å‡ºç±»å‹ï¼š&lt;class 'diffusers.models.autoencoders.vae.DecoderOutput'&gt;    image = vae.decode(latents)    image = image.sample\n\næœ€åï¼Œå°†å›¾åƒè½¬æ¢ä¸º PIL.Image ä»¥æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼\n# è°ƒæ•´å›¾åƒçš„èŒƒå›´å’Œæ•°æ®ç±»å‹num = len(os.listdir('./results'))image = (image / 2 + 0.5).clamp(0, 1).squeeze()image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()image = Image.fromarray(image)image.save(f'./results/learn_sp{num}.png')\n\n\n\nä»åŸºæœ¬ç®¡é“åˆ°å¤æ‚ç®¡é“ï¼Œæ‚¨å·²ç»çœ‹åˆ°ç¼–å†™è‡ªå·±çš„æ‰©æ•£ç³»ç»ŸçœŸæ­£éœ€è¦çš„åªæ˜¯ä¸€ä¸ªé™å™ªå¾ªç¯ã€‚è¯¥å¾ªç¯åº”è®¾ç½®è°ƒåº¦ç¨‹åºçš„æ—¶é—´æ­¥é•¿ï¼Œå¯¹å…¶è¿›è¡Œè¿­ä»£ï¼Œå¹¶äº¤æ›¿è°ƒç”¨ UNet æ¨¡å‹æ¥é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™è°ƒåº¦ç¨‹åºä»¥è®¡ç®—å…ˆå‰çš„å™ªå£°æ ·æœ¬ã€‚\nè¿™å°±æ˜¯Diffusers çš„è®¾è®¡ç›®çš„ï¼šè®©ä½¿ç”¨æ¨¡å‹å’Œè°ƒåº¦ç¨‹åºç›´è§‚ã€è½»æ¾åœ°ç¼–å†™è‡ªå·±çš„æ‰©æ•£ç³»ç»Ÿã€‚\nNext:\n\nContribute a community pipeline (huggingface.co)\nPipelines (huggingface.co)\nè¿ä½œæœºåˆ¶ï¼šStable Diffusion with ğŸ§¨ Diffusers (huggingface.co)\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"3.è®­ç»ƒdiffusion model","url":"/2024/04/09/3.%20%E8%AE%AD%E7%BB%83Diffusion%20Model/","content":"Train a diffusion modelæ— æ¡ä»¶å›¾åƒç”Ÿæˆæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§æµè¡Œåº”ç”¨ï¼Œå®ƒç”Ÿæˆçš„å›¾åƒä¸ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ä¸­çš„å›¾åƒç›¸ä¼¼ã€‚é€šå¸¸ï¼Œæœ€å¥½çš„ç»“æœæ˜¯é€šè¿‡åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥è·å¾—çš„ã€‚\næ‚¨å¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ°è®¸å¤šè¿™æ ·çš„æ£€æŸ¥ç‚¹ï¼Œä½†å¦‚æœæ‚¨æ‰¾ä¸åˆ°æ‚¨å–œæ¬¢çš„æ£€æŸ¥ç‚¹ï¼Œæ‚¨å¯ä»¥éšæ—¶è®­ç»ƒè‡ªå·±çš„æ£€æŸ¥ç‚¹ï¼\næœ¬æ•™ç¨‹å°†æ•™æ‚¨å¦‚ä½•åœ¨ Smithsonian Butterflies æ•°æ®é›†çš„å­é›†ä¸Šä»å¤´å¼€å§‹è®­ç»ƒ UNet2DModelï¼Œä»¥ç”Ÿæˆæ‚¨è‡ªå·±çš„Butterflies\n\ndiffusers_training_example.ipynb - Colaboratory (google.com)\n\nè®­ç»ƒé…ç½®ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«è®­ç»ƒè¶…å‚æ•°çš„ TrainingConfig ç±»ï¼š\nfrom dataclasses import dataclass@dataclassclass TrainingConfig:    image_size = 128                       # ç”Ÿæˆå›¾åƒçš„åˆ†è¾¨ç‡    train_batch_size = 16                  # è®­ç»ƒbatch    eval_batch_size = 16                   # éªŒè¯é˜¶æ®µçš„å›¾åƒæ•°é‡    num_epochs = 50    gradient_accumulation_steps = 1        #    learning_rate = 1e-4    lr_warmup_steps = 500    save_image_epochs = 10    save_model_epochs = 30    mixed_precision = \"fp16\"               # `no` for float32, `fp16` for automatic mixed precision    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub    push_to_hub = False  # whether to upload the saved model to the HF Hub    hub_model_id = \"TO-Hitori/my-awesome-model\"  # the name of the repository to create on the HF Hub    hub_private_repo = False    overwrite_output_dir = True  # overwrite the old model when re-running the notebook    seed = 0config = TrainingConfig()\n\n\n\nè½½å…¥æ•°æ®é›†æ‚¨å¯ä»¥ä½¿ç”¨datasetsåº“è½»æ¾åŠ è½½Smithsonian Butterfliesæ•°æ®é›†ï¼š\nfrom datasets import load_dataset# æ•°æ®é›†è·¯å¾„ï¼šæœ¬åœ°è·¯å¾„æˆ–hugging-faceè·¯å¾„config.dataset_name = r\"D:\\MyData\\smithsonian_butterflies_subset\"dataset = load_dataset(config.dataset_name, split=\"train\")\n\n\nåœ¨æ­¤æŸ¥æ‰¾å…¶ä»–æ•°æ®é›†ï¼šhuggan (HugGAN Community) (huggingface.co)\n\nDatasets ä½¿ç”¨ Image åŠŸèƒ½è‡ªåŠ¨è§£ç å›¾åƒæ•°æ®å¹¶å°†å…¶åŠ è½½ä¸ºæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–çš„ PIL.Imageï¼š\nimport matplotlib.pyplot as pltfig, axs = plt.subplots(1, 4, figsize=(16, 4))for i, image in enumerate(dataset[:4][\"image\"]):    axs[i].imshow(image)    axs[i].set_axis_off()fig.show()\n\nè¿™äº›å›¾åƒçš„å°ºå¯¸å„ä¸ç›¸åŒï¼Œå› æ­¤éœ€è¦å…ˆå¯¹å®ƒä»¬è¿›è¡Œé¢„å¤„ç†ï¼š\n\nResize å°†å›¾åƒå¤§å°æ›´æ”¹ä¸º config.image_size ä¸­å®šä¹‰çš„å¤§å°ã€‚ \nRandomHorizontalFlip é€šè¿‡éšæœºé•œåƒç¿»è½¬å›¾åƒæ¥å¢å¼ºæ•°æ®é›†ã€‚ \nNormalizeå¯¹äºå°†åƒç´ å€¼é‡æ–°ç¼©æ”¾åˆ° [-1, 1] èŒƒå›´éå¸¸é‡è¦ï¼Œè¿™æ˜¯æ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥èŒƒå›´ã€‚\n\nfrom torchvision import transformspreprocess = transforms.Compose(    [        transforms.Resize((config.image_size, config.image_size)),        transforms.RandomHorizontalFlip(),        transforms.ToTensor(),        transforms.Normalize([0.5], [0.5]),    ])\n\nä½¿ç”¨Datasetsçš„ set_transform æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€åº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼š\ndef transform(examples):    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]    return {\"images\": images}dataset.set_transform(transform)\n\nå°†æ•°æ®é›†åŒ…è£…åœ¨torchçš„ DataLoader ä¸­è¿›è¡Œè®­ç»ƒï¼\nimport torchtrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n\n\n\nåˆ›å»º UNet2D æ¨¡å‹ Diffusers ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ä½¿ç”¨æ‚¨æƒ³è¦çš„å‚æ•°è½»æ¾åœ°ä»å…¶æ¨¡å‹ç±»åˆ›å»ºã€‚ä¾‹å¦‚ï¼Œè¦åˆ›å»º UNet2DModelï¼š\n# åˆ›å»ºU-Netfrom diffusers import UNet2DModelmodel = UNet2DModel(    sample_size=config.image_size,  # å›¾åƒåˆ†è¾¨ç‡    in_channels=3,                  # è¾“å…¥å›¾åƒçš„é€šé“æ•°é‡    out_channels=3,                 # è¾“å‡ºå›¾åƒçš„é€šé“æ•°é‡    layers_per_block=2,             # æ¯å±‚ä½¿ç”¨çš„æ®‹å·®å—ä¸ªæ•°    block_out_channels=(128, 128, 256, 256, 512, 512),  # æ¯ä¸€å±‚çš„è¾“å‡ºé€šé“æ•°é‡    down_block_types=(        \"DownBlock2D\",              # æ®‹å·®ä¸‹é‡‡æ ·æ¨¡å—        \"DownBlock2D\",        \"DownBlock2D\",        \"DownBlock2D\",        \"AttnDownBlock2D\",          # æœ‰spatial self-attentionçš„ä¸‹é‡‡æ ·æ®‹å·®æ¨¡å—        \"DownBlock2D\",    ),    up_block_types=(        \"UpBlock2D\",                # æ®‹å·®ä¸Šé‡‡æ ·æ¨¡å—        \"AttnUpBlock2D\",            # æœ‰spatial self-attentionçš„ä¸Šé‡‡æ ·æ®‹å·®æ¨¡å—        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",    ),)print(\"UNet2DModel have {} paramerters in total\".format(sum(x.numel() for x in model.parameters())))# UNet2DModel have 113673219 paramerters in total\n\næ£€æŸ¥æ ·æœ¬å›¾åƒå½¢çŠ¶ä¸æ¨¡å‹è¾“å‡ºå½¢çŠ¶æ˜¯å¦åŒ¹é…ï¼š\nsample_image = dataset[0][\"images\"].unsqueeze(0)print(\"Input shape:\", sample_image.shape)print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)'''Input shape: torch.Size([1, 3, 128, 128])Output shape: torch.Size([1, 3, 128, 128])'''\n\n\n\nåˆ›å»ºé‡‡æ ·å™¨æ ¹æ®æ‚¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œè®­ç»ƒè¿˜æ˜¯æ¨ç†ï¼Œé‡‡æ ·å™¨çš„è¡Œä¸ºä¼šæœ‰æ‰€ä¸åŒã€‚\n\nåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡æ ·å™¨æ ¹æ®å™ªå£°ç”Ÿæˆå›¾åƒã€‚\nåœ¨è®­ç»ƒæœŸé—´ï¼Œé‡‡æ ·å™¨ä»æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç‰¹å®šç‚¹è·å–æ¨¡å‹è¾“å‡ºï¼ˆæˆ–æ ·æœ¬ï¼‰ï¼Œå¹¶æ ¹æ®å™ªå£°è°ƒåº¦noise scheduleå’Œæ›´æ–°è§„åˆ™update ruleå°†å™ªå£°æ³¨å…¥å›¾åƒã€‚\n\nè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ DDPMScheduler å¹¶ä½¿ç”¨ add_noise æ–¹æ³•å‘sample_image æ·»åŠ ä¸€äº›éšæœºå™ªå£°ï¼š\nimport torchfrom PIL import Imagefrom diffusers import DDPMSchedulerfrom torchvision.utils import save_image, make_grid# ç”¨æ€»æ­¥æ•°æ¥åˆå§‹åŒ–é‡‡æ ·å™¨noise_scheduler = DDPMScheduler(num_train_timesteps=1000)# è®¾å®šåŠ å™ªåºåˆ—ï¼Œè¿™é‡Œé€‰æ‹©äº†7ä¸ªä¾æ¬¡å¢å¤§çš„æ—¶é—´æ­¥timesteps = torch.LongTensor([50, 150, 250, 450, 650, 850, 990])# æ—¶é—´æ­¥çš„æ•°é‡ä¸ºbatchï¼Œé‡‡æ ·å›¾ç‰‡çš„å½¢çŠ¶åä¸‰ä¸ªç»´åº¦ï¼Œæ„å»ºé‡‡æ ·å™ªå£°noise = torch.randn(timesteps.shape + sample_image.shape[1:])# åˆ©ç”¨é‡‡æ ·å™¨çš„add_noiseæ–¹æ³•å°†å™ªå£°æ³¨å…¥é‡‡æ ·å›¾ç‰‡noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)# ä¿å­˜å›¾ç‰‡è§‚æµ‹é‡‡æ ·å™¨çš„åŠ å™ªæ•ˆæœsave_to_show = make_grid(torch.cat([sample_image, noisy_image], dim=0))save_image(save_to_show, './test_scheduler.png')\n\næ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯é¢„æµ‹æ·»åŠ åˆ°å›¾åƒä¸­çš„å™ªå£°ã€‚è¿™ä¸€æ­¥çš„æŸå¤±å¯ä»¥é€šè¿‡ä¸‹å¼è®¡ç®—ï¼š\n# æŸå¤±å‡½æ•°ï¼šæœ€ç®€å•çš„MSEæŸå¤±å‡½æ•°import torch.nn.functional as Fprint('test loss func')noise_pred = model(noisy_image, timesteps).sampleloss = F.mse_loss(noise_pred, noise)print('loss value = ', loss.item())\n\n\n\nè®­ç»ƒæ¨¡å‹åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»æŒæ¡äº†å¼€å§‹è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†å†…å®¹ï¼Œå‰©ä¸‹çš„å°±æ˜¯å°†æ‰€æœ‰å†…å®¹ç»„åˆåœ¨ä¸€èµ·ã€‚ é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨å’Œä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š\n# å¼€å§‹è®­ç»ƒï¼šè®¾ç½®è°ƒåº¦å™¨from diffusers.optimization import get_cosine_schedule_with_warmupoptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)lr_scheduler = get_cosine_schedule_with_warmup(    optimizer=optimizer,    num_warmup_steps=config.lr_warmup_steps,    num_training_steps=(len(train_dataloader) * config.num_epochs),)\n\nç„¶åï¼Œæ‚¨éœ€è¦ä¸€ç§è¯„ä¼°æ¨¡å‹çš„æ–¹æ³•ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ DDPMPipeline ç”Ÿæˆä¸€æ‰¹æ ·æœ¬å›¾åƒå¹¶å°†å…¶ä¿å­˜ä¸ºç½‘æ ¼ï¼š\nfrom diffusers import DDPMPipelinefrom diffusers.utils import make_image_gridimport osdef evaluate(config, epoch, pipeline):    # Sample some images from random noise (this is the backward diffusion process).    # The default pipeline output type is `List[PIL.Image]`    # ä½¿ç”¨pipelineç”Ÿæˆå›¾åƒ    images = pipeline(        batch_size=config.eval_batch_size,        generator=torch.manual_seed(config.seed),    ).images    # å°†å›¾åƒæ‹¼æ¥ä¸ºç½‘æ ¼    image_grid = make_image_grid(images, rows=4, cols=4)    # ä¿å­˜éªŒè¯å›¾åƒ    test_dir = os.path.join(config.output_dir, \"samples\")    os.makedirs(test_dir, exist_ok=True)    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n\nç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Accelerate å°†æ‰€æœ‰è¿™äº›ç»„ä»¶åŒ…è£…åœ¨ä¸€ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œä»¥è½»æ¾è¿›è¡Œï¼š\n\nTensorBoard æ—¥å¿—è®°å½•\næ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚\nè¦å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¯·ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è·å–å­˜å‚¨åº“åç§°å’Œä¿¡æ¯ï¼Œç„¶åå°†å…¶æ¨é€åˆ° Hubã€‚\n\næ¥ä¸‹æ¥æ˜¯è®­ç»ƒæ ¸å¿ƒéƒ¨åˆ†ï¼š\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):    # åˆå§‹åŒ– accelerator å’Œ tensorboard æ—¥å¿—è®°å½•    accelerator = Accelerator(        mixed_precision=config.mixed_precision,                          # æ˜¯å¦æ··åˆç²¾åº¦è®­ç»ƒ        gradient_accumulation_steps=config.gradient_accumulation_steps,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°        log_with=\"tensorboard\",                                          # ä½¿ç”¨tensorboardè®°å½•æ—¥å¿—        project_dir=os.path.join(config.output_dir, \"logs\"),             # æ—¥å¿—è·¯å¾„    )    # å¦‚æœåœ¨ä¸»è¿›ç¨‹    if accelerator.is_main_process:        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹        if config.output_dir is not None:            os.makedirs(config.output_dir, exist_ok=True)        # ä¸Šä¼ åˆ°HFçš„è®¾ç½®        if config.push_to_hub:            repo_id = create_repo(                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True            ).repo_id        # åˆå§‹åŒ–è¿½è¸ªå™¨        accelerator.init_trackers(\"train_example\")    # ç”¨accelerateåŒ…è£…ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®åŠ è½½å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨    # ä¿è¯è¾“å…¥å’Œè¾“å‡ºçš„é¡ºåºä¸€è‡´å³å¯    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(        model, optimizer, train_dataloader, lr_scheduler    )    # åˆå§‹åŒ–å…¨å±€æ­¥æ•°    global_step = 0    # å¼€å§‹è®­ç»ƒæ¨¡å‹    for epoch in range(config.num_epochs):        # åˆ›å»ºè¿›åº¦æ¡        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)        # è®¾ç½®è¿›åº¦æ¡æè¿°        progress_bar.set_description(f\"Epoch {epoch}\")        # å¯¹æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå¾ªç¯        for step, batch in enumerate(train_dataloader):            # ä»æ•°æ®é›†è·å–å›¾åƒ            clean_images = batch[\"images\"]            # ç”Ÿæˆå³å°†åŠ å…¥å›¾åƒçš„å™ªå£°            noise = torch.randn(clean_images.shape, device=clean_images.device)            # è·å–å½“å‰batch-size            bs = clean_images.shape[0]            # ä¸ºå½“å‰batchä¸­æ¯ä¸ªå›¾åƒéšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥            timesteps = torch.randint(                0,                                          # èµ·ç‚¹                noise_scheduler.config.num_train_timesteps, # ç»ˆç‚¹                (bs,),                                      # å½¢çŠ¶/æ•°é‡                device=clean_images.device,                dtype=torch.int64            )            # ä½¿ç”¨é‡‡æ ·å™¨ï¼Œæ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥çš„å™ªå£°å¹…åº¦å‘å¹²å‡€çš„å›¾åƒæ·»åŠ å™ªå£°            # è¿™æ˜¯å‰å‘æ‰©æ•£è¿‡ç¨‹            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)            # ä½¿ç”¨acceleratorç´¯ç§¯æ¨¡å‹æ¢¯åº¦            with accelerator.accumulate(model):                # é¢„æµ‹å™ªå£°æ®‹å·®                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]                # è®¡ç®—æŸå¤±å‡½æ•°                loss = F.mse_loss(noise_pred, noise)                # åå‘ä¼ æ’­æ¢¯åº¦                accelerator.backward(loss)                # æ¢¯åº¦è£å‰ª                accelerator.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()      # è¿­ä»£ä¼˜åŒ–å™¨                lr_scheduler.step()   # è¿­ä»£å­¦ä¹ ç‡è°ƒåº¦å™¨                optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦            # æ›´æ–°åŸºç£å¾’            progress_bar.update(1)            # è®°å½•æ—¥å¿—ï¼šæŸå¤±å‡½æ•°ï¼Œå­¦ä¹ ç‡å˜åŒ–            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}            progress_bar.set_postfix(**logs)            accelerator.log(logs, step=global_step)            # å…¨å±€æ­¥æ•°å¢åŠ             global_step += 1        # åœ¨æ¯ä¸ªepochåï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨evaluate()é‡‡æ ·ä¸€äº›æ¼”ç¤ºå›¾åƒå¹¶ä¿å­˜æ¨¡å‹        if accelerator.is_main_process:            # åˆå§‹åŒ–ä¸€ä¸ªpipelineï¼Œä¼ å…¥å½“å‰è®­ç»ƒçš„æ¨¡å‹å’Œè°ƒåº¦å™¨            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)            # æ ¹æ®éªŒè¯é¢‘ç‡ä¿å­˜ç ”ç©¶ç»“æœ            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:                evaluate(config, epoch, pipeline)            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:                if config.push_to_hub:                    upload_folder(                        repo_id=repo_id,                        folder_path=config.output_dir,                        commit_message=f\"Epoch {epoch}\",                        ignore_patterns=[\"step_*\", \"epoch_*\"],                    )                else:                    pipeline.save_pretrained(config.output_dir)train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextdiffusers_training_example.ipynb - Colaboratory (google.com)\nTextual Inversion (huggingface.co)\nDreamBooth (huggingface.co)\nText-to-image (huggingface.co)\nLoRA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"2.AutoPipelineåŸºæœ¬æ•™ç¨‹","url":"/2024/04/09/2.%20Auto%20Pipeline/","content":"AutoPipelineåŸºæœ¬æ•™ç¨‹AutoPipeline çš„è®¾è®¡ç›®çš„æ˜¯ï¼š \n\nè½»æ¾åŠ è½½ checkpointï¼Œè€Œæ— éœ€çŸ¥é“è¦ä½¿ç”¨çš„ç‰¹å®špipelineçš„ç±»å‹ \nåœ¨å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨å¤šä¸ªpipeline\n\nAutoPipeline ç±»æ—¨åœ¨ç®€åŒ– Diffusers ä¸­çš„å„ç§Pipelineã€‚å®ƒæ˜¯ä¸€ä¸ªé€šç”¨çš„ã€ä»»åŠ¡ä¼˜å…ˆçš„ç®¡é“ã€‚ AutoPipeline ä¼šè‡ªåŠ¨æ£€æµ‹è¦ä½¿ç”¨çš„æ­£ç¡® Pipeline ç±»ï¼Œè¿™ä½¿å¾—åœ¨ä¸çŸ¥é“ç‰¹å®šPipelineç±»åç§°çš„æƒ…å†µä¸‹è½»æ¾åŠ è½½ä»»åŠ¡çš„æ£€æŸ¥ç‚¹\n\nAPI: AutoPipeline (huggingface.co)\n\nDiffusers èƒ½å¤Ÿå®Œæˆè®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ‚¨é€šå¸¸å¯ä»¥å°†ç›¸åŒçš„é¢„è®­ç»ƒæƒé‡é‡å¤ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®å¤ã€‚å¦‚æœæ‚¨å¯¹åº“å’Œæ‰©æ•£æ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œå¯èƒ½å¾ˆéš¾çŸ¥é“è¦ä½¿ç”¨å“ªä¸ªç®¡é“æ¥å®Œæˆä»»åŠ¡ã€‚\nä¸ºæ‚¨çš„ä»»åŠ¡é€‰æ‹© AutoPipelineé¦–å…ˆé€‰æ‹©ä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæƒ³è¦ä½¿ç”¨ runwayml/stable-diffusion-v1-5 æ£€æŸ¥ç‚¹æ¥è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ä»»åŠ¡ï¼Œè¯·ä½¿ç”¨ AutoPipelineForText2Imageï¼š\nfrom diffusers import AutoPipelineForText2Imageimport torch, ospipeline = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",                 # æƒé‡è·¯å¾„    torch_dtype=torch.float16, # æ•°æ®ç±»å‹    use_safetensors=True,      # ä½¿ç”¨safetensorç±»å‹çš„æƒé‡    variant='fp16',            # åŠ è½½æƒé‡æ—¶é€‰æ‹©æ–‡ä»¶åä¸­å¸¦æœ‰â€˜fp16â€™çš„).to(\"cuda\")prompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"image = pipeline(prompt, num_inference_steps=25).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur_{num}.png')\n\næ·±å…¥å±‚æ¬¡æ¢è®¨ AutoPipelineForText2Image \n\nAutoPipelineå°†è‡ªåŠ¨ä» model_index.json æ–‡ä»¶ä¸­æ£€æµ‹ StableDiffusionPipeline ç±»\næ ¹æ®ç±»ååŠ è½½ç›¸åº”çš„æ–‡æœ¬åˆ°å›¾åƒçš„StableDiffusionPipeline\n\nmodel_index.jsonæ–‡ä»¶å†…å®¹ï¼š\n{  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.6.0\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n\n\nåŒæ ·ï¼Œå¯¹äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡çš„AutoPipelineï¼ŒAutoPipelineForImage2Image ä» model_index.json æ–‡ä»¶ä¸­æ£€æµ‹åˆ° â€œStableDiffusionâ€ æ£€æŸ¥ç‚¹ï¼Œå¹¶å°†åœ¨å¹•ååŠ è½½ç›¸åº”çš„ StableDiffusionImg2ImgPipelineã€‚\nè¿˜å¯ä»¥ä¼ é€’ç‰¹å®šäº Pipeline ç±»çš„ä»»ä½•å…¶ä»–å‚æ•°ï¼Œå¦‚guidance_scaleã€strength\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForImage2Imageimport torchimport requestsfrom PIL import Imagefrom io import BytesIOpipeline = AutoPipelineForImage2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    variant='fp16',).to(\"cuda\")prompt = \"a portrait of a dog wearing a pearl earring\"url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"response = requests.get(url)image = Image.open(BytesIO(response.content)).convert(\"RGB\")image.thumbnail((768, 768))image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur2_{num}.png')\n\n\n\nå¦‚æœæ‚¨æƒ³è¿›è¡Œå›¾åƒä¿®å¤ï¼Œåˆ™ AutoPipelineForInpainting ä»¥ç›¸åŒçš„æ–¹å¼åŠ è½½åº•å±‚çš„ StableDiffusionInpaintPipeline ç±»ï¼š\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForInpaintingfrom diffusers.utils import load_imageimport torchpipeline = AutoPipelineForInpainting.from_pretrained(    r\"D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',).to(\"cuda\")img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"init_image = load_image(img_url).convert(\"RGB\")mask_image = load_image(mask_url).convert(\"RGB\")prompt = \"A majestic tiger sitting on a bench\"image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur3_{num}.png')\n\nå¦‚æœæ‚¨å°è¯•åŠ è½½ä¸å—æ”¯æŒçš„æ£€æŸ¥ç‚¹ï¼Œåˆ™ä¼šæŠ›å‡ºé”™è¯¯\nä½¿ç”¨å¤šä¸ªPipelineå¯¹äºæŸäº›å·¥ä½œæµç¨‹æˆ–å¦‚æœæ‚¨è¦åŠ è½½è®¸å¤šPipelineï¼Œä»æ£€æŸ¥ç‚¹é‡ç”¨ç›¸åŒçš„ç»„ä»¶ä¼šæ›´èŠ‚çœå†…å­˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”æƒ³è¦å†æ¬¡å°†å…¶ç”¨äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡ï¼Œè¯·ä½¿ç”¨ from_pipe() æ–¹æ³•ã€‚æ­¤æ–¹æ³•ä»å…ˆå‰åŠ è½½çš„Pipelineçš„ç»„ä»¶åˆ›å»ºæ–°Pipelineï¼Œæ— éœ€é¢å¤–çš„å†…å­˜æˆæœ¬ã€‚\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Imageimport torchpipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',)print(type(pipeline_text2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'&gt;\"\n\nç„¶å from_pipe() å°†åŸå§‹çš„ StableDiffusionInpaintPipeline ç±»æ˜ å°„åˆ° StableDiffusionImg2ImgPipelineï¼š\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(type(pipeline_img2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'&gt;\"\n\n\n\nå¦‚æœæ‚¨å°†å¯é€‰å‚æ•°ï¼ˆä¾‹å¦‚ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ï¼‰ä¼ é€’ç»™åŸå§‹ Pipelineï¼Œåˆ™è¯¥å‚æ•°ä¹Ÿä¼šä¼ é€’ç»™æ–°ç®¡é“ï¼š\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    requires_safety_checker=False,    variant='fp16').to(\"cuda\")pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(pipeline_img2img.config.requires_safety_checker)\"False\"\n\nå¦‚æœæ‚¨æƒ³æ›´æ”¹æ–°ç®¡é“çš„è¡Œä¸ºï¼Œæ‚¨å¯ä»¥è¦†ç›–åŸå§‹ç®¡é“ä¸­çš„ä»»ä½•å‚æ•°ç”šè‡³é…ç½®ã€‚ä¾‹å¦‚ï¼Œè¦é‡æ–°æ‰“å¼€å®‰å…¨æ£€æŸ¥å™¨å¹¶æ·»åŠ å¼ºåº¦å‚æ•°ï¼š\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(    pipeline_text2img,     requires_safety_checker=True,     strength=0.3)print(pipeline_img2img.config.requires_safety_checker)\"True\"\n\n\nNextAPI: AutoPipeline (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"4.è½½å…¥ LoRA è¿›è¡Œæ¨ç†","url":"/2024/04/10/4.%20%E8%BD%BD%E5%85%A5%20LoRA%20%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86/","content":"\n      \n        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\n      \n      \n        \n          \n          \n            è¯·è¾“å…¥ä¸ Rhodes Islandâ„¢ å–å¾—å¼±ç¥ç»è¿æ¥æ—¶çš„å£ä»¤ï¼š\n          \n        \n        \n      \n    \n    ","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"5.åŠ é€Ÿ T2I æ‰©æ•£æ¨¡å‹çš„æ¨ç†","url":"/2024/04/10/5.%20%E5%8A%A0%E9%80%9F%20T2I%20%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86/","content":"åŠ é€Ÿ T2I æ‰©æ•£æ¨¡å‹çš„æ¨ç†ç”±äºè¿­ä»£å’Œåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œæ‰©æ•£æ¨¡å‹æ¯” GAN æ¨¡å‹æ…¢ã€‚æœ‰å¤šç§æŠ€æœ¯å¯ä»¥è§£å†³æ­¤é™åˆ¶ï¼š\n\næ¸è¿›æ—¶é—´æ­¥è’¸é¦ (LCM LoRA)\næ¨¡å‹å‹ç¼© (SSD-1B) \né‡ç”¨é™å™ªå™¨çš„ç›¸é‚»ç‰¹å¾ (DeepCache)ã€‚\n\n\n\nPerforming inference with LCM-LoRA (huggingface.co)\n\nsegmind/SSD-1B Â· Hugging Face\n\nDeepCache (huggingface.co)\n\n\n\nä½†æ˜¯ï¼Œä¸ä¸€å®šéœ€è¦ä½¿ç”¨è¿™äº›æŠ€æœ¯æ¥åŠ é€Ÿæ¨ç†ã€‚ä»…ä½¿ç”¨ PyTorch 2ï¼Œæ‚¨å°±å¯ä»¥å°†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨ç†åŠ é€Ÿæœ€å¤š 3 å€ã€‚\né™ä½ç²¾åº¦-bfloat16å¯ç”¨ç¬¬ä¸€ä¸ªä¼˜åŒ–ï¼šé™ä½ç²¾åº¦æˆ–æ›´å…·ä½“åœ°è¯´ bfloat16ã€‚ä½¿ç”¨é™ä½çš„ç²¾åº¦æœ‰å‡ ä¸ªå¥½å¤„ï¼š\n\nä½¿ç”¨é™ä½çš„æ•°å€¼ç²¾åº¦ï¼ˆä¾‹å¦‚ float16 æˆ– bfloat16ï¼‰è¿›è¡Œæ¨ç†ä¸ä¼šå½±å“ç”Ÿæˆè´¨é‡ï¼Œä½†ä¼šæ˜¾è‘—æ”¹å–„æ¨ç†å»¶è¿Ÿã€‚\nä¸ float16 ç›¸æ¯”ï¼Œä½¿ç”¨ bfloat16 çš„ä¼˜åŠ¿å–å†³äºç¡¬ä»¶ï¼Œç°ä»£ GPU å€¾å‘äºä½¿ç”¨ bfloat16ã€‚\nä¸ float16 ç›¸æ¯”ï¼Œbfloat16 åœ¨ä¸é‡åŒ–ä¸€èµ·ä½¿ç”¨æ—¶æ›´å…·å¼¹æ€§ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨çš„æœ€æ–°ç‰ˆæœ¬çš„é‡åŒ–åº“ torchao ä¸å­˜åœ¨ float16 çš„æ•°å€¼é—®é¢˜ã€‚\n\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # ä½¿ç”¨float16    variant=\"fp16\").to(\"cuda\")# Run the attention ops without SDPA.pipe.unet.set_default_attn_processor()pipe.vae.set_default_attn_processor()prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\n\nSDPAAttentionæ¨¡å—è¿è¡Œè€—æ—¶é•¿ã€‚ä½¿ç”¨ PyTorch çš„ scaled_dot_product_attention å‡½æ•°åï¼Œå®ƒçš„æ•ˆç‡è¦é«˜å¾ˆå¤šã€‚ Diffusers ä¸­é»˜è®¤ä½¿ç”¨æ­¤å‡½æ•°ï¼Œå› æ­¤æ‚¨æ— éœ€å¯¹ä»£ç è¿›è¡Œä»»ä½•æ›´æ”¹ã€‚\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # ä½¿ç”¨float16    variant=\"fp16\").to(\"cuda\")prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\nSDPA (huggingface.co)\n\ntorch.compile\nWindows ä¸æ”¯æŒï¼š\nRuntimeError: Windows not yet supported for torch.compile\n\nPyTorch 2 åŒ…å« torch.compileï¼Œå®ƒä½¿ç”¨å¿«é€Ÿä¸”ä¼˜åŒ–çš„å†…æ ¸ã€‚åœ¨ Diffusers ä¸­ï¼Œé€šå¸¸ç¼–è¯‘ UNet å’Œ VAEæ¨¡å—ï¼Œå› ä¸ºå®ƒä»¬æ˜¯è®¡ç®—æœ€è€—æ—¶çš„æ¨¡å—ã€‚\nfrom diffusers import StableDiffusionXLPipelineimport torchtorch._inductor.config.conv_1x1_as_mm = Truetorch._inductor.config.coordinate_descent_tuning = Truetorch._inductor.config.epilogue_fusion = Falsetorch._inductor.config.coordinate_descent_check_all_directions = True\n\nåœ¨ç¼–è¯‘ UNet å’Œ VAE æ—¶å°†å…¶å†…å­˜å¸ƒå±€æ›´æ”¹ä¸ºâ€œchannels_lastâ€ä¹Ÿå¾ˆé‡è¦ï¼Œä»¥ç¡®ä¿æœ€å¤§é€Ÿåº¦ã€‚\npipe.unet.to(memory_format=torch.channels_last)pipe.vae.to(memory_format=torch.channels_last)\n\nç°åœ¨ç¼–è¯‘ UNet å’Œ VAE å¹¶æ‰§è¡Œæ¨ç†ï¼š\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)pipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\ntorch.compile æä¾›ä¸åŒçš„åç«¯å’Œæ¨¡å¼ã€‚ä¸ºäº†è·å¾—æœ€å¤§æ¨ç†é€Ÿåº¦ï¼Œè¯·å¯¹ inductor backend ä½¿ç”¨â€œmax-autotuneâ€ã€‚ \nâ€œmax-autotuneâ€ä½¿ç”¨ CUDA å›¾å¹¶ä¸“é—¨é’ˆå¯¹å»¶è¿Ÿä¼˜åŒ–ç¼–è¯‘å›¾ã€‚ CUDA å›¾é€šè¿‡ä½¿ç”¨é€šè¿‡å•ä¸ª CPU æ“ä½œå¯åŠ¨å¤šä¸ª GPU æ“ä½œçš„æœºåˆ¶ï¼Œå¤§å¤§å‡å°‘äº†å¯åŠ¨ GPU æ“ä½œçš„å¼€é”€ã€‚\nPrevent graph breaksæŒ‡å®š fullgraph=True å¯ç¡®ä¿åº•å±‚æ¨¡å‹ä¸­æ²¡æœ‰å›¾å½¢ä¸­æ–­ï¼Œä»¥å……åˆ†åˆ©ç”¨ torch.compileï¼Œè€Œä¸ä¼šé™ä½ä»»ä½•æ€§èƒ½ã€‚å¯¹äº UNet å’Œ VAEï¼Œè¿™æ„å‘³ç€æ›´æ”¹è®¿é—®è¿”å›å˜é‡çš„æ–¹å¼ã€‚\n- latents = unet(    latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds-).sample+ latents = unet(+   latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds, +   return_dict=False+)[0]\n\n\n\n\n\nç»„åˆAttentionæ¨¡å—çš„æŠ•å½±çŸ©é˜µSDXL ä¸­çš„ UNet å’Œ VAE ä½¿ç”¨ç±»ä¼¼ Transformer çš„æ¨¡å—ï¼Œç”±Attentionæ¨¡å—å’Œfeed-forwardæ¨¡å—ç»„æˆã€‚\nåœ¨Attentionæ¨¡å—ä¸­ï¼Œä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„æŠ•å½±çŸ©é˜µï¼ˆQã€K å’Œ Vï¼‰å°†è¾“å…¥æŠ•å½±åˆ°ä¸‰ä¸ªå­ç©ºé—´ä¸­ã€‚è¿™äº›æŠ•å½±åœ¨è¾“å…¥ä¸Šå•ç‹¬æ‰§è¡Œã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†æŠ•å½±çŸ©é˜µæ°´å¹³ç»„åˆæˆä¸€ä¸ªçŸ©é˜µå¹¶ä¸€æ­¥æ‰§è¡ŒæŠ•å½±ã€‚è¿™å¢åŠ äº†è¾“å…¥æŠ•å½±çš„çŸ©é˜µä¹˜æ³•çš„å¤§å°å¹¶æ”¹å–„äº†é‡åŒ–çš„å½±å“ã€‚\nåªéœ€ä¸€è¡Œä»£ç å³å¯ç»„åˆæŠ•å½±çŸ©é˜µï¼š\npipe.fuse_qkv_projections()\n\n\nè¿™ç§æ–¹æ³•æ•ˆæœä¼˜å…ˆï¼Œä¸”ä»…é€‚ç”¨äºéƒ¨åˆ†æ‰©æ•£æ¨¡å‹\n\nåŠ¨æ€é‡åŒ–è¿˜å¯ä»¥ä½¿ç”¨è¶…è½»é‡çº§ PyTorch é‡åŒ–åº“ torchaoï¼ˆcommit SHA 54bcd5a10d0abbe7b0c045052029257099f83fd9ï¼‰å°†åŠ¨æ€ int8 é‡åŒ–åº”ç”¨äº UNet å’Œ VAEã€‚é‡åŒ–ç»™æ¨¡å‹å¢åŠ äº†é¢å¤–çš„è½¬æ¢å¼€é”€ï¼Œå¯ä»¥é€šè¿‡æ›´å¿«çš„ matmulsï¼ˆåŠ¨æ€é‡åŒ–ï¼‰æ¥å¼¥è¡¥ã€‚\nå¦‚æœçŸ©é˜µç›¸ä¹˜å¤ªå°ï¼Œè¿™äº›æŠ€æœ¯å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚\nNextSpeed up inference (huggingface.co)\nSDPA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"YAML","url":"/2024/03/14/YAML%E5%9F%BA%E7%A1%80/","content":"YAMLYAML Ainâ€™t a Markup Language\nYet Another Markup Language\nYAMLæ˜¯â€YAML Ainâ€™t a Markup Languageâ€ï¼ˆYAMLä¸æ˜¯ä¸€ç§æ ‡è®°è¯­è¨€ï¼‰çš„é€’å½’ç¼©å†™ã€‚åœ¨å¼€å‘çš„è¿™ç§è¯­è¨€æ—¶ï¼ŒYAMLçš„æ„æ€å…¶å®æ˜¯ï¼šâ€Yet Another Markup Languageâ€ï¼ˆä»æ˜¯ä¸€ç§æ ‡è®°è¯­è¨€ï¼‰ï¼Œä½†ä¸ºäº†å¼ºè°ƒè¿™ç§è¯­è¨€ä»¥æ•°æ®ä¸ºä¸­å¿ƒï¼Œè€Œä¸æ˜¯ä»¥æ ‡è®°è¯­è¨€ä¸ºé‡ç‚¹ï¼Œè€Œç”¨åå‘ç¼©ç•¥è¯­é‡å‘½åã€‚YAML (wikipedia.org)\nYAMLç‰¹ç‚¹æ˜¯ä½¿ç”¨ç©ºæ ¼æ¥è¡¨è¾¾å±‚æ¬¡ç»“æ„ï¼Œç‰¹åˆ«é€‚åˆç”¨æ¥è¡¨è¾¾æˆ–ç¼–è¾‘æ•°æ®ç»“æ„ã€å„ç§é…ç½®æ–‡ä»¶ï¼Œå…¶æ–‡ä»¶ä¸€èˆ¬ä»¥ .yaml ä¸ºåç¼€ã€‚\nåŸºæœ¬è¯­æ³•\nä»¥ k: v çš„å½¢å¼æ¥è¡¨ç¤ºé”®å€¼å¯¹çš„å…³ç³»\n\nå†’å·åé¢å¿…é¡»æœ‰ä¸€ä¸ªç©ºæ ¼\n\n\nåªæ”¯æŒå•è¡Œæ³¨é‡Šï¼Œæ³¨é‡Šç¬¦å·ï¼š# \n\nå¤§å°å†™æ•æ„Ÿ\n\né€šè¿‡ç¼©è¿›æ¥è¡¨ç¤ºå±‚çº§å…³ç³»\n\nç¼©æ’ä¸­ç©ºæ ¼çš„æ•°ç›®ä¸é‡è¦ï¼Œåªè¦ç›¸åŒé˜¶å±‚çš„å…ƒç´ å·¦ä¾§å¯¹é½å°±å¯ä»¥äº†\nç¼©è¿›åªèƒ½ä½¿ç”¨ç©ºæ ¼ï¼Œä¸èƒ½ä½¿ç”¨ tab ç¼©è¿›\n\n\nå­—ç¬¦ä¸²å¯ä»¥ä¸ç”¨åŒå¼•å·\n\nä¸€ä¸ªæ–‡ä»¶ä¸­å¯ä»¥åŒ…å«å¤šä¸ªæ–‡ä»¶çš„å†…å®¹\n\nç”¨--- å³ä¸‰ä¸ªç ´æŠ˜å·è¡¨ç¤ºä¸€ä»½å†…å®¹çš„å¼€å§‹\nç”¨...å³ä¸‰ä¸ªå°æ•°ç‚¹è¡¨ç¤ºä¸€ä»½å†…å®¹çš„ç»“æŸï¼ˆéå¿…éœ€ï¼‰\n\n\n\næ•°æ®ç»“æ„ä¸ç±»å‹å¯¹è±¡ä»¥é”®å€¼å¯¹ key: value å½¢å¼ç»„ç»‡æ•°æ®\n1. ä½¿ç”¨**å†’å·+ç©ºæ ¼**æ¥åˆ†å¼€é”®ä¸å€¼\n1. æ”¯æŒå¤šå±‚åµŒå¥—ï¼ˆ**ç”¨ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»**ï¼‰\n\nmodel:  base_learning_rate: 4.5e-6  target: ldm.models.autoencoder.AutoencoderKL  params:    monitor: &quot;val/rec_loss&quot;    embed_dim: 64    lossconfig:      target: ldm.modules.losses.LPIPSWithDiscriminator      params:        disc_start: 50001        kl_weight: 0.000001        disc_weight: 0.5\n\n\næ”¯æŒæµå¼é£æ ¼ï¼ˆFlow styleï¼‰çš„è¯­æ³•ï¼šç”¨èŠ±æ‹¬å·åŒ…è£¹ï¼Œç”¨é€—å·åŠ ç©ºæ ¼åˆ†éš”\n\nkey: &#123;child-key1: value1, child-key2: value2 &#125;\n\n\n\næ•°ç»„\nä¸€ç»„ä»¥åŒºå—æ ¼å¼ï¼ˆâ€œç ´æŠ˜å·+ç©ºæ ¼â€ï¼‰å¼€å¤´çš„æ•°æ®ç»„æˆä¸€ä¸ªæ•°ç»„\n\nunet_config:  target: ldm.modules.diffusionmodules.openaimodel.UNetModel  params:    image_size: 64    in_channels: 3    out_channels: 3    model_channels: 224    attention_resolutions:    - 8    - 4    - 2    num_res_blocks: 2    channel_mult:    - 1    - 2    - 3    - 4    num_head_channels: 32\n\n\nä¹Ÿæ”¯æŒå†…è”æ ¼å¼æ¥è¡¨è¾¾ï¼ˆç”¨æ–¹æ‹¬å·åŒ…è£¹ï¼Œé€—å·åŠ ç©ºæ ¼åˆ†éš”ï¼‰\n\nddconfig:  double_z: True  z_channels: 64  resolution: 256  in_channels: 3  out_ch: 3  ch: 128  ch_mult: [1, 1, 2, 2, 4, 4]    num_res_blocks: 2  attn_resolutions: [16, 8]  dropout: 0.0\n\n\næ”¯æŒå¤šç»´æ•°ç»„ï¼ˆç”¨ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»ï¼‰\n\nvalues:  - - 1    - 2  - - 3    - 4# ç­‰ä»·ï¼š    values: [[1, 2], [3, 4]]\n\n\n\nå­—ç¬¦ä¸²\nå­—ç¬¦ä¸²ä¸€èˆ¬ä¸éœ€è¦ç”¨å¼•å·åŒ…è£¹\nå­—ç¬¦ä¸²æ¢è¡Œè§†ä¸ºä¸€ä¸ªç©ºæ ¼\nå•å¼•å·å¯ä»¥å±è”½è½¬ä¹‰\nå­—ç¬¦ä¸²ä¸­éœ€è¦ä½¿ç”¨è½¬ä¹‰å­—ç¬¦\\å°±å¿…é¡»ä½¿ç”¨åŒå¼•å·åŒ…è£¹\n\nstrings:  - Hello world # ä¸ç”¨å¼•å·åŒ…è£¹  - Hello     world # æ¢è¡Œè§†ä¸ºä¸€ä¸ªç©ºæ ¼  - &#x27;å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º&#x27;  # å•å¼•å·å¯ä»¥å±è”½è½¬ä¹‰  - &quot;å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º&quot;  # åŒå¼•å·ä½¿ç”¨è½¬ç§»ç¬¦å·# ç»“æœï¼š- Hello world- Hello world- å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º- &#x27;å­—ç¬¦ä¸²  æ¢è¡Œ  æ¼”ç¤º&#x27;\n\n\nä¿ç•™æ¢è¡Œï¼šä½¿ç”¨ç«–çº¿ç¬¦â€œ | â€æ¥è¡¨ç¤ºè¯¥è¯­æ³•ï¼Œæ¯è¡Œçš„ç¼©è¿›å’Œè¡Œå°¾ç©ºç™½éƒ½ä¼šè¢«å»æ‰ï¼Œè€Œé¢å¤–çš„ç¼©è¿›ä¼šè¢«ä¿ç•™\n\nlines: |  æˆ‘æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ    æˆ‘æ˜¯å´å½¦ç¥–      æˆ‘æ˜¯ç¬¬å››è¡Œ  æˆ‘æ˜¯ç¬¬äº”è¡Œ  # ç»“æœ&quot;æˆ‘æ˜¯ç¬¬ä¸€è¡Œ\\næˆ‘æ˜¯ç¬¬äºŒè¡Œ\\n  æˆ‘æ˜¯å´å½¦ç¥–\\n    æˆ‘æ˜¯ç¬¬å››è¡Œ\\næˆ‘æ˜¯ç¬¬äº”è¡Œ\\n&quot;\n\n\næŠ˜å æ¢è¡Œï¼šä½¿ç”¨å³å°–æ‹¬å·â€œ &gt; â€æ¥è¡¨ç¤ºè¯¥è¯­æ³•ï¼Œåªæœ‰ç©ºç™½è¡Œæ‰ä¼šè¢«è¯†åˆ«ä¸ºæ¢è¡Œï¼ŒåŸæ¥çš„æ¢è¡Œç¬¦éƒ½ä¼šè¢«è½¬æ¢æˆç©ºæ ¼\n\nlines: &gt;  æˆ‘æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä»æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä¾æ—§æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ  è¿™ä¹ˆå·§æˆ‘ä¹Ÿæ˜¯ç¬¬äºŒè¡Œ# ç»“æœlines2: &#x27;æˆ‘æ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä»æ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä¾æ—§æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ è¿™ä¹ˆå·§æˆ‘ä¹Ÿæ˜¯ç¬¬äºŒè¡Œ  &#x27;\n\n\n\nå¸ƒå°”å€¼\nâ€œtrueâ€ã€â€œTrueâ€ã€â€œTRUEâ€ã€â€œyesâ€ã€â€œYesâ€å’Œâ€œYESâ€çš†ä¸ºçœŸ\nâ€œfalseâ€ã€â€œFalseâ€ã€â€œFALSEâ€ã€â€œnoâ€ã€â€œNoâ€å’Œâ€œNOâ€çš†ä¸ºå‡\n\næ•´æ•°\næ”¯æŒäºŒè¿›åˆ¶è¡¨ç¤º\n\nint:  - 666  - 0001_0000# ç»“æœint:- 666- 4096\n\n\n\næµ®ç‚¹æ•°\næ”¯æŒç§‘å­¦è®¡æ•°æ³•\n\nfloat:  - 3.14  - 6.8523015e+5 # ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•# ç»“æœfloat:- 3.14- 685230.15\n\nç©º Nullnullã€Nullã€~ å’Œä¸æŒ‡å®šå€¼éƒ½è¡¨ç¤ºç©º\nnulls:  - null  - Null  - ~  -# ç»“æœnulls:- null- null- null- null\n\n\n\nå¼ºåˆ¶ç±»å‹è½¬æ¢åŒæ„Ÿå¹å·+ç›®æ ‡ç±»å‹æ¥å¼ºåˆ¶è½¬æ¢ç±»å‹\na: !!float &#x27;666&#x27; # !! ä¸ºä¸¥æ ¼ç±»å‹æ ‡ç­¾b: !!int &#x27;666&#x27;   # å­—ç¬¦ä¸²è½¬ä¸ºæ•´å‹c: !!str 666     # æ•´æ•°è½¬ä¸ºå­—ç¬¦ä¸²d: !!str 666.66  # æµ®ç‚¹æ•°è½¬ä¸ºå­—ç¬¦ä¸²e: !!str true    # å¸ƒå°”å€¼è½¬ä¸ºå­—ç¬¦ä¸²f: !!bool &#x27;yes&#x27;  # å­—ç¬¦ä¸²è½¬ä¸ºå¸ƒå°”å€¼# ç»“æœa: 666.0b: 666c: &#x27;666&#x27;d: &#x27;666.66&#x27;e: &#x27;true&#x27;f: true\n\n\n\næ•°æ®å¤ç”¨ä¸åˆå¹¶æ•°æ®å¤ç”¨åœ¨keyçš„å†’å·åï¼Œä½¿ç”¨é”šç‚¹ç¬¦å·&amp;è®¾å®šé”šç‚¹ï¼Œä½¿ç”¨å¼•ç”¨ç¬¦å·*å¼•ç”¨é”šç‚¹\nmodel: &amp;all_parm  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params: &amp;model_parm    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model: *all_parmnew_params: *model_parm# ç»“æœnew_model:  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_params:  linear_start: 0.0015  linear_end: 0.0195  num_timesteps_cond: 1  log_every_t: 200  timesteps: 1000  first_stage_key: image  image_size: 64  channels: 3  monitor: val/loss_simple_ema\n\n\n\næ•°æ®åˆå¹¶åˆå¹¶æ ‡ç­¾ç¬¦å·â€œ&lt;&lt;â€é…åˆé”šç‚¹ç¬¦å·å’Œå¼•ç”¨ç¬¦å·ä½¿ç”¨å¯ä»¥ä¸ä»»æ„æ•°æ®è¿›è¡Œåˆå¹¶ï¼Œå¯ä»¥è§†ä¸ºé¢å‘å¯¹è±¡ä¸­çš„ç»§æ‰¿\nmodel_location: &amp;loc  target: ldm.models.diffusion.ddpm.LatentDiffusionmodel_params: &amp;params  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model:  base_learning_rate: 2.0e-06  &lt;&lt;: *loc  &lt;&lt;: *params  # ç»“æœnew_model:  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_ema  base_learning_rate: 2.0e-06\n\n\n\nå‚è€ƒä¸€æ–‡çœ‹æ‡‚ YAML - çŸ¥ä¹ (zhihu.com)\n","categories":["Tech"],"tags":["python"]},{"title":"condaå¸¸ç”¨å‘½ä»¤","url":"/2021/11/25/conda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/","content":"condaå¸¸ç”¨å‘½ä»¤åœ¨windowsçš„cmdä¸‹ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤è¿›å…¥condaï¼š\nactivate\n\nç¯å¢ƒç®¡ç†åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼šconda create -n [env_name] python=[X.X]\n\n\nenv_nameï¼šè¦åˆ›å»ºçš„ç¯å¢ƒçš„åå­—\nX.Xï¼šè¦åˆ›å»ºçš„ç¯å¢ƒçš„pythonçš„ç‰ˆæœ¬ï¼Œå¦‚3.7\n\næ¿€æ´»è™šæ‹Ÿç¯å¢ƒconda activate [env_name]\n\nåœç”¨å½“å‰ç¯å¢ƒconda deactivate\n\næŸ¥çœ‹å½“å‰ç¯å¢ƒçš„pythonç‰ˆæœ¬python --version\n\næŸ¥çœ‹æ‰€æœ‰å­˜åœ¨çš„è™šæ‹Ÿç¯å¢ƒconda info -econda env list\n\nåˆ é™¤è™šæ‹Ÿç¯å¢ƒï¼šconda remove -n [env_name] --all\n\né‡å‘½åç¯å¢ƒ\ncondaæ²¡æœ‰ç›´æ¥é‡å‘½åç¯å¢ƒçš„åŠŸèƒ½ï¼Œä½†å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤å®Œæˆï¼š\nå…‹éš†è¦é‡å‘½åçš„ç¯å¢ƒ\nå°†åŸç¯å¢ƒåˆ é™¤\n\n\n\nconda create --name [newname] --clone [oldname]conda remove --name [oldname] --all\n\n\n\nåŒ…ç®¡ç†å®‰è£…åŒ…conda install [pac_name]=[åŒ…çš„ç‰ˆæœ¬å·]\n\næŸ¥çœ‹å·²ç»å®‰è£…çš„åŒ…\næŸ¥çœ‹å½“å‰ç¯å¢ƒï¼š\n\nconda listpip list\n\n\næŸ¥çœ‹æŒ‡å®šç¯å¢ƒï¼š\n\nconda list -n [env_name]\n\nåˆ é™¤åŒ…conda uninstall [pac_name]\n\næ›´æ–°æŒ‡å®šåŒ…conda update [pac_name]pip install [pac_name] -U\n\næ¸…ç†åŒ…\né€šè¿‡ä»¥ä¸‹æŒ‡ä»¤æ¥åˆ é™¤ä¸€äº›æ²¡ç”¨çš„åŒ…ï¼Œè¿™ä¸ªå‘½ä»¤ä¼šæ£€æŸ¥å“ªäº›åŒ…æ²¡æœ‰åœ¨åŒ…ç¼“å­˜ä¸­è¢«ç¡¬ä¾èµ–åˆ°å…¶ä»–åœ°æ–¹ï¼Œå¹¶åˆ é™¤å®ƒä»¬\n\nconda clean -p\n\n\nåˆ é™¤condaä¿å­˜ä¸‹æ¥çš„taråŒ…\n\nconda clean -t\n\n\nåˆ é™¤æ‰€æœ‰çš„å®‰è£…åŒ…åŠcache\n\nconda clean -y --all\n\næ›´æ–°condaconda update conda\n\nå®‰è£…requirements.txtæ–‡ä»¶å†…çš„åŒ…\né¦–å…ˆé€šè¿‡cdæŒ‡ä»¤è¿›å…¥requirements.txtæ–‡ä»¶æ‰€åœ¨è·¯å¾„ï¼Œç„¶åæ‰§è¡Œå¦‚ä¸‹æŒ‡ä»¤å³å¯\n\npip install -r requirements.txt\n\nåŒ…çš„æ•°æ®æºç®¡ç†\næ˜¾ç¤ºç›®å‰condaçš„æ•°æ®æºæœ‰å“ªäº›ï¼š\n\nconda config --show channels\n\n\næ·»åŠ æ•°æ®æºï¼š(æ¸…åæº)\n\nconda config --add https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\nåˆ é™¤æ•°æ®æº\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\nå®‰è£…å‡ºç°å¼‚å¸¸ï¼šåˆ›å»ºæ–‡ä»¶ï¼šC:\\Users\\[user name]\\AppData\\Roaming\\pip\\pip.iniå†™å…¥ï¼š[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host = mirrors.aliyun.com\n\n","categories":["Tech"],"tags":["python"]},{"title":"PPTæ³¨æ„äº‹é¡¹","url":"/2023/12/25/ppt%E5%88%B6%E4%BD%9C/","content":"PPTæ³¨æ„äº‹é¡¹\næ— è¡¬çº¿å­—ä½“\nè‹±æ–‡é¦–å­—æ¯å¤§å†™å³å¯\nå¸¸è§„å†…å®¹ï¼š18-36å­—å·ï¼Œåº•éƒ¨å’Œå¼•ç”¨å­—å·&lt;12\nç©ºç™½ç®€å•èƒŒæ™¯\nå¾½æ ‡ä¸è¦åœ¨å†…å®¹é¡µå‡ºç°ï¼Œç”¨äºé¦–é¡µã€è¿‡æ¸¡é¡µã€å°¾é¡µ\né¿å…é«˜é¥±å’Œé¢œè‰²çš„æ’è‰²ï¼Œé»‘ç™½æ°¸ä¸è¿‡æ—¶\nç»™é¡µé¢ç•™ç™½ï¼šä¾§é¢ç•™å‡ºç©ºé—´ï¼Œåº•éƒ¨ä¸è¦æ”¾å¤ªå¤šå†…å®¹\næ ‡é¢˜ï¼šæ¯é¡µéƒ½è¦æœ‰æ ‡é¢˜ï¼Œä¸€ä¸ªç®€å•å¥ã€ä¸è¦è¶…è¿‡ä¸¤è¡Œ\nä¸èƒ½å‡ºç°å¤§æ®µæ–‡å­—\nå•é¡µä¸èƒ½æœ‰è¿‡å¤šå†…å®¹ï¼Œç‹¬ç«‹å†…å®¹æ”¾åœ¨ä¸åŒé¡µï¼Œé¿å…å¤±å»ç„¦ç‚¹\nåŒä¸€é¡µåˆ—è¡¨ä¸è¦è¶…è¿‡3ä¸ªæ¡ç›®\nå›¾è¡¨çš„å…¨éƒ¨è¦ç´ éƒ½è¦è§£é‡Šæ¸…æ¥š\nè€ƒè™‘ä¸åŒæ—¶é—´é™åˆ¶çš„æƒ…å†µä¸‹å†…å®¹çš„å®‰æ’\nåŠ¨ç”»ï¼šå°‘å°±æ˜¯å¤šï¼Œç®€å•ä¸ºä¸»ã€‚ç”¨æ¥è¡¨è¾¾é€’è¿›ã€æ”¾å¤§ã€è¿›ä¸€æ­¥ã€å˜åŒ–ç­‰é€»è¾‘\né¡µé¢åˆ‡æ¢ï¼šå¹³æ»‘\næ€»ç»“ï¼šå¼ºè°ƒé‡è¦å†…å®¹ï¼Œå¢åŠ é¡µé¢çš„æ€»ç»“å’Œé¡µé¢ä¹‹é—´çš„ä¸²è”è®²è§£ï¼Œè®©å¬ä¼—æ˜ç™½å½“å‰çš„æ¼”è®²å¤„äºä»€ä¹ˆé˜¶æ®µ\n\nåŸºæœ¬åŸåˆ™\nå§‹ç»ˆè€ƒè™‘å¬ä¼—å¦‚ä½•æ›´å®¹æ˜“çš„æ¥å—å†…å®¹\nä¸æ‰“ç®—èŠçš„å†…å®¹åˆ é™¤\nå¥½çš„æ¼”è®²å§‹äºä¸€ä¸ªå¥½é—®é¢˜\nä¸€é¡µä¸­ä¿æŒä¸€ä¸ªå†…å®¹\n\n","categories":["Note"]},{"title":"AccelerateåŸºæœ¬ä½¿ç”¨æ–¹æ³•","url":"/2024/04/19/Accelerate%20base/","content":"Accelerate-NoteAccelerate æ˜¯ä¸€ä¸ªç”± Hugging Face å¼€å‘çš„ Python åº“ï¼Œå®ƒå…è®¸å¼€å‘è€…å°†ç›¸åŒçš„ PyTorch ä»£ç è¿è¡Œåœ¨ä»»ä½•åˆ†å¸ƒå¼é…ç½®ä¸Šï¼Œåªéœ€æ·»åŠ å››è¡Œä»£ç å³å¯ã€‚è¿™ä¸ªåº“ç®€åŒ–äº†åœ¨å¤šç§ç¯å¢ƒï¼ˆåŒ…æ‹¬å•æœºã€å¤š GPUã€TPU å’Œå„ç§åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼‰ä¸­è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒçš„è¿‡ç¨‹ã€‚å®ƒåŸºäº torch_xla å’Œ torch.distributed æ„å»ºï¼Œå¯ä»¥è½»æ¾åœ°å°†ç°æœ‰ä»£ç åº“è½¬æ¢ä¸ºä½¿ç”¨ DeepSpeed è¿›è¡Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œå¤„ç†ï¼Œå¹¶è‡ªåŠ¨æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚\nå°†Accelerateæ·»åŠ åˆ°PyTorchä»£ç ä¸­Accelerate æä¾›äº†ä¸€ç§å‹å¥½çš„æ–¹å¼æ¥é€‚åº”å„ç±»åˆ†å¸ƒå¼æ¡†æ¶ï¼Œè€Œæ— éœ€å­¦ä¹ æ¯ä¸ªæ¡†æ¶çš„å…·ä½“ç»†èŠ‚ã€‚\næ¥ä¸‹æ¥å°†ä»ä¸€ä¸ªåŸºæœ¬çš„ PyTorch è®­ç»ƒå¾ªç¯å¼€å§‹ï¼ˆå‡è®¾æ¨¡å‹å’Œä¼˜åŒ–å™¨ç­‰æ‰€æœ‰è®­ç»ƒå¯¹è±¡éƒ½å·²è®¾ç½®å®Œæ¯•ï¼‰ï¼Œç„¶åé€æ­¥å°† Accelerate é›†æˆåˆ°å…¶ä¸­ã€‚\n# åˆå§‹åŒ–model = ...                # torch.nn.Moduletraining_dataloader = ...  # torch.utils.data.DataLoaderoptimizer = ...            # torch.optimscheduler = ...            # torch.optim.lr_schedulerdevice = \"cuda\"model.to(device)for batch in training_dataloader:    optimizer.zero_grad()    inputs, targets = batch    inputs = inputs.to(device)    targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)    loss.backward()    optimizer.step()    scheduler.step()\n\nå®Œæ•´å®ä¾‹å¦‚ä¸‹ï¼š\nimport torchfrom torch import nnfrom torch import optimimport torch.nn.functional as Ffrom torch.optim import lr_schedulerfrom torch.utils.data import DataLoaderfrom torch.utils.data import random_splitimport torchvision.datasets as datasetsimport torchvision.transforms as transformsfrom torch.utils.tensorboard import SummaryWriterfrom tqdm import tqdmdef check_accuracy(loader, model):    num_correct = 0    num_samples = 0    model.eval()    with torch.no_grad():        # Loop through the data        for x, y in loader:            # Move data to device            x = x.to(device=device)            y = y.to(device=device)            # Get to correct shape            x = x.reshape(x.shape[0], -1)            # Forward pass            scores = model(x)            _, predictions = scores.max(1)            # Check how many we got correct            num_correct += (predictions == y).sum()            # Keep track of number of samples            num_samples += predictions.size(0)    model.train()    return num_correct / num_samples# è¶…å‚æ•°è®¾ç½®input_size = 784num_classes = 10learning_rate = 0.001batch_size = 64num_epochs = 10sw = SummaryWriter('./mnist-logs')# 0. è®¾å¤‡è®¾ç½®device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 1. æ¨¡å‹class NN(nn.Module):    def __init__(self, input_size, num_classes):        super().__init__()        self.fc1 = nn.Linear(input_size, 50)        self.fc2 = nn.Linear(50, num_classes)    def forward(self, x):        x = F.relu(self.fc1(x))        x = self.fc2(x)        return xmodel = NN(input_size=input_size, num_classes=num_classes).to(device)# 2. æ•°æ®åŠ è½½å™¨entire_dataset = datasets.MNIST(    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)train_ds, val_ds = random_split(entire_dataset, [50000, 10000])test_ds = datasets.MNIST(    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)val_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)# 3.ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨optimizer = optim.Adam(    model.parameters(),    lr=learning_rate)scheduler = lr_scheduler.CosineAnnealingWarmRestarts(    optimizer,    T_0=2,    eta_min=1e-6)# æŸå¤±å‡½æ•°loss_func = nn.CrossEntropyLoss()# è®­ç»ƒå¾ªç¯len_per_epoch = len(train_loader)global_step = 0for epoch in range(num_epochs):    for i, (data, targets) in enumerate(tqdm(train_loader)):        # æ¸…ç©ºæ¢¯åº¦        optimizer.zero_grad()        # æ•°æ®ç§»è‡³device        data = data.to(device=device)        targets = targets.to(device=device)        # è®¡ç®—æŸå¤±        data = data.reshape(data.shape[0], -1)        scores = model(data)        loss = loss_func(scores, targets)        # Backward        loss.backward()        # Gradient descent or adam step        optimizer.step()        scheduler.step(epoch + i / len_per_epoch)        global_step += 1        sw.add_scalar('lr', scheduler.get_last_lr()[0], global_step=global_step)    val_accuracy = check_accuracy(val_loader, model) * 100    sw.add_scalar('val_accuracy', val_accuracy, global_step=epoch)# Check accuracy on training &amp; test to see how good our modelmodel.to(device)print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")print(f\"Accuracy on validation set: {check_accuracy(val_loader, model)*100:.2f}\")print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n\nAcceleratorAccelerator æ˜¯ç”¨äºè°ƒæ•´ä»£ç ä»¥ä¸ Accelerate é…åˆä½¿ç”¨çš„ä¸»ç±»ã€‚å®ƒäº†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„åˆ†å¸ƒå¼è®¾ç½®ï¼Œå¦‚ä¸åŒè¿›ç¨‹çš„æ•°é‡å’Œç¡¬ä»¶ç±»å‹ã€‚ä½¿æ‚¨çš„ PyTorch ä»£ç èƒ½å¤Ÿåœ¨ä»»ä½•åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­å·¥ä½œï¼Œå¹¶ç®¡ç†å’Œæ‰§è¡Œè·¨è®¾å¤‡è¿›ç¨‹ã€‚\nä»å¯¼å…¥å’Œåˆ›å»º Accelerator å®ä¾‹å¼€å§‹ï¼š\nfrom accelerate import Acceleratoraccelerator = Accelerator()\n\nAccelerator çŸ¥é“å°† PyTorch å¯¹è±¡ç§»åŠ¨åˆ°å“ªä¸ªè®¾å¤‡ä¸Šï¼Œå› æ­¤å»ºè®®è®© Accelerate è®¾ç½® device å‚æ•°ï¼š\n- device = \"cuda\"+ device = accelerator.device  model.to(device)\n\nå‡†å¤‡ PyTorch å¯¹è±¡æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä¸ºåˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡å¥½ PyTorch å¯¹è±¡ï¼ˆæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€æ•°æ®åŠ è½½å™¨ç­‰ï¼‰ã€‚\nprepare() æ–¹æ³•ä¼šæ ¹æ®è®­ç»ƒè®¾ç½®å°†æ¨¡å‹æ”¾å…¥é€‚å½“çš„å®¹å™¨ï¼ˆå¦‚å•GPUæˆ–å¤šGPUï¼‰ä¸­ï¼Œè°ƒæ•´ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ä»¥ä½¿ç”¨ Accelerate çš„ AcceleratedOptimizer å’Œ AcceleratedSchedulerï¼Œå¹¶åˆ›å»ºå¯è·¨è¿›ç¨‹åˆ†ç‰‡çš„æ–°æ•°æ®åŠ è½½å™¨ã€‚\n\nprepare() æ–¹æ³•ä»…å‡†å¤‡ä» PyTorch ç±»ç»§æ‰¿çš„å¯¹è±¡ï¼Œä¾‹å¦‚ torch.optim.Optimizerã€‚\n\nPyTorch å¯¹è±¡çš„è¿”å›é¡ºåºä¸è¾“å…¥é¡ºåºç›¸åŒï¼š\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(    model, optimizer, training_dataloader, scheduler)\n\n\n\nè®­ç»ƒå¾ªç¯æœ€åï¼Œåˆ é™¤è®­ç»ƒå¾ªç¯ä¸­å¯¹è¾“å…¥å’Œç›®æ ‡æ•°æ®çš„ to(device) è°ƒç”¨ï¼Œå› ä¸º Accelerate çš„ DataLoader ç±»ä¼šè‡ªåŠ¨å°†å®ƒä»¬æ”¾ç½®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šã€‚\næ‚¨è¿˜åº”è¯¥å°†é€šå¸¸çš„loss.backward()ä¼ é€’æ›¿æ¢ä¸º Accelerate çš„backward()æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šä¸ºæ‚¨ç¼©æ”¾æ¢¯åº¦ï¼Œå¹¶æ ¹æ®æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®ï¼ˆä¾‹å¦‚ï¼ŒDeepSpeedæˆ–Megatronï¼‰ä½¿ç”¨é€‚å½“çš„backward()æ–¹æ³•ã€‚\n-   inputs = inputs.to(device)-   targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)-   loss.backward()+   accelerator.backward(loss)\n\n\n\næ€»ç»“å°†æ‰€æœ‰å†…å®¹æ”¾åœ¨ä¸€èµ·ï¼Œæ‚¨çš„ Accelerate è®­ç»ƒå¾ªç¯ç°åœ¨åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼\nfrom accelerate import Acceleratoraccelerator = Accelerator()# åˆå§‹åŒ–model = ...                # torch.nn.Moduletraining_dataloader = ...  # torch.utils.data.DataLoaderoptimizer = ...            # torch.optimscheduler = ...            # torch.optim.lr_schedulerdevice = accelerator.devicemodel, optimizer, training_dataloader, scheduler = accelerator.prepare(    model, optimizer, training_dataloader, scheduler)for batch in training_dataloader:    optimizer.zero_grad()        inputs, targets = batch        outputs = model(inputs)    loss = loss_function(outputs, targets)        accelerator.backward(loss)        optimizer.step()    scheduler.step()\n\n\n\nTraining featuresAccelerate æä¾›äº†é¢å¤–çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ¢¯åº¦ç´¯ç§¯ (gradient accumulation)ã€æ¢¯åº¦è£å‰ª (gradient clipping)ã€æ··åˆç²¾åº¦è®­ç»ƒ (mixed precision training)ç­‰ï¼Œæ‚¨å¯ä»¥å°†å…¶æ·»åŠ åˆ°è„šæœ¬ä¸­ä»¥æ”¹è¿›è®­ç»ƒã€‚\næ¢¯åº¦ç´¯ç§¯æ¢¯åº¦ç´¯ç§¯ä½¿æ‚¨èƒ½å¤Ÿåœ¨æ›´æ–°æƒé‡ä¹‹å‰é€šè¿‡ç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦æ¥è·å–æ›´å¤§çš„ç­‰æ•ˆ batch_sizeã€‚è¿™å¯¹äºè§£å†³æ˜¾å­˜å¯¹ batch_size çš„é™åˆ¶å¾ˆæœ‰ç”¨ã€‚\nè¦åœ¨ Accelerate ä¸­å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨åŠ é€Ÿå™¨ç±»ä¸­æŒ‡å®š gradient_accumulation_steps å‚æ•°ï¼Œå¹¶åœ¨è„šæœ¬ä¸­æ·»åŠ  accumulate() ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š\n+ accelerator = Accelerator(gradient_accumulation_steps=2)  model, optimizer, training_dataloader = accelerator.prepare(      model, optimizer, training_dataloader  )  for input, label in training_dataloader:+     with accelerator.accumulate(model):          predictions = model(input)          loss = loss_function(predictions, label)          accelerator.backward(loss)          optimizer.step()          scheduler.step()          optimizer.zero_grad()\n\næ¢¯åº¦è£å‰ªæ¢¯åº¦è£å‰ªæ˜¯ä¸€ç§é˜²æ­¢â€œæ¢¯åº¦çˆ†ç‚¸â€çš„æŠ€æœ¯ï¼ŒAccelerate æä¾›ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š\n\nclip_grad_value_ï¼šå°†å¯è¿­ä»£å‚æ•°çš„æ¢¯åº¦è£å‰ªä¸ºæŒ‡å®šå€¼ã€‚ æ¢¯åº¦å°±åœ°ä¿®æ”¹ï¼ˆin-placeï¼‰ã€‚\nparametresï¼šå¯è¿­ä»£çš„å¼ é‡æˆ–å•ä¸ªå¼ é‡ï¼Œå…¶æ¢¯åº¦å°†å½’ä¸€åŒ–\nclip_valueï¼šæ¢¯åº¦çš„é˜ˆå€¼ã€‚æ¢¯åº¦è¢«é™åˆ¶åœ¨èŒƒå›´å†…\n\n\nclip_grad_norm_ï¼šèŒƒæ•°æ˜¯å¯¹æ‰€æœ‰æ¢¯åº¦ä¸€èµ·è®¡ç®—çš„ã€‚æ¢¯åº¦å°±åœ°ä¿®æ”¹ã€‚\nparametersï¼šå¯è¿­ä»£çš„å¼ é‡æˆ–å•ä¸ªå¼ é‡ï¼Œå…¶æ¢¯åº¦å°†å½’ä¸€åŒ–\nmax_normï¼šæ¢¯åº¦çš„æœ€å¤§èŒƒæ•°\nnorm_typeï¼šfloatï¼Œé»˜è®¤ä¸º2.0ï¼Œç”¨çš„ p-èŒƒæ•°çš„ç±»å‹ã€‚infè¡¨ç¤ºæ— ç©·èŒƒæ•°ã€‚\n\n\n\nfrom accelerate import Acceleratoraccelerator = Accelerator(gradient_accumulation_steps=2)dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)for input, target in dataloader:     optimizer.zero_grad()     output = model(input)     loss = loss_func(output, target)     accelerator.backward(loss)     if accelerator.sync_gradients:     # äºŒè€…å–å…¶ä¸€ï¼š    \taccelerator.clip_grad_value_(model.parameters(), clip_value)        accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)             optimizer.step()\n\n\n\næ··åˆç²¾åº¦è®­ç»ƒæ··åˆç²¾åº¦é€šè¿‡ä½¿ç”¨ fp16ï¼ˆåŠç²¾åº¦ï¼‰ç­‰è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚è¦æƒ³ä½¿ç”¨ Accelerate è·å¾—æœ€ä½³æ€§èƒ½ï¼Œåº”åœ¨æ¨¡å‹å†…éƒ¨è®¡ç®—æŸå¤±ï¼ˆå¦‚åœ¨ Transformers æ¨¡å‹ä¸­ï¼‰ï¼Œå› ä¸ºæ¨¡å‹å¤–éƒ¨çš„è®¡ç®—æ˜¯ä»¥å…¨ç²¾åº¦è¿›è¡Œçš„ã€‚\nè®¾ç½®è¦åœ¨ accelerater ä¸­ä½¿ç”¨çš„æ··åˆç²¾åº¦ç±»å‹ï¼Œç„¶åä½¿ç”¨ autocast() ä¸Šä¸‹æ–‡ç®¡ç†å™¨å°†å€¼è‡ªåŠ¨è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹ã€‚\nfrom accelerate import Accelerator+ accelerator = Accelerator(mixed_precision=\"fp16\")+ with accelerator.autocast():      loss = complex_loss_function(outputs, target):\n\n\n\nä¿å­˜å’ŒåŠ è½½è®­ç»ƒå®Œæˆåï¼ŒåŠ é€Ÿè¿˜å¯ä»¥ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Œæˆ–è€…æ‚¨è¿˜å¯ä»¥ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆoptimizer stateï¼‰ï¼Œè¿™å¯¹äºæ¢å¤è®­ç»ƒå¾ˆæœ‰ç”¨ã€‚\næ¨¡å‹æ‰€æœ‰è¿‡ç¨‹å®Œæˆåï¼Œåœ¨ä¿å­˜æ¨¡å‹å‰ä½¿ç”¨ unwrap_model() æ–¹æ³•è§£é™¤æ¨¡å‹çš„å°è£…ï¼Œå› ä¸ºè®­ç»ƒå¼€å§‹å‰æ‰§è¡Œçš„ prepare() æ–¹æ³•å°†æ¨¡å‹å°è£…åˆ°äº†é€‚åˆçš„åˆ†å¸ƒå¼è®­ç»ƒæ¥å£ä¸­ã€‚å¦‚æœä¸è§£é™¤å¯¹æ¨¡å‹çš„å°è£…ï¼Œä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸çš„åŒæ—¶ä¹Ÿä¼šä¿å­˜å¤§æ¨¡å‹ä¸­ä»»ä½•æ½œåœ¨çš„é¢å¤–å±‚ï¼Œè¿™æ ·å°±æ— æ³•å°†æƒé‡åŠ è½½å›åŸºç¡€æ¨¡å‹ä¸­ã€‚\nä½¿ç”¨ save_model() æ–¹æ³•æ¥è§£åŒ…å¹¶ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚æ­¤æ–¹æ³•è¿˜å¯ä»¥å°†æ¨¡å‹ä¿å­˜åˆ°åˆ‡ç‰‡æ£€æŸ¥ç‚¹ sharded checkpoints æˆ–safetensorsæ ¼å¼ä¸­ã€‚\naccelerator.wait_for_everyone()accelerator.save_model(model, save_directory)\n\n\nå¯¹äº Transformers åº“ä¸­çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ save_pretrained æ–¹æ³•ä¿å­˜æ¨¡å‹ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ from_pretrainedæ–¹æ³•é‡æ–°åŠ è½½ã€‚\nfrom transformers import AutoModelunwrapped_model = accelerator.unwrap_model(model)unwrapped_model.save_pretrained(    \"path/to/my_model_directory\",    is_main_process=accelerator.is_main_process,    save_function=accelerator.save,)model = AutoModel.from_pretrained(\"path/to/my_model_directory\")\n\nè¦åŠ è½½æƒé‡ï¼Œè¯·åœ¨åŠ è½½æƒé‡ä¹‹å‰å…ˆä½¿ç”¨ unwrap_model() æ–¹æ³•è§£åŒ…æ¨¡å‹ã€‚æ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½æ˜¯å¯¹å¼ é‡çš„å¼•ç”¨ï¼Œå› æ­¤è¿™ä¼šå°†æ‚¨çš„æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚\nunwrapped_model = accelerator.unwrap_model(model)path_to_checkpoint = os.path.join(save_directory,\"pytorch_model.bin\")unwrapped_model.load_state_dict(torch.load(path_to_checkpoint))\n\nåˆ‡ç‰‡æ£€æŸ¥ç‚¹è®¾ç½® safe_serialization=True å°†æ¨¡å‹ä¿å­˜ä¸º safetensor æ ¼å¼ã€‚\naccelerator.wait_for_everyone()accelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)\n\nè¦åŠ è½½åˆ†ç‰‡æ£€æŸ¥ç‚¹æˆ– safetensor æ ¼å¼çš„æ£€æŸ¥ç‚¹ï¼Œè¯·ä½¿ç”¨ load_checkpoint_in_model() æ–¹æ³•ã€‚æ­¤æ–¹æ³•å…è®¸æ‚¨å°†æ£€æŸ¥ç‚¹åŠ è½½åˆ°ç‰¹å®šè®¾å¤‡ä¸Šã€‚\nload_checkpoint_in_model(unwrapped_model, save_directory, device_map={\"\":device})\n\n\n\nçŠ¶æ€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›ä¿å­˜æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€éšæœºç”Ÿæˆå™¨ä»¥åŠå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å½“å‰çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨åŒä¸€ä¸ªè„šæœ¬ä¸­æ¢å¤å®ƒä»¬ã€‚ä½ åº”è¯¥åœ¨è„šæœ¬ä¸­æ·»åŠ  save_state() å’Œ load_state() æ–¹æ³•æ¥ä¿å­˜å’ŒåŠ è½½çŠ¶æ€ã€‚\nä»»ä½•å…¶ä»–éœ€è¦å­˜å‚¨çš„æœ‰çŠ¶æ€é¡¹ç›®éƒ½åº”ä½¿ç”¨ register_for_checkpointing() æ–¹æ³•æ³¨å†Œï¼Œä»¥ä¾¿ä¿å­˜å’ŒåŠ è½½ã€‚ä¼ é€’ç»™æ­¤æ–¹æ³•çš„æ¯ä¸ªè¦å­˜å‚¨çš„å¯¹è±¡éƒ½å¿…é¡»å…·æœ‰ load_state_dict å’Œ state_dict å‡½æ•°ã€‚\næ‰§è¡Œè¿›ç¨‹åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ—¶ï¼Œç®¡ç†è·¨ GPU æ‰§è¡Œæµç¨‹çš„æ–¹å¼å’Œæ—¶é—´éå¸¸é‡è¦ã€‚æœ‰äº›è¿›ç¨‹æ¯”å…¶ä»–è¿›ç¨‹å®Œæˆå¾—æ›´å¿«ï¼Œæœ‰äº›è¿›ç¨‹åœ¨å…¶ä»–è¿›ç¨‹å°šæœªå®Œæˆæ—¶å°±ä¸åº”å¼€å§‹ã€‚Accelerate æä¾›äº†ç”¨äºåè°ƒè¿›ç¨‹æ‰§è¡Œæ—¶é—´çš„å·¥å…·ï¼Œä»¥ç¡®ä¿æ‰€æœ‰è®¾å¤‡ä¸Šçš„ä¸€åˆ‡ä¿æŒåŒæ­¥ã€‚\nåœ¨ä¸€ä¸ªè¿›ç¨‹ä¸Šæ‰§è¡ŒæŸäº›ä»£ç åªéœ€åœ¨ç‰¹å®šæœºå™¨ä¸Šè¿è¡Œä¸€æ¬¡ï¼Œå¦‚æ‰“å°æ—¥å¿—è¯­å¥æˆ–åªåœ¨æœ¬åœ°ä¸»è¿›ç¨‹ä¸Šæ˜¾ç¤ºä¸€ä¸ªè¿›åº¦æ¡ã€‚\nstatementåº”ä½¿ç”¨ accelerator.is_local_main_process æ¥æŒ‡ç¤ºåªåº”æ‰§è¡Œä¸€æ¬¡çš„ä»£ç ã€‚\n\naccelerator.is_local_main_process ï¼š\n\nç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æœ¬åœ°èŠ‚ç‚¹ï¼ˆæœåŠ¡å™¨ï¼‰ä¸Šçš„ä¸»è¿›ç¨‹ï¼Œ\nå¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡åœ¨å¤šå°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œæ¯å°æœåŠ¡å™¨éƒ½æœ‰ä¸€ä¸ªä¸»è¿›ç¨‹ã€‚is_local_main_process() å¦‚æœè¿”å› Trueï¼Œè¡¨ç¤ºå½“å‰è¿›ç¨‹æ˜¯æœ¬åœ°èŠ‚ç‚¹ä¸Šçš„ä¸»è¿›ç¨‹ã€‚\né€šå¸¸ï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°èŠ‚ç‚¹çš„ä¸»è¿›ç¨‹ä¸Šæ‰§è¡Œä¸€äº›åªéœ€æ‰§è¡Œä¸€æ¬¡çš„æ“ä½œï¼Œä¾‹å¦‚åˆå§‹åŒ–æ•°æ®ã€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç­‰ã€‚\n\n\nfrom tqdm.auto import tqdmprogress_bar = tqdm(    range(args.max_train_steps),     disable=not accelerator.is_local_main_process)\n\nè¿˜å¯ä»¥ä½¿ç”¨ accelerator.is_local_main_process åŒ…è£…è¯­å¥ã€‚\nif accelerator.is_local_main_process:    print(\"Accelerate is the best\")\n\n\n\nè¿˜å¯ä»¥æŒ‡ç¤º Accelerate åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­éƒ½è¦æ‰§è¡Œä¸€æ¬¡çš„ä»£ç ï¼Œè€Œä¸ç®¡æœ‰å¤šå°‘å°æœºå™¨ã€‚å¦‚æœæ‚¨è¦å°†æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚\n\naccelerator.is_main_processï¼š\n\nè¿™ä¸ªå‡½æ•°ç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æ•´ä¸ªè®­ç»ƒä»»åŠ¡ä¸­çš„ä¸»è¿›ç¨‹ã€‚\nä¸»è¿›ç¨‹é€šå¸¸è´Ÿè´£ä¸€äº›å…¨å±€æ“ä½œï¼Œä¾‹å¦‚æ¨¡å‹ä¿å­˜ã€æ—¥å¿—è®°å½•ç­‰ã€‚å› æ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨ is_main_process() æ¥ç¡®ä¿è¿™äº›æ“ä½œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡ã€‚\nå¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡åœ¨å¤šå°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œis_main_process() å°†è¿”å› Trueï¼Œåªæœ‰ä¸€ä¸ªæœåŠ¡å™¨ä¸Šçš„ä¸»è¿›ç¨‹ä¼šæ»¡è¶³è¿™ä¸ªæ¡ä»¶ã€‚\n\n\nif accelerator.is_main_process:    repo.push_to_hub()\n\n\n\nfunctionå¯¹äºåªåº”æ‰§è¡Œä¸€æ¬¡çš„å‡½æ•°ï¼Œè¯·ä½¿ç”¨ on_local_main_process è£…é¥°å™¨ã€‚\n@accelerator.on_local_main_processdef do_my_thing():    \"Something done once per server\"    do_thing_once_per_server()\n\nå¯¹äºåªåº”åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡çš„å‡½æ•°ï¼Œè¯·ä½¿ç”¨ on_main_process è£…é¥°å™¨ã€‚\n@accelerator.on_main_processdef do_my_thing():    \"Something done once per server\"    do_thing_once()\n\n\n\n\n\nåœ¨ç‰¹å®šè¿›ç¨‹ä¸Šæ‰§è¡ŒAccelerate è¿˜å¯ä»¥æ‰§è¡Œåªåº”åœ¨ç‰¹å®šè¿›ç¨‹æˆ–æœ¬åœ°è¿›ç¨‹ç´¢å¼•ä¸Šæ‰§è¡Œçš„å‡½æ•°ã€‚\nä½¿ç”¨ on_process() è£…é¥°å™¨æŒ‡å®šè¦æ‰§è¡Œå‡½æ•°çš„è¿›ç¨‹ç´¢å¼•ã€‚\n@accelerator.on_process(process_index=0)def do_my_thing():    \"Something done on process index 0\"    do_thing_on_index_zero()\n\nä½¿ç”¨ on_local_process() è£…é¥°å™¨æŒ‡å®šè¦æ‰§è¡Œå‡½æ•°çš„æœ¬åœ°è¿›ç¨‹ç´¢å¼•ã€‚\n@accelerator.on_local_process(local_process_idx=0)def do_my_thing():    \"Something done on process index 0 on each server\"    do_thing_on_index_zero_on_each_server()\n\n\n\næ¨è¿Ÿæ‰§è¡Œå½“åŒæ—¶åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œè„šæœ¬æ—¶ï¼ŒæŸäº›ä»£ç çš„æ‰§è¡Œé€Ÿåº¦å¯èƒ½ä¼šæ¯”å…¶ä»–ä»£ç å¿«ã€‚åœ¨æ‰§è¡Œä¸‹ä¸€ç»„æŒ‡ä»¤ä¹‹å‰ï¼Œæ‚¨å¯èƒ½éœ€è¦ç­‰å¾…æ‰€æœ‰è¿›ç¨‹éƒ½è¾¾åˆ°ä¸€å®šç¨‹åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¡®ä¿æ¯ä¸ªè¿›ç¨‹éƒ½å®Œæˆè®­ç»ƒä¹‹å‰ï¼Œæ‚¨ä¸åº”è¯¥ä¿å­˜æ¨¡å‹ã€‚\nä¸ºæ­¤ï¼Œè¯·åœ¨ä»£ç ä¸­æ·»åŠ  wait_for_everyone()ã€‚è¿™ä¼šé˜»æ­¢æ‰€æœ‰å…ˆå®Œæˆè®­ç»ƒçš„è¿›ç¨‹ç»§ç»­è®­ç»ƒï¼Œç›´åˆ°æ‰€æœ‰å‰©ä½™è¿›ç¨‹éƒ½è¾¾åˆ°ç›¸åŒç‚¹ï¼ˆå¦‚æœåœ¨å•ä¸ª GPU æˆ– CPU ä¸Šè¿è¡Œï¼Œåˆ™æ²¡æœ‰å½±å“ï¼‰ã€‚\naccelerator.wait_for_everyone()\n\n\n\nå¯åŠ¨Accelerateè„šæœ¬é¦–å…ˆï¼Œå°†è®­ç»ƒä»£ç é‡å†™ä¸ºå‡½æ•°ï¼Œå¹¶ä½¿å…¶å¯ä½œä¸ºè„šæœ¬è°ƒç”¨ã€‚ä¾‹å¦‚ï¼š\n  from accelerate import Accelerator  + def main():      accelerator = Accelerator()      model, optimizer, training_dataloader, scheduler = accelerator.prepare(          model, optimizer, training_dataloader, scheduler      )      for batch in training_dataloader:          optimizer.zero_grad()          inputs, targets = batch          outputs = model(inputs)          loss = loss_function(outputs, targets)          accelerator.backward(loss)          optimizer.step()          scheduler.step()+ if __name__ == \"__main__\":+     main()\n\n\n\nç„¶åï¼Œåœ¨å‘½ä»¤è¡Œä½¿ç”¨ accelerate config æ¥é…ç½®accelerateçš„è¿è¡Œç¯å¢ƒ\n\nThe Command Line (huggingface.co)\n\nä½¿ç”¨ accelerate launchAccelerate æœ‰ä¸€ä¸ªç‰¹æ®Šçš„ CLI å‘½ä»¤ï¼Œå¯å¸®åŠ©æ‚¨é€šè¿‡åŠ é€Ÿå¯åŠ¨åœ¨ç³»ç»Ÿä¸­å¯åŠ¨ä»£ç ã€‚è¯¥å‘½ä»¤åŒ…å«åœ¨å„ç§å¹³å°ä¸Šå¯åŠ¨è„šæœ¬æ‰€éœ€çš„æ‰€æœ‰ä¸åŒå‘½ä»¤\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼š\naccelerate launch --accelerate-arg {script_name.py} --script-arg1 --script-arg2 ...\n\nç”±äºè¿™ä¼šè¿è¡Œå„ç§ torch ç”Ÿæˆæ–¹æ³•ï¼Œå› æ­¤ä¹Ÿå¯ä»¥åœ¨æ­¤å¤„ä¿®æ”¹æ‰€æœ‰ç¯å¢ƒå˜é‡ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨å•ä¸ª GPU ï¼š\nCUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...\n\næ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ accelerate launchï¼Œè€Œæ— éœ€å…ˆæ‰§è¡Œ accelerate configï¼Œä½†å¯èƒ½éœ€è¦æ‰‹åŠ¨è¾“å…¥æ­£ç¡®çš„é…ç½®å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒAccelerate ä¼šä¸ºä½ åšå‡ºä¸€äº›è¶…å‚æ•°å†³å®šï¼Œä¾‹å¦‚ï¼Œå¦‚æœ GPU å¯ç”¨ï¼Œå®ƒä¼šé»˜è®¤ä½¿ç”¨æ‰€æœ‰ GPUï¼Œä¸ä½¿ç”¨æ··åˆç²¾åº¦ã€‚\næŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼š\naccelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\nä½¿ç”¨æ··åˆç²¾åº¦åœ¨ä¸¤ä¸ª GPU ä¸Šå¯åŠ¨ç›¸åŒçš„è„šæœ¬\naccelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\nè¦è·å–å¯ä»¥ä¼ å…¥çš„å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·è¿è¡Œï¼š\naccelerate launch -h\n\nä»è¯¥è‡ªå®šä¹‰ yaml æ–‡ä»¶å¯åŠ¨è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š\naccelerate launch --config_file {path/to/config/my_config_file.yaml} {script_name.py} {--arg1} {--arg2} ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["Tech"],"tags":["python","Training-Trick"]},{"title":"NeuSç¬”è®°","url":"/2025/05/06/NeuS%E7%AC%94%E8%AE%B0/","content":"NeuS Noteæ¸²æŸ“è¿‡ç¨‹åœºæ™¯è¡¨ç¤ºè¦é‡å»ºçš„ç‰©ä½“åœºæ™¯ç”±ä¸¤ä¸ªå‡½æ•°è¡¨ç¤ºï¼š\n\nï¼šå°†ç©ºé—´ä½ç½®  æ˜ å°„åˆ°ç‰©ä½“çš„ç¬¦å·è·ç¦»\nï¼šå°†ç©ºé—´ä½ç½®  å’Œè§†å›¾æ–¹å‘  æ˜ å°„ä¸ºé¢œè‰²\n\nè¿™ä¸¤ä¸ªå‡½æ•°éƒ½é€šè¿‡ MLP å®ç°\nç‰©ä½“çš„è¡¨é¢  ç”±å…¶ SDFï¼ˆsigned distance functionï¼‰ çš„é›¶ç­‰å€¼é¢è¡¨ç¤ºï¼Œå³Missing or unrecognized delimiter for \\left \\mathcal{S} = \\left{ \\mathbf{x} \\in \\mathbb{R}^3 \\mid f(\\mathbf{x}) = 0 \\right}. \\tag{1} \n\nç¬¦å·è·ç¦»å‡½æ•° SDF\nå…·ä½“æ¥è¯´ï¼ŒSDFæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸º ï¼Œå…¶ä¸­  æ˜¯ç©ºé—´çš„ç»´åº¦ï¼ˆä¾‹å¦‚äºŒç»´æˆ–ä¸‰ç»´ï¼‰ã€‚å¯¹äºç©ºé—´ä¸­çš„ä»»æ„ä¸€ç‚¹ ï¼ŒSDF  è¡¨ç¤ºä»ç‚¹  åˆ°æœ€è¿‘è¡¨é¢çš„è·ç¦»ï¼Œå¹¶ä¸”ï¼š\n\nå¦‚æœ ï¼Œåˆ™ç‚¹  åœ¨ç‰©ä½“å†…éƒ¨ï¼›\nå¦‚æœ ï¼Œåˆ™ç‚¹  åœ¨ç‰©ä½“è¡¨é¢ä¸Šï¼›\nå¦‚æœ ï¼Œåˆ™ç‚¹  åœ¨ç‰©ä½“å¤–éƒ¨ã€‚\n\n\nS-å¯†åº¦ä¸ºäº†å°†ä½“æ¸²æŸ“çš„æ–¹æ³•ç”¨äºè®­ç»ƒ SDF ç¥ç»ç½‘ç»œï¼Œå¼•å…¥æ¦‚ç‡å¯†åº¦å‡½æ•°  ï¼Œç§°ä¸º S-å¯†åº¦åœºï¼šS-å¯†åº¦åœºæ˜¯ Sigmoid å‡½æ•°  çš„å¯¼æ•°\nå³ ã€‚\n\nåŸåˆ™ä¸Šï¼Œ å¯ä»¥æ˜¯ä»»ä½•ä»¥ 0 ä¸ºä¸­å¿ƒçš„å•å³°ï¼ˆå³é’Ÿå½¢ï¼‰å¯†åº¦åˆ†å¸ƒï¼›è¿™é‡Œæˆ‘ä»¬é€‰æ‹©é€»è¾‘å¯†åº¦åˆ†å¸ƒæ˜¯å› ä¸ºå®ƒçš„è®¡ç®—æ–¹ä¾¿ã€‚\næ³¨æ„ï¼Œ çš„æ ‡å‡†å·®ç”±  ç»™å‡ºï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå³éšç€ç½‘ç»œè®­ç»ƒçš„æ”¶æ•›ï¼Œ æ¥è¿‘äºé›¶ã€‚\néšç€  çš„å¢å¤§ï¼Œ çš„çš„å³°å€¼é«˜åº¦ä¼šå¢åŠ ï¼Œå®½åº¦ä¼šå˜çª„ï¼Œä¹Ÿå°±æ˜¯è¯´éšç€ s çš„å¢å¤§è€Œå˜å¾—æ›´åŠ å°–é”å’Œé›†ä¸­\n\n\nNeuS çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œåœ¨ S-å¯†åº¦åœº  çš„å¸®åŠ©ä¸‹ï¼Œä½¿ç”¨ä½“æ¸²æŸ“é€šè¿‡ä»…ä»¥ 2D è¾“å…¥å›¾åƒä½œä¸ºç›‘ç£æ¥è®­ç»ƒ SDF ç½‘ç»œã€‚åœ¨æˆåŠŸæœ€å°åŒ–åŸºäºè¿™ç§ç›‘ç£çš„æŸå¤±å‡½æ•°ä¹‹åï¼ŒæœŸæœ›ç½‘ç»œç¼–ç çš„ SDF çš„é›¶ç­‰å€¼é¢èƒ½å¤Ÿå‡†ç¡®åœ°è¡¨ç¤ºé‡å»ºçš„è¡¨é¢ ï¼Œå…¶è¯±å¯¼çš„ S-å¯†åº¦  åœ¨è¡¨é¢é™„è¿‘å‘ˆç°å‡ºæ˜¾è‘—çš„é«˜å€¼ã€‚\n\n  è¶Šå¤§ -&gt; è¶Šè¶‹è¿‘äº0 -&gt;  è¶Šè¶‹è¿‘äºç‰©ä½“è¡¨é¢\n\næ¸²æŸ“ç»™å®šä¸€ä¸ªåƒç´ ï¼Œæˆ‘ä»¬å°†ä»è¯¥åƒç´ å‘å‡ºçš„å°„çº¿è¡¨ç¤ºä¸º ï¼Œå…¶ä¸­  æ˜¯ç›¸æœºçš„ä¸­å¿ƒï¼Œ æ˜¯å°„çº¿çš„å•ä½æ–¹å‘å‘é‡ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼æ²¿å°„çº¿ç´¯ç§¯é¢œè‰²ï¼š\n\n\n æ˜¯è¯¥åƒç´ çš„è¾“å‡ºé¢œè‰²\n æ˜¯ç‚¹  çš„æƒé‡å‡½æ•°\n æ˜¯æ²¿è§†å›¾æ–¹å‘  çš„ç‚¹  å¤„çš„é¢œè‰²ã€‚\n\nå¯¹æƒé‡å‡½æ•°  çš„è¦æ±‚ä» 2D å›¾åƒä¸­å­¦ä¹ ç²¾ç¡®çš„ SDF è¡¨ç¤ºçš„å…³é”®æ˜¯å»ºç«‹è¾“å‡ºé¢œè‰²  å’Œ SDF  ä¹‹é—´çš„é€‚å½“è”ç³»ï¼Œå³åŸºäºåœºæ™¯çš„ SDF  å¯¼å‡ºæ²¿å°„çº¿çš„é€‚å½“æƒé‡å‡½æ•° ã€‚\næƒé‡å‡½æ•°  è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š\n\næ— åæ€§ã€‚ç»™å®šä¸€æ¡ç›¸æœºå°„çº¿ ï¼Œ åœ¨è¡¨é¢äº¤ç‚¹  å¤„å–å¾—å±€éƒ¨æœ€å¤§å€¼ï¼Œå³ ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç‚¹  ä½äº SDF çš„é›¶ç­‰å€¼é¢ä¸Šï¼ˆï¼‰ã€‚\n\né®æŒ¡æ„ŸçŸ¥ã€‚ç»™å®šä»»æ„ä¸¤ä¸ªæ·±åº¦å€¼  å’Œ ï¼Œæ»¡è¶³ ï¼Œï¼Œï¼Œä¸” ï¼Œæœ‰ ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ä¸¤ä¸ªç‚¹å…·æœ‰ç›¸åŒçš„ SDF å€¼ï¼ˆå› æ­¤å…·æœ‰ç›¸åŒçš„ SDF è¯±å¯¼çš„ S-å¯†åº¦å€¼ï¼‰æ—¶ï¼Œæ›´é è¿‘è§†ç‚¹çš„ç‚¹åº”è¯¥å¯¹æœ€ç»ˆè¾“å‡ºé¢œè‰²æœ‰æ›´å¤§çš„è´¡çŒ®ã€‚\n\n\nä¸€ä¸ªæ— åçš„æƒé‡å‡½æ•°  ä¿è¯äº†ç›¸æœºå°„çº¿ä¸ SDF çš„é›¶ç­‰å€¼é¢çš„äº¤ç‚¹å¯¹åƒç´ é¢œè‰²çš„è´¡çŒ®æœ€å¤§ã€‚\né®æŒ¡æ„ŸçŸ¥å±æ€§ç¡®ä¿äº†å½“ä¸€æ¡å°„çº¿ä¾æ¬¡ç©¿è¿‡å¤šä¸ªè¡¨é¢æ—¶ï¼Œæ¸²æŸ“è¿‡ç¨‹å°†æ­£ç¡®åœ°ä½¿ç”¨ç¦»ç›¸æœºæœ€è¿‘çš„è¡¨é¢çš„é¢œè‰²æ¥è®¡ç®—è¾“å‡ºé¢œè‰²ã€‚\nNeRFçš„æœ´ç´ è§£NeRFä¸­çš„æƒé‡å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š\n\n æ˜¯NeRFä¸­æ‰€è°“çš„ä½“å¯†åº¦ï¼Œè¿™é‡Œå°†  è®¾ç½®ä¸ºç­‰äº S-å¯†åº¦å€¼ï¼Œå³  \n è¡¨ç¤ºæ²¿å°„çº¿çš„ç´¯ç§¯é€å°„ç‡ã€‚\n\nå°½ç®¡ç”±æ­¤äº§ç”Ÿçš„æƒé‡å‡½æ•°å…·æœ‰é®æŒ¡æ„ŸçŸ¥æ€§ï¼Œä½†å®ƒæ˜¯æœ‰åçš„ï¼Œå› ä¸ºå®ƒåœ¨é‡å»ºçš„è¡¨é¢ä¸­å¼•å…¥äº†å›ºæœ‰çš„è¯¯å·®ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæƒé‡å‡½æ•°  åœ¨å°„çº¿è¾¾åˆ°æ»¡è¶³  çš„è¡¨é¢ç‚¹  ä¹‹å‰çš„æŸç‚¹è¾¾åˆ°å±€éƒ¨æœ€å¤§å€¼\n\nNeuSå¯¹æƒé‡å‡½æ•°  çš„è§£ä¸ºäº†ä»‹ç» NeuS çš„è§£å†³æ–¹æ¡ˆï¼Œé¦–å…ˆä»‹ç»ä¸€ç§ç›´æ¥ä½¿ç”¨å½’ä¸€åŒ– S-å¯†åº¦ä½œä¸ºæƒé‡æ¥æ„å»ºæ— åæƒé‡å‡½æ•°çš„ç®€å•æ–¹æ³•ï¼šè¿™ç§æƒé‡å‡½æ•°çš„æ„é€ æ˜¯æ— åçš„ï¼Œä½†ä¸å…·å¤‡é®æŒ¡æ„ŸçŸ¥æ€§ã€‚\n\nä¾‹å¦‚ï¼Œå¦‚æœå°„çº¿ç©¿é€ä¸¤ä¸ªè¡¨é¢ï¼ŒSDF å‡½æ•°  å°†åœ¨å°„çº¿ä¸Šæœ‰ä¸¤ä¸ªé›¶ç‚¹ï¼Œè¿™å¯¼è‡´æƒé‡å‡½æ•°  ä¸Šæœ‰ä¸¤ä¸ªå³°å€¼ï¼Œå¹¶ä¸”ç”±æ­¤äº§ç”Ÿçš„æƒé‡å‡½æ•°å°†å¹³å‡æ··åˆè¿™ä¸¤ä¸ªè¡¨é¢çš„é¢œè‰²ï¼Œè€Œä¸è€ƒè™‘é®æŒ¡ã€‚\n\nä¸ºäº†ç¡®ä¿æƒé‡å‡½æ•°  çš„é®æŒ¡æ„ŸçŸ¥å±æ€§ï¼Œæˆ‘ä»¬ä»å°†éµå¾ªNeRF çš„åŸºæœ¬æ¡†æ¶ï¼Œä»¥ä¸€ç§æ–°çš„æ–¹å¼ä» S-å¯†åº¦å®šä¹‰æˆ‘ä»¬çš„æƒé‡å‡½æ•° ï¼š\n\nï¼šä¸é€æ˜å¯†åº¦å‡½æ•°ï¼Œæ˜¯æ ‡å‡†ä½“ç§¯æ¸²æŸ“ä¸­ä½“ç§¯å¯†åº¦  çš„å¯¹åº”\nï¼šç´¯ç§¯é€å°„ç‡ï¼Œ åŒºé—´çš„ä¸é€æ˜åº¦è¶Šå¤§ -&gt;  çš„ç§¯åˆ†è¶Šå¤§ -&gt;  è¶Šå°ï¼Œç´¯ç§¯é€æ˜åº¦è¶Šå°\n\nä¸é€æ˜å¯†åº¦å‡½æ•°  çš„æ¨å¯¼é¦–å…ˆè€ƒè™‘ä¸€ä¸ªç®€å•çš„ç†æƒ³æƒ…å†µï¼šè¡¨é¢æ˜¯ä¸€ä¸ªè¿œç¦»ç›¸æœºçš„å¹³é¢ï¼Œä¸”åªæœ‰ä¸€ä¸ªäº¤ç‚¹\næ­¤æ—¶çš„ç¬¦å·è·ç¦»å‡½æ•° SDF å¾ˆæ˜æ˜¾æ˜¯ï¼š\n\nç„¦ç‚¹ä½ç½®ï¼š\n æ˜¯è§†å›¾æ–¹å‘  å’Œå‘å¤–è¡¨é¢æ³•çº¿å‘é‡  ä¹‹é—´çš„è§’åº¦\n\nåœ¨è¿™ç§å‡è®¾ä¸‹å…¬å¼ 5 ç¡®å®æ»¡è¶³è¦æ±‚ï¼Œç”±äº æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¯ä»¥å¾—å‡ºï¼š\nåœ¨ä½“ç§¯æ¸²æŸ“æ¡†æ¶å†…ï¼Œæƒé‡å‡½æ•°ç”±  ç»™å‡ºï¼Œä¸ºäº†æ¨å¯¼ ï¼Œæœ‰ï¼š\n\nç”±äº ï¼Œå¾ˆå®¹æ˜“éªŒè¯ ã€‚\næ ¹æ®  å‡½æ•°çš„è®¾å®šï¼šï¼Œå¯çŸ¥  \nç”±æ­¤å¯å¾—ï¼š\n\næœ´ç´ è§£ä¸­çš„åå·®æƒé‡å‡½æ•°å®šä¹‰ä¸º ï¼Œå…¶ä¸­ä¸é€æ˜åº¦ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰å¾…å®š\n","categories":["Note"],"tags":["ä¸‰ç»´é‡å»º"]},{"url":"/2025/05/07/hexo-blog/Welcome/","content":"This is your new vault.\nMake a note of something, [[create a link]], or try the Importer!\nWhen youâ€™re ready, delete this note and make the vault your own.\n"}]