[{"title":"0.è§£æ„åŸºæœ¬pipeline","url":"/2024/04/06/0.%20%E8%A7%A3%E6%9E%84%E5%9F%BA%E6%9C%ACpipeline/","content":"è§£æ„åŸºæœ¬pipelineå€ŸåŠ©pipelineï¼Œä»…éœ€å››è¡Œä»£ç å³å¯ç”Ÿæˆå›¾åƒï¼š\nfrom diffusers import DDPMPipelineddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")image = ddpm(num_inference_steps=25).images[0]\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼ŒpipelineåŒ…å«ï¼š\n\nå»å™ªæ¨¡å‹ï¼šUNet2DModel\né‡‡æ ·å™¨ï¼šDDPMScheduler\n\npipelineé€šè¿‡è·å–æ‰€éœ€è¾“å‡ºå¤§å°çš„éšæœºå™ªå£°å¹¶å°†å…¶å¤šæ¬¡é€šè¿‡å»å™ªæ¨¡å‹æ¥å¯¹å›¾åƒè¿›è¡Œå»å™ªã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œå»å™ªæ¨¡å‹é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œè°ƒé‡‡æ ·å™¨ä½¿ç”¨å®ƒæ¥é¢„æµ‹å™ªå£°è¾ƒå°çš„å›¾åƒã€‚ç®¡é“é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°åˆ°è¾¾æŒ‡å®šæ•°é‡çš„æ¨ç†æ­¥éª¤çš„å›¾åƒã€‚\nè¦åˆ†åˆ«ä½¿ç”¨å»å™ªæ¨¡å‹å’Œé‡‡æ ·å™¨é‡æ–°åˆ›å»ºpipelineï¼Œè®©æˆ‘ä»¬ç¼–å†™è‡ªå·±çš„å»å™ªè¿‡ç¨‹ï¼š\n\nè½½å…¥å»å™ªæ¨¡å‹å’Œé‡‡æ ·å™¨ï¼š\nfrom diffusers import DDPMScheduler, UNet2DModelscheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n\né€šè¿‡é‡‡æ ·å™¨è®¾ç½®å»å™ªè¿‡ç¨‹çš„æ—¶é—´æ­¥æ•°ï¼š\nscheduler.set_timesteps(50)\n\nè®¾ç½®é‡‡æ ·å™¨çš„æ—¶é—´æ­¥æ•°ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«å‡åŒ€é—´éš”å…ƒç´ çš„å¼ é‡ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­ä¸º 50ã€‚æ¯ä¸ªå…ƒç´ å¯¹åº”äºæ¨¡å‹å¯¹å›¾åƒè¿›è¡Œå»å™ªçš„æ—¶é—´æ­¥ã€‚åœ¨è¿›è¡Œå»å™ªå¾ªç¯æ—¶ï¼Œå°†è¿­ä»£è¯¥å¼ é‡ä»¥å¯¹å›¾åƒè¿›è¡Œå»å™ªï¼š\nscheduler.timestepstensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,    \t700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,    \t420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,    \t140, 120, 100,  80,  60,  40,  20,   0])\n\nåˆ›å»ºä¸æ‰€éœ€è¾“å‡ºå›¾åƒå½¢çŠ¶ç›¸åŒçš„éšæœºå™ªå£°ï¼š\nimport torchsample_size = model.config.sample_sizenoise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n\nç°åœ¨ç¼–å†™ä¸€ä¸ªå¾ªç¯æ¥è¿­ä»£ã€‚\n\nåœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œéƒ½ä¼šæ‰§è¡Œ UNet2DModel.forward() ï¼Œæ ¹æ®è¾“å…¥çš„å™ªå£°å›¾åƒ(input)å’Œæ—¶é—´æ­¥(t)è¿”å›å™ªå£°æ®‹å·®(noisy_residual)ã€‚\né‡‡æ ·å™¨çš„ step() æ–¹æ³•åˆ©ç”¨å™ªå£°æ®‹å·®(noisy_residual)ã€æ—¶é—´æ­¥(t))å’Œå™ªå£°å›¾åƒ(input)é¢„æµ‹å‰ä¸€ä¸ªæ—¶é—´æ­¥é•¿çš„å›¾åƒã€‚è¯¥è¾“å‡ºæˆä¸ºå»å™ªå¾ªç¯çš„ä¸‹ä¸€ä¸ªè¾“å…¥ã€‚\n\ninput = noisefor t in scheduler.timesteps:    with torch.no_grad():        noisy_residual = model(input, t).sample    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample    input = previous_noisy_sample\n\næœ€åä¸€æ­¥æ˜¯å°†å»å™ªè¾“å‡ºè½¬æ¢ä¸ºå›¾åƒï¼š\nfrom PIL import Imageimport numpy as np# inputçš„æ•°æ®èŒƒå›´ä¸º[-1, 1], è½¬ä¸º[0, 1]# shape: [1, 3, sample_size, sample_size] -&gt; [3, sample_size, sample_size]image = (input / 2 + 0.5).clamp(0, 1).squeeze()# shape: [3, sample_size, sample_size] -&gt; [sample_size, sample_size, 3]# data range: (float32)[0, 1] -&gt; (uint8)[0, 255]# device: cuda -&gt; cpu# type: torch.tensor -&gt; numpy.arrayimage = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()# è¾“å‡ºä¸ºå›¾åƒimage = Image.fromarray(image)image\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"1.è§£æ„Stable Diffusion","url":"/2024/04/07/1.%20%E8%A7%A3%E6%9E%84%20Stable%20Diffusion/","content":"è§£æ„Stable DiffusionStable Diffusionæ˜¯ä¸€ç§æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚å®ƒä½¿ç”¨å›¾åƒçš„ä½ç»´è¡¨ç¤ºè€Œä¸æ˜¯å®é™…çš„åƒç´ ç©ºé—´ï¼Œè¿™ä½¿å¾—å®ƒçš„å†…å­˜æ•ˆç‡æ›´é«˜ã€‚\n\nVAEç¼–ç å™¨å°†å›¾åƒå‹ç¼©ä¸ºä½ç»´è¡¨ç¤ºï¼ŒVAEè§£ç å™¨å°†å‹ç¼©çš„ä½ç»´è¡¨ç¤ºè½¬æ¢å›å›¾åƒã€‚\n\nå¯¹äºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œéœ€è¦ä¸€ä¸ªåˆ†è¯å™¨ï¼ˆtokenizerï¼‰å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆtext encoderï¼‰æ¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆtext embeddingsï¼‰ã€‚\n\n\nå¦‚ä¸Šæ‰€è¿°ï¼ŒSD pipelineæ¯”ä»…åŒ…å« UNet æ¨¡å‹çš„ DDPM pipelineæ›´å¤æ‚ã€‚Stable Diffusionå…·æœ‰ä¸‰ä¸ªç‹¬ç«‹çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚\n\nè¿ä½œæœºåˆ¶ï¼šStable Diffusion with ğŸ§¨ Diffusers (huggingface.co)\n\n\nThe autoencoder (VAE): AutoencoderKL\nThe U-Net: UNet2DConditionModel\nThe Text-encoder: CLIPTextModel\n\nSD pipeline è¿ä½œæœºåˆ¶è½½å…¥æ‰€æœ‰ç»„ä»¶ä½¿ç”¨ from_pretrained() æ–¹æ³•åŠ è½½æ‰€æœ‰ç»„ä»¶ã€‚å¯ä»¥åœ¨é¢„è®­ç»ƒçš„ runwayml/stable-diffusion-v1-5 ä¸­æ‰¾åˆ°å®ƒä»¬ï¼Œæ¯ä¸ªç»„ä»¶éƒ½å­˜å‚¨åœ¨å•ç‹¬çš„å­æ–‡ä»¶å¤¹ä¸­ï¼š\nfrom PIL import Imageimport torchfrom transformers import CLIPTextModel, CLIPTokenizerfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler# è‡ªç¼–ç å™¨ï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡vae = AutoencoderKL.from_pretrained(\"sd-v1.5\", subfolder=\"vae\", variant='fp16', use_safetensors=True)# åˆ†è¯å™¨tokenizer = CLIPTokenizer.from_pretrained(\"sd-v1.5\", subfolder=\"tokenizer\")# æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡text_encoder = CLIPTextModel.from_pretrained(\"sd-v1.5\", subfolder=\"text_encoder\", variant='fp16', use_safetensors=True)# å»å™ªå™¨Unetï¼Œä½¿ç”¨fp16åŠç²¾åº¦æƒé‡unet = UNet2DConditionModel.from_pretrained(\"sd-v1.5\", subfolder=\"unet\", variant='fp16', use_safetensors=True)\n\nå°†é‡‡æ ·å™¨æ›¿æ¢ä¸º UniPCMultistepSchedulerï¼Œè€Œä¸æ˜¯ sd1.5 é»˜è®¤çš„ PNDMSchedulerï¼Œé€šè¿‡ä»¥ä¸‹ä»£ç å®ç°ï¼š\nfrom diffusers import UniPCMultistepScheduler# é‡‡æ ·å™¨ï¼Œä¿®æ”¹ä¸ºUniPCMultistepScheduler è€Œä¸æ˜¯é»˜è®¤çš„PNDMSchedulerscheduler = UniPCMultistepScheduler.from_pretrained(\"sd-v1.5\", subfolder=\"scheduler\")\n\nä¸ºäº†åŠ é€Ÿæ¨ç†ï¼Œå°†å…·æœ‰å¯è®­ç»ƒæƒé‡çš„æ¨¡å‹ç§»è‡³ GPUï¼š\ntorch_device = torch.device(\"cuda\")vae.to(torch_device)text_encoder.to(torch_device)unet.to(torch_device)\n\n\n\nåˆ›å»ºæ–‡æœ¬åµŒå…¥å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ä»¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚è¯¥æ–‡æœ¬ç”¨äºä¸º UNet æ¨¡å‹è¾“å…¥æ–‡æœ¬æ¡ä»¶ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å¯¼ç”Ÿæˆå†…å®¹ã€‚\n\nguidance_scale: è¯¥å‚æ•°å†³å®šç”Ÿæˆå›¾åƒæ—¶åº”ç»™äºˆæ–‡æœ¬æç¤ºçš„æƒé‡ï¼Œå³æ–‡æœ¬å¼•å¯¼çš„å¼ºåº¦\n\néšæ„é€‰æ‹©ä½ å–œæ¬¢çš„ä»»ä½•æ–‡æœ¬æç¤ºï¼è®¾å®šåŸºæœ¬å‚æ•°å¦‚ä¸‹ï¼š\nprompt = [\"a photograph of an astronaut riding a horse\"]batch_size = len(prompt)          # ç”Ÿæˆçš„æ‰¹é‡å¤§å°height = 512                      # ç›®æ ‡ç”Ÿæˆçš„å›¾åƒçš„é«˜width = 512                       # ç›®æ ‡ç”Ÿæˆçš„å›¾åƒçš„å®½num_inference_steps = 25          # å»å™ªæ€»æ­¥æ•°guidance_scale = 7.5              # classifier-free guidance æ¡ä»¶å¼•å¯¼å¼ºåº¦generator = torch.manual_seed(0)  # ç”¨äºç”Ÿæˆéšæœºå™ªå£°çš„éšæœºæ•°ç§å­\n\nä½¿ç”¨åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°å¹¶ç”ŸæˆåµŒå…¥ï¼š\n# åˆ†è¯text_input = tokenizer(    prompt,                                # æ–‡æœ¬    padding=\"max_length\",                  # å¡«å……æ–‡æœ¬ä»¥è¾¾åˆ°æœ€å¤§é•¿åº¦    max_length=tokenizer.model_max_length, # è®¾ç½®äº†æ–‡æœ¬åˆ†è¯çš„æœ€å¤§é•¿åº¦ max_length = 77    truncation=True,                       # å¦‚æœæ–‡æœ¬è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå°†æˆªæ–­å®ƒã€‚    return_tensors=\"pt\"                    # è¡¨ç¤ºè¿”å› PyTorch å¼ é‡æ ¼å¼çš„ç»“æœã€‚).to(torch_device)# å°†åˆ†è¯ç»“æœè½¬ä¸ºæ–‡æœ¬åµŒå…¥with torch.no_grad():    text_embeddings = text_encoder(text_input.input_ids)[0]\n\néƒ¨åˆ†å˜é‡ç»†èŠ‚ï¼š\ntext_input:&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;text_input.input_ids:  - type: &lt;class 'torch.Tensor'&gt;  - shape: torch.Size([1, 77]) [batch, max_length]  - dtype: torch.int64  - device: cuda:0    text_embeddings:  - &lt;class 'torch.Tensor'&gt;  - torch.Size([1, 77, 768])  - torch.float32  - cuda:0\n\næ¥ä¸‹æ¥è¿˜éœ€è¦ç”Ÿæˆæ— æ¡ä»¶æ–‡æœ¬åµŒå…¥ï¼Œå³å¡«å……æ ‡è®°çš„åµŒå…¥ã€‚è¿™äº›éœ€è¦ä¸æ¡ä»¶ text_embeddings å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼ˆbatch_size å’Œ seq_lengthï¼‰ï¼š\nmax_length = text_input.input_ids.shape[-1] # 77# ç”¨ç©ºæ–‡æœ¬è·å–idsuncond_input = tokenizer(    [\"\"] * batch_size,     padding=\"max_length\",     max_length=max_length,     return_tensors=\"pt\")uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\nè®©æˆ‘ä»¬å°†æ¡ä»¶åµŒå…¥å’Œæ— æ¡ä»¶åµŒå…¥è¿æ¥ä¸ºä¸€ä¸ªæ‰¹é‡ï¼Œä»¥é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼š\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])  - &lt;class 'torch.Tensor'&gt;  - torch.Size([2, 77, 768])  - torch.float32  - cuda:0\n\n\n\nåˆ›å»ºéšæœºå™ªå£°æ¥ä¸‹æ¥ï¼Œç”Ÿæˆä¸€äº›åˆå§‹éšæœºå™ªå£°ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„èµ·ç‚¹ã€‚å®ƒå°†é€æ¸å»å™ªæ¥ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚æ­¤æ—¶ï¼Œæ½œåœ¨è¡¨ç¤ºå°äºæœ€ç»ˆå›¾åƒå°ºå¯¸ï¼ŒVAEè§£ç å™¨åä¼šå°†å…¶è½¬æ¢ä¸ºæœ€ç»ˆçš„  å›¾åƒã€‚\nprint(vae.config.block_out_channels) # [128, 256, 512, 512]print(2 ** (len(vae.config.block_out_channels) - 1))VAEçš„å±‚æ•°ä¸º4ï¼Œæ¯ä¸¤å±‚ä¹‹é—´ä¸€æ¬¡ä¸‹é‡‡æ ·ï¼Œä¸‹é‡‡æ ·å€ç‡ä¸º8\n\nç”Ÿæˆåˆå§‹å™ªå£°ï¼š\n# shape: [1, 4, 64, 64]latents = torch.randn(    (batch_size, unet.config.in_channels, height // 8, width // 8),     generator=generator                                             ).to(torch_device)\n\n\n\nå¯¹å›¾åƒè¿›è¡Œå»å™ªé¦–å…ˆä½¿ç”¨ sigmaï¼ˆå™ªå£°ç¼©æ”¾å€¼ï¼‰ç¼©æ”¾åˆå§‹å™ªå£°åˆ†å¸ƒï¼Œè¿™æ˜¯ UniPCMultistepScheduler ç­‰æ”¹è¿›é‡‡æ ·å™¨æ‰€éœ€çš„ï¼š\nlatents = latents * scheduler.init_noise_sigma# init_noise_sigma = 1.0\n\næœ€åä¸€æ­¥æ˜¯åˆ›å»ºå»å™ªå¾ªç¯ï¼Œè¯¥å¾ªç¯å°†é€æ­¥å°†çº¯å™ªå£°è½¬æ¢ä¸ºæ–‡æœ¬æç¤ºæ‰€æè¿°çš„å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚å»å™ªå¾ªç¯éœ€è¦åšä¸‰ä»¶äº‹ï¼š\n\nè®¾ç½®å»å™ªæœŸé—´é‡‡æ ·å™¨ä½¿ç”¨çš„æ—¶é—´æ­¥é•¿ã€‚\nè¿­ä»£æ—¶é—´æ­¥ã€‚\nåœ¨æ¯ä¸ªæ—¶é—´æ­¥ ï¼Œè°ƒç”¨ UNet æ¨¡å‹æ¥æ ¹æ®  é¢„æµ‹å™ªå£°æ®‹å·® ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™é‡‡æ ·å™¨ä»¥è®¡ç®—ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„å™ªå£°æ ·æœ¬ ã€‚from tqdm.auto import tqdm# è®¾å®šé‡‡æ ·å™¨çš„è¿­ä»£æ­¥æ•°scheduler.set_timesteps(num_inference_steps) # 25# è¿­ä»£æ—¶é—´æ­¥ï¼šscheduler.timesteps#   - &lt;class 'torch.Tensor'&gt;#   - torch.Size([25])#   - torch.int64#   - cpufor t in tqdm(scheduler.timesteps):    # å¦‚æœä½¿ç”¨CFGï¼Œä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼Œåˆ™å¯ä»¥åœ¨batchç»´åº¦æ‰©å±•è¾“å…¥ã€‚    latent_model_input = torch.cat([latents] * 2)    # é¢„æµ‹å™ªå£°å‰ï¼Œæ ¹æ®æ—¶é—´æ­¥tç¼©æ”¾Unetçš„è¾“å…¥x_t    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)    # é¢„æµ‹å™ªå£°æ®‹å·®    with torch.no_grad():        # unetçš„è¾“å‡ºç±»å‹ï¼š&lt;class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput'&gt;        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)        noise_pred = noise_pred.sample        # &lt;class 'torch.Tensor'&gt;        # torch.Size([2, 4, 64, 64])        # torch.float32        # cuda: 0        # ä½¿ç”¨CFGè®¡ç®—æ–°çš„å™ªå£°æ®‹å·®    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)        # ä½¿ç”¨é‡‡æ ·å™¨è®¡ç®—ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„æ ·æœ¬ï¼šx_t -&gt; x_t-1    # é‡‡æ ·å™¨çš„è¾“å‡ºç±»å‹ï¼š&lt;class 'diffusers.schedulers.scheduling_utils.SchedulerOutput'&gt;    latents = scheduler.step(noise_pred, t, latents)    latents = latents.prev_sample\n\nè§£ç æ½œåœ¨è¡¨ç¤ºä¸ºå›¾åƒæœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ vae å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒï¼Œå¹¶é€šè¿‡æ ·æœ¬è·å–è§£ç è¾“å‡ºï¼š\n# åœ¨å°†æ½œåœ¨è¡¨ç¤ºè¾“å…¥VAEè§£ç å™¨å‰è¿›è¡Œç¼©æ”¾latents = 1 / 0.18215 * latentswith torch.no_grad():    # VAEè§£ç å™¨è¾“å‡ºç±»å‹ï¼š&lt;class 'diffusers.models.autoencoders.vae.DecoderOutput'&gt;    image = vae.decode(latents)    image = image.sample\n\næœ€åï¼Œå°†å›¾åƒè½¬æ¢ä¸º PIL.Image ä»¥æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼\n# è°ƒæ•´å›¾åƒçš„èŒƒå›´å’Œæ•°æ®ç±»å‹num = len(os.listdir('./results'))image = (image / 2 + 0.5).clamp(0, 1).squeeze()image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()image = Image.fromarray(image)image.save(f'./results/learn_sp{num}.png')\n\n\n\nä»åŸºæœ¬ç®¡é“åˆ°å¤æ‚ç®¡é“ï¼Œæ‚¨å·²ç»çœ‹åˆ°ç¼–å†™è‡ªå·±çš„æ‰©æ•£ç³»ç»ŸçœŸæ­£éœ€è¦çš„åªæ˜¯ä¸€ä¸ªé™å™ªå¾ªç¯ã€‚è¯¥å¾ªç¯åº”è®¾ç½®è°ƒåº¦ç¨‹åºçš„æ—¶é—´æ­¥é•¿ï¼Œå¯¹å…¶è¿›è¡Œè¿­ä»£ï¼Œå¹¶äº¤æ›¿è°ƒç”¨ UNet æ¨¡å‹æ¥é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™è°ƒåº¦ç¨‹åºä»¥è®¡ç®—å…ˆå‰çš„å™ªå£°æ ·æœ¬ã€‚\nè¿™å°±æ˜¯Diffusers çš„è®¾è®¡ç›®çš„ï¼šè®©ä½¿ç”¨æ¨¡å‹å’Œè°ƒåº¦ç¨‹åºç›´è§‚ã€è½»æ¾åœ°ç¼–å†™è‡ªå·±çš„æ‰©æ•£ç³»ç»Ÿã€‚\nNext:\n\nContribute a community pipeline (huggingface.co)\nPipelines (huggingface.co)\nè¿ä½œæœºåˆ¶ï¼šStable Diffusion with ğŸ§¨ Diffusers (huggingface.co)\n\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"3.è®­ç»ƒdiffusion model","url":"/2024/04/09/3.%20%E8%AE%AD%E7%BB%83Diffusion%20Model/","content":"Train a diffusion modelæ— æ¡ä»¶å›¾åƒç”Ÿæˆæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§æµè¡Œåº”ç”¨ï¼Œå®ƒç”Ÿæˆçš„å›¾åƒä¸ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ä¸­çš„å›¾åƒç›¸ä¼¼ã€‚é€šå¸¸ï¼Œæœ€å¥½çš„ç»“æœæ˜¯é€šè¿‡åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥è·å¾—çš„ã€‚\næ‚¨å¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ°è®¸å¤šè¿™æ ·çš„æ£€æŸ¥ç‚¹ï¼Œä½†å¦‚æœæ‚¨æ‰¾ä¸åˆ°æ‚¨å–œæ¬¢çš„æ£€æŸ¥ç‚¹ï¼Œæ‚¨å¯ä»¥éšæ—¶è®­ç»ƒè‡ªå·±çš„æ£€æŸ¥ç‚¹ï¼\næœ¬æ•™ç¨‹å°†æ•™æ‚¨å¦‚ä½•åœ¨ Smithsonian Butterflies æ•°æ®é›†çš„å­é›†ä¸Šä»å¤´å¼€å§‹è®­ç»ƒ UNet2DModelï¼Œä»¥ç”Ÿæˆæ‚¨è‡ªå·±çš„Butterflies\n\ndiffusers_training_example.ipynb - Colaboratory (google.com)\n\nè®­ç»ƒé…ç½®ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«è®­ç»ƒè¶…å‚æ•°çš„ TrainingConfig ç±»ï¼š\nfrom dataclasses import dataclass@dataclassclass TrainingConfig:    image_size = 128                       # ç”Ÿæˆå›¾åƒçš„åˆ†è¾¨ç‡    train_batch_size = 16                  # è®­ç»ƒbatch    eval_batch_size = 16                   # éªŒè¯é˜¶æ®µçš„å›¾åƒæ•°é‡    num_epochs = 50    gradient_accumulation_steps = 1        #    learning_rate = 1e-4    lr_warmup_steps = 500    save_image_epochs = 10    save_model_epochs = 30    mixed_precision = \"fp16\"               # `no` for float32, `fp16` for automatic mixed precision    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub    push_to_hub = False  # whether to upload the saved model to the HF Hub    hub_model_id = \"TO-Hitori/my-awesome-model\"  # the name of the repository to create on the HF Hub    hub_private_repo = False    overwrite_output_dir = True  # overwrite the old model when re-running the notebook    seed = 0config = TrainingConfig()\n\n\n\nè½½å…¥æ•°æ®é›†æ‚¨å¯ä»¥ä½¿ç”¨datasetsåº“è½»æ¾åŠ è½½Smithsonian Butterfliesæ•°æ®é›†ï¼š\nfrom datasets import load_dataset# æ•°æ®é›†è·¯å¾„ï¼šæœ¬åœ°è·¯å¾„æˆ–hugging-faceè·¯å¾„config.dataset_name = r\"D:\\MyData\\smithsonian_butterflies_subset\"dataset = load_dataset(config.dataset_name, split=\"train\")\n\n\nåœ¨æ­¤æŸ¥æ‰¾å…¶ä»–æ•°æ®é›†ï¼šhuggan (HugGAN Community) (huggingface.co)\n\nDatasets ä½¿ç”¨ Image åŠŸèƒ½è‡ªåŠ¨è§£ç å›¾åƒæ•°æ®å¹¶å°†å…¶åŠ è½½ä¸ºæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–çš„ PIL.Imageï¼š\nimport matplotlib.pyplot as pltfig, axs = plt.subplots(1, 4, figsize=(16, 4))for i, image in enumerate(dataset[:4][\"image\"]):    axs[i].imshow(image)    axs[i].set_axis_off()fig.show()\n\nè¿™äº›å›¾åƒçš„å°ºå¯¸å„ä¸ç›¸åŒï¼Œå› æ­¤éœ€è¦å…ˆå¯¹å®ƒä»¬è¿›è¡Œé¢„å¤„ç†ï¼š\n\nResize å°†å›¾åƒå¤§å°æ›´æ”¹ä¸º config.image_size ä¸­å®šä¹‰çš„å¤§å°ã€‚ \nRandomHorizontalFlip é€šè¿‡éšæœºé•œåƒç¿»è½¬å›¾åƒæ¥å¢å¼ºæ•°æ®é›†ã€‚ \nNormalizeå¯¹äºå°†åƒç´ å€¼é‡æ–°ç¼©æ”¾åˆ° [-1, 1] èŒƒå›´éå¸¸é‡è¦ï¼Œè¿™æ˜¯æ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥èŒƒå›´ã€‚\n\nfrom torchvision import transformspreprocess = transforms.Compose(    [        transforms.Resize((config.image_size, config.image_size)),        transforms.RandomHorizontalFlip(),        transforms.ToTensor(),        transforms.Normalize([0.5], [0.5]),    ])\n\nä½¿ç”¨Datasetsçš„ set_transform æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€åº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼š\ndef transform(examples):    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]    return {\"images\": images}dataset.set_transform(transform)\n\nå°†æ•°æ®é›†åŒ…è£…åœ¨torchçš„ DataLoader ä¸­è¿›è¡Œè®­ç»ƒï¼\nimport torchtrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n\n\n\nåˆ›å»º UNet2D æ¨¡å‹ Diffusers ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ä½¿ç”¨æ‚¨æƒ³è¦çš„å‚æ•°è½»æ¾åœ°ä»å…¶æ¨¡å‹ç±»åˆ›å»ºã€‚ä¾‹å¦‚ï¼Œè¦åˆ›å»º UNet2DModelï¼š\n# åˆ›å»ºU-Netfrom diffusers import UNet2DModelmodel = UNet2DModel(    sample_size=config.image_size,  # å›¾åƒåˆ†è¾¨ç‡    in_channels=3,                  # è¾“å…¥å›¾åƒçš„é€šé“æ•°é‡    out_channels=3,                 # è¾“å‡ºå›¾åƒçš„é€šé“æ•°é‡    layers_per_block=2,             # æ¯å±‚ä½¿ç”¨çš„æ®‹å·®å—ä¸ªæ•°    block_out_channels=(128, 128, 256, 256, 512, 512),  # æ¯ä¸€å±‚çš„è¾“å‡ºé€šé“æ•°é‡    down_block_types=(        \"DownBlock2D\",              # æ®‹å·®ä¸‹é‡‡æ ·æ¨¡å—        \"DownBlock2D\",        \"DownBlock2D\",        \"DownBlock2D\",        \"AttnDownBlock2D\",          # æœ‰spatial self-attentionçš„ä¸‹é‡‡æ ·æ®‹å·®æ¨¡å—        \"DownBlock2D\",    ),    up_block_types=(        \"UpBlock2D\",                # æ®‹å·®ä¸Šé‡‡æ ·æ¨¡å—        \"AttnUpBlock2D\",            # æœ‰spatial self-attentionçš„ä¸Šé‡‡æ ·æ®‹å·®æ¨¡å—        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",        \"UpBlock2D\",    ),)print(\"UNet2DModel have {} paramerters in total\".format(sum(x.numel() for x in model.parameters())))# UNet2DModel have 113673219 paramerters in total\n\næ£€æŸ¥æ ·æœ¬å›¾åƒå½¢çŠ¶ä¸æ¨¡å‹è¾“å‡ºå½¢çŠ¶æ˜¯å¦åŒ¹é…ï¼š\nsample_image = dataset[0][\"images\"].unsqueeze(0)print(\"Input shape:\", sample_image.shape)print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)'''Input shape: torch.Size([1, 3, 128, 128])Output shape: torch.Size([1, 3, 128, 128])'''\n\n\n\nåˆ›å»ºé‡‡æ ·å™¨æ ¹æ®æ‚¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œè®­ç»ƒè¿˜æ˜¯æ¨ç†ï¼Œé‡‡æ ·å™¨çš„è¡Œä¸ºä¼šæœ‰æ‰€ä¸åŒã€‚\n\nåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡æ ·å™¨æ ¹æ®å™ªå£°ç”Ÿæˆå›¾åƒã€‚\nåœ¨è®­ç»ƒæœŸé—´ï¼Œé‡‡æ ·å™¨ä»æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç‰¹å®šç‚¹è·å–æ¨¡å‹è¾“å‡ºï¼ˆæˆ–æ ·æœ¬ï¼‰ï¼Œå¹¶æ ¹æ®å™ªå£°è°ƒåº¦noise scheduleå’Œæ›´æ–°è§„åˆ™update ruleå°†å™ªå£°æ³¨å…¥å›¾åƒã€‚\n\nè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ DDPMScheduler å¹¶ä½¿ç”¨ add_noise æ–¹æ³•å‘sample_image æ·»åŠ ä¸€äº›éšæœºå™ªå£°ï¼š\nimport torchfrom PIL import Imagefrom diffusers import DDPMSchedulerfrom torchvision.utils import save_image, make_grid# ç”¨æ€»æ­¥æ•°æ¥åˆå§‹åŒ–é‡‡æ ·å™¨noise_scheduler = DDPMScheduler(num_train_timesteps=1000)# è®¾å®šåŠ å™ªåºåˆ—ï¼Œè¿™é‡Œé€‰æ‹©äº†7ä¸ªä¾æ¬¡å¢å¤§çš„æ—¶é—´æ­¥timesteps = torch.LongTensor([50, 150, 250, 450, 650, 850, 990])# æ—¶é—´æ­¥çš„æ•°é‡ä¸ºbatchï¼Œé‡‡æ ·å›¾ç‰‡çš„å½¢çŠ¶åä¸‰ä¸ªç»´åº¦ï¼Œæ„å»ºé‡‡æ ·å™ªå£°noise = torch.randn(timesteps.shape + sample_image.shape[1:])# åˆ©ç”¨é‡‡æ ·å™¨çš„add_noiseæ–¹æ³•å°†å™ªå£°æ³¨å…¥é‡‡æ ·å›¾ç‰‡noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)# ä¿å­˜å›¾ç‰‡è§‚æµ‹é‡‡æ ·å™¨çš„åŠ å™ªæ•ˆæœsave_to_show = make_grid(torch.cat([sample_image, noisy_image], dim=0))save_image(save_to_show, './test_scheduler.png')\n\næ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯é¢„æµ‹æ·»åŠ åˆ°å›¾åƒä¸­çš„å™ªå£°ã€‚è¿™ä¸€æ­¥çš„æŸå¤±å¯ä»¥é€šè¿‡ä¸‹å¼è®¡ç®—ï¼š\n# æŸå¤±å‡½æ•°ï¼šæœ€ç®€å•çš„MSEæŸå¤±å‡½æ•°import torch.nn.functional as Fprint('test loss func')noise_pred = model(noisy_image, timesteps).sampleloss = F.mse_loss(noise_pred, noise)print('loss value = ', loss.item())\n\n\n\nè®­ç»ƒæ¨¡å‹åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»æŒæ¡äº†å¼€å§‹è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†å†…å®¹ï¼Œå‰©ä¸‹çš„å°±æ˜¯å°†æ‰€æœ‰å†…å®¹ç»„åˆåœ¨ä¸€èµ·ã€‚ é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨å’Œä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š\n# å¼€å§‹è®­ç»ƒï¼šè®¾ç½®è°ƒåº¦å™¨from diffusers.optimization import get_cosine_schedule_with_warmupoptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)lr_scheduler = get_cosine_schedule_with_warmup(    optimizer=optimizer,    num_warmup_steps=config.lr_warmup_steps,    num_training_steps=(len(train_dataloader) * config.num_epochs),)\n\nç„¶åï¼Œæ‚¨éœ€è¦ä¸€ç§è¯„ä¼°æ¨¡å‹çš„æ–¹æ³•ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ DDPMPipeline ç”Ÿæˆä¸€æ‰¹æ ·æœ¬å›¾åƒå¹¶å°†å…¶ä¿å­˜ä¸ºç½‘æ ¼ï¼š\nfrom diffusers import DDPMPipelinefrom diffusers.utils import make_image_gridimport osdef evaluate(config, epoch, pipeline):    # Sample some images from random noise (this is the backward diffusion process).    # The default pipeline output type is `List[PIL.Image]`    # ä½¿ç”¨pipelineç”Ÿæˆå›¾åƒ    images = pipeline(        batch_size=config.eval_batch_size,        generator=torch.manual_seed(config.seed),    ).images    # å°†å›¾åƒæ‹¼æ¥ä¸ºç½‘æ ¼    image_grid = make_image_grid(images, rows=4, cols=4)    # ä¿å­˜éªŒè¯å›¾åƒ    test_dir = os.path.join(config.output_dir, \"samples\")    os.makedirs(test_dir, exist_ok=True)    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n\nç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Accelerate å°†æ‰€æœ‰è¿™äº›ç»„ä»¶åŒ…è£…åœ¨ä¸€ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œä»¥è½»æ¾è¿›è¡Œï¼š\n\nTensorBoard æ—¥å¿—è®°å½•\næ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚\nè¦å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¯·ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è·å–å­˜å‚¨åº“åç§°å’Œä¿¡æ¯ï¼Œç„¶åå°†å…¶æ¨é€åˆ° Hubã€‚\n\næ¥ä¸‹æ¥æ˜¯è®­ç»ƒæ ¸å¿ƒéƒ¨åˆ†ï¼š\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):    # åˆå§‹åŒ– accelerator å’Œ tensorboard æ—¥å¿—è®°å½•    accelerator = Accelerator(        mixed_precision=config.mixed_precision,                          # æ˜¯å¦æ··åˆç²¾åº¦è®­ç»ƒ        gradient_accumulation_steps=config.gradient_accumulation_steps,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°        log_with=\"tensorboard\",                                          # ä½¿ç”¨tensorboardè®°å½•æ—¥å¿—        project_dir=os.path.join(config.output_dir, \"logs\"),             # æ—¥å¿—è·¯å¾„    )    # å¦‚æœåœ¨ä¸»è¿›ç¨‹    if accelerator.is_main_process:        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹        if config.output_dir is not None:            os.makedirs(config.output_dir, exist_ok=True)        # ä¸Šä¼ åˆ°HFçš„è®¾ç½®        if config.push_to_hub:            repo_id = create_repo(                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True            ).repo_id        # åˆå§‹åŒ–è¿½è¸ªå™¨        accelerator.init_trackers(\"train_example\")    # ç”¨accelerateåŒ…è£…ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®åŠ è½½å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨    # ä¿è¯è¾“å…¥å’Œè¾“å‡ºçš„é¡ºåºä¸€è‡´å³å¯    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(        model, optimizer, train_dataloader, lr_scheduler    )    # åˆå§‹åŒ–å…¨å±€æ­¥æ•°    global_step = 0    # å¼€å§‹è®­ç»ƒæ¨¡å‹    for epoch in range(config.num_epochs):        # åˆ›å»ºè¿›åº¦æ¡        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)        # è®¾ç½®è¿›åº¦æ¡æè¿°        progress_bar.set_description(f\"Epoch {epoch}\")        # å¯¹æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå¾ªç¯        for step, batch in enumerate(train_dataloader):            # ä»æ•°æ®é›†è·å–å›¾åƒ            clean_images = batch[\"images\"]            # ç”Ÿæˆå³å°†åŠ å…¥å›¾åƒçš„å™ªå£°            noise = torch.randn(clean_images.shape, device=clean_images.device)            # è·å–å½“å‰batch-size            bs = clean_images.shape[0]            # ä¸ºå½“å‰batchä¸­æ¯ä¸ªå›¾åƒéšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥            timesteps = torch.randint(                0,                                          # èµ·ç‚¹                noise_scheduler.config.num_train_timesteps, # ç»ˆç‚¹                (bs,),                                      # å½¢çŠ¶/æ•°é‡                device=clean_images.device,                dtype=torch.int64            )            # ä½¿ç”¨é‡‡æ ·å™¨ï¼Œæ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥çš„å™ªå£°å¹…åº¦å‘å¹²å‡€çš„å›¾åƒæ·»åŠ å™ªå£°            # è¿™æ˜¯å‰å‘æ‰©æ•£è¿‡ç¨‹            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)            # ä½¿ç”¨acceleratorç´¯ç§¯æ¨¡å‹æ¢¯åº¦            with accelerator.accumulate(model):                # é¢„æµ‹å™ªå£°æ®‹å·®                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]                # è®¡ç®—æŸå¤±å‡½æ•°                loss = F.mse_loss(noise_pred, noise)                # åå‘ä¼ æ’­æ¢¯åº¦                accelerator.backward(loss)                # æ¢¯åº¦è£å‰ª                accelerator.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()      # è¿­ä»£ä¼˜åŒ–å™¨                lr_scheduler.step()   # è¿­ä»£å­¦ä¹ ç‡è°ƒåº¦å™¨                optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦            # æ›´æ–°åŸºç£å¾’            progress_bar.update(1)            # è®°å½•æ—¥å¿—ï¼šæŸå¤±å‡½æ•°ï¼Œå­¦ä¹ ç‡å˜åŒ–            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}            progress_bar.set_postfix(**logs)            accelerator.log(logs, step=global_step)            # å…¨å±€æ­¥æ•°å¢åŠ             global_step += 1        # åœ¨æ¯ä¸ªepochåï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨evaluate()é‡‡æ ·ä¸€äº›æ¼”ç¤ºå›¾åƒå¹¶ä¿å­˜æ¨¡å‹        if accelerator.is_main_process:            # åˆå§‹åŒ–ä¸€ä¸ªpipelineï¼Œä¼ å…¥å½“å‰è®­ç»ƒçš„æ¨¡å‹å’Œè°ƒåº¦å™¨            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)            # æ ¹æ®éªŒè¯é¢‘ç‡ä¿å­˜ç ”ç©¶ç»“æœ            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:                evaluate(config, epoch, pipeline)            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:                if config.push_to_hub:                    upload_folder(                        repo_id=repo_id,                        folder_path=config.output_dir,                        commit_message=f\"Epoch {epoch}\",                        ignore_patterns=[\"step_*\", \"epoch_*\"],                    )                else:                    pipeline.save_pretrained(config.output_dir)train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextdiffusers_training_example.ipynb - Colaboratory (google.com)\nTextual Inversion (huggingface.co)\nDreamBooth (huggingface.co)\nText-to-image (huggingface.co)\nLoRA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"2.AutoPipelineåŸºæœ¬æ•™ç¨‹","url":"/2024/04/09/2.%20Auto%20Pipeline/","content":"AutoPipelineåŸºæœ¬æ•™ç¨‹AutoPipeline çš„è®¾è®¡ç›®çš„æ˜¯ï¼š \n\nè½»æ¾åŠ è½½ checkpointï¼Œè€Œæ— éœ€çŸ¥é“è¦ä½¿ç”¨çš„ç‰¹å®špipelineçš„ç±»å‹ \nåœ¨å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨å¤šä¸ªpipeline\n\nAutoPipeline ç±»æ—¨åœ¨ç®€åŒ– Diffusers ä¸­çš„å„ç§Pipelineã€‚å®ƒæ˜¯ä¸€ä¸ªé€šç”¨çš„ã€ä»»åŠ¡ä¼˜å…ˆçš„ç®¡é“ã€‚ AutoPipeline ä¼šè‡ªåŠ¨æ£€æµ‹è¦ä½¿ç”¨çš„æ­£ç¡® Pipeline ç±»ï¼Œè¿™ä½¿å¾—åœ¨ä¸çŸ¥é“ç‰¹å®šPipelineç±»åç§°çš„æƒ…å†µä¸‹è½»æ¾åŠ è½½ä»»åŠ¡çš„æ£€æŸ¥ç‚¹\n\nAPI: AutoPipeline (huggingface.co)\n\nDiffusers èƒ½å¤Ÿå®Œæˆè®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ‚¨é€šå¸¸å¯ä»¥å°†ç›¸åŒçš„é¢„è®­ç»ƒæƒé‡é‡å¤ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®å¤ã€‚å¦‚æœæ‚¨å¯¹åº“å’Œæ‰©æ•£æ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œå¯èƒ½å¾ˆéš¾çŸ¥é“è¦ä½¿ç”¨å“ªä¸ªç®¡é“æ¥å®Œæˆä»»åŠ¡ã€‚\nä¸ºæ‚¨çš„ä»»åŠ¡é€‰æ‹© AutoPipelineé¦–å…ˆé€‰æ‹©ä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæƒ³è¦ä½¿ç”¨ runwayml/stable-diffusion-v1-5 æ£€æŸ¥ç‚¹æ¥è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ä»»åŠ¡ï¼Œè¯·ä½¿ç”¨ AutoPipelineForText2Imageï¼š\nfrom diffusers import AutoPipelineForText2Imageimport torch, ospipeline = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",                 # æƒé‡è·¯å¾„    torch_dtype=torch.float16, # æ•°æ®ç±»å‹    use_safetensors=True,      # ä½¿ç”¨safetensorç±»å‹çš„æƒé‡    variant='fp16',            # åŠ è½½æƒé‡æ—¶é€‰æ‹©æ–‡ä»¶åä¸­å¸¦æœ‰â€˜fp16â€™çš„).to(\"cuda\")prompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"image = pipeline(prompt, num_inference_steps=25).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur_{num}.png')\n\næ·±å…¥å±‚æ¬¡æ¢è®¨ AutoPipelineForText2Image \n\nAutoPipelineå°†è‡ªåŠ¨ä» model_index.json æ–‡ä»¶ä¸­æ£€æµ‹ StableDiffusionPipeline ç±»\næ ¹æ®ç±»ååŠ è½½ç›¸åº”çš„æ–‡æœ¬åˆ°å›¾åƒçš„StableDiffusionPipeline\n\nmodel_index.jsonæ–‡ä»¶å†…å®¹ï¼š\n{  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.6.0\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n\n\nåŒæ ·ï¼Œå¯¹äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡çš„AutoPipelineï¼ŒAutoPipelineForImage2Image ä» model_index.json æ–‡ä»¶ä¸­æ£€æµ‹åˆ° â€œStableDiffusionâ€ æ£€æŸ¥ç‚¹ï¼Œå¹¶å°†åœ¨å¹•ååŠ è½½ç›¸åº”çš„ StableDiffusionImg2ImgPipelineã€‚\nè¿˜å¯ä»¥ä¼ é€’ç‰¹å®šäº Pipeline ç±»çš„ä»»ä½•å…¶ä»–å‚æ•°ï¼Œå¦‚guidance_scaleã€strength\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForImage2Imageimport torchimport requestsfrom PIL import Imagefrom io import BytesIOpipeline = AutoPipelineForImage2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    variant='fp16',).to(\"cuda\")prompt = \"a portrait of a dog wearing a pearl earring\"url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"response = requests.get(url)image = Image.open(BytesIO(response.content)).convert(\"RGB\")image.thumbnail((768, 768))image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur2_{num}.png')\n\n\n\nå¦‚æœæ‚¨æƒ³è¿›è¡Œå›¾åƒä¿®å¤ï¼Œåˆ™ AutoPipelineForInpainting ä»¥ç›¸åŒçš„æ–¹å¼åŠ è½½åº•å±‚çš„ StableDiffusionInpaintPipeline ç±»ï¼š\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForInpaintingfrom diffusers.utils import load_imageimport torchpipeline = AutoPipelineForInpainting.from_pretrained(    r\"D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',).to(\"cuda\")img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"init_image = load_image(img_url).convert(\"RGB\")mask_image = load_image(mask_url).convert(\"RGB\")prompt = \"A majestic tiger sitting on a bench\"image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]num = len(os.listdir('./results'))image.save(f'./results/Auto_tur3_{num}.png')\n\nå¦‚æœæ‚¨å°è¯•åŠ è½½ä¸å—æ”¯æŒçš„æ£€æŸ¥ç‚¹ï¼Œåˆ™ä¼šæŠ›å‡ºé”™è¯¯\nä½¿ç”¨å¤šä¸ªPipelineå¯¹äºæŸäº›å·¥ä½œæµç¨‹æˆ–å¦‚æœæ‚¨è¦åŠ è½½è®¸å¤šPipelineï¼Œä»æ£€æŸ¥ç‚¹é‡ç”¨ç›¸åŒçš„ç»„ä»¶ä¼šæ›´èŠ‚çœå†…å­˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”æƒ³è¦å†æ¬¡å°†å…¶ç”¨äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡ï¼Œè¯·ä½¿ç”¨ from_pipe() æ–¹æ³•ã€‚æ­¤æ–¹æ³•ä»å…ˆå‰åŠ è½½çš„Pipelineçš„ç»„ä»¶åˆ›å»ºæ–°Pipelineï¼Œæ— éœ€é¢å¤–çš„å†…å­˜æˆæœ¬ã€‚\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Imageimport torchpipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",     torch_dtype=torch.float16,     use_safetensors=True,     variant='fp16',)print(type(pipeline_text2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'&gt;\"\n\nç„¶å from_pipe() å°†åŸå§‹çš„ StableDiffusionInpaintPipeline ç±»æ˜ å°„åˆ° StableDiffusionImg2ImgPipelineï¼š\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(type(pipeline_img2img))# \"&lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'&gt;\"\n\n\n\nå¦‚æœæ‚¨å°†å¯é€‰å‚æ•°ï¼ˆä¾‹å¦‚ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ï¼‰ä¼ é€’ç»™åŸå§‹ Pipelineï¼Œåˆ™è¯¥å‚æ•°ä¹Ÿä¼šä¼ é€’ç»™æ–°ç®¡é“ï¼š\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(    \"sd-v1.5\",    torch_dtype=torch.float16,    use_safetensors=True,    requires_safety_checker=False,    variant='fp16').to(\"cuda\")pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)print(pipeline_img2img.config.requires_safety_checker)\"False\"\n\nå¦‚æœæ‚¨æƒ³æ›´æ”¹æ–°ç®¡é“çš„è¡Œä¸ºï¼Œæ‚¨å¯ä»¥è¦†ç›–åŸå§‹ç®¡é“ä¸­çš„ä»»ä½•å‚æ•°ç”šè‡³é…ç½®ã€‚ä¾‹å¦‚ï¼Œè¦é‡æ–°æ‰“å¼€å®‰å…¨æ£€æŸ¥å™¨å¹¶æ·»åŠ å¼ºåº¦å‚æ•°ï¼š\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(    pipeline_text2img,     requires_safety_checker=True,     strength=0.3)print(pipeline_img2img.config.requires_safety_checker)\"True\"\n\n\nNextAPI: AutoPipeline (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"4.è½½å…¥ LoRA è¿›è¡Œæ¨ç†","url":"/2024/04/10/4.%20%E8%BD%BD%E5%85%A5%20LoRA%20%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86/","content":"\n      \n        bb1dc9bacfe3cd6aff299e3ddad7816ff49ed3bc0e6cf4972e7d1b8ba6b5cbe8832742676388db28d2b562985101fd9598c15b0dea26aeaaa65268a782a2d54eb30ace93699eb503d6bf86d1b6e4608222a16a01cae532573a335042fe911004223f3bd2cc76cbbe3cb365a43c6f26ba432b8bf011c384728879f92c6b0cb1048a5ae0eb5f613f93b8520a9a515950572336c47b826da89cf20a9665d72ed5d421aea8cfe0dad870c41d65d5732f729b495f475cf6067d2812f4aad363f77611343d2c9dc2eec1736debab855230b9668b026d2f060a77eb1ba0c66880eaac41d5f394b07ca9de0b4c52bdae9137ad3a3ba964386713ff850bf94d458b8d65b0e5d4af35513ba2c87d49322de9f0bc1d4c20d18b939a843ba424fc8dd5cf01612683113090493ed283e76a82084114e4a6d7c6fe58460b9c0c1703b1d5317d5d5388c2e5eec876c2bd53eeb8a6bc083fbc1db825d6960cb7e93216a661133adb4325f31810eaef4d9405991a14ceed992a5d67e8c64ae42a823c8e13c09d7310003b5c95e04787447670bb8a83337af1ec2bd0f3e3622bbc920965e67aa40fa8877c2e7e2ae14a8664dc99336c642aba99010d70fd20338d885ea91b311ad62445be656aaa2bc62f55ac04d888b416d2272072ad0a27d10aaadb6a5dc5e966ec317dc4b31c606f3bd0fd12dd7eb0650826353981a5ab6209b747fec7ae4499da2d3ba3ebe78ef915164f51b115a89eae26c31efecf2d2f5e6cf4da71042b944bc7535e7599a23585ed32aa30f1b4b45b040a1cbd6181f7b762244681ca2a3001b2dbad612536dbdfd459d00efcfd7f58ec5996f9f4dd3b75c8ed68f06eba2a70c397cc0e5eba5d8baa06293be7be53b7746e7661b8528164aa3d7ae842d12d78ea4596da4c418822f734ac80d226b4b9f0e4f7015cdd00301483099231e21a6d0f21c9fc2ae076ed48c00f0e5a56e13b40ccbc78b026ab93eb504d13d59e8f5371302864f651f7bbeb1ded9b25130021a3aed1b06ad147273b7c879d0b70a33d31fff11c5ce2fed2f0398dcd5929efcb91131d71b0f0a6918db2610f478c3ec8fdbfdf81b4cb1fdc1d871a0f140026c62622644fd45ca368af65aeba9b98e7f116e95f0d6ddec6b9832686efd84067f9287c930ae16c1663e063e68ecc89ee62c89294a5e7d74b9c86b3aa4a18bf580056f918285c1267a198f073c84606d6645a13b85b6a0b28363ae41a177068f5885113e240e1ca751687c915773ab3a86e6fc4404a0173787c0d7538b21e948b709730db1448d0fffebef76d74deafd3ed1bd8719a7a6d2e8f07a3946100615849f16763ff24fd28109bcd2b9234c5b471e0a366fba31382ac629f88c1bd6376bd1848301cb48410fa083deaf814bfe59d61a44ac0e312db58d7a95076961da570cd41aca1bc5ed865aaed2921b4676c090ce2e8d0bf04ea802538b31bc676355c9ec4ebc8aee5b72d34e36d7b97c81823115b4296512c7d8a4b5ed7f0db5c3bf6d3e114c5101c24f4f9c47754de092c19431678a65bbcecd38b040003c848bf0a5f26de5d962d34e8a5b91d4167ae54c799263422645d0c6e1615aee1606a2e5dc53d0d7751581e3ee19cedb7bda04f09762b46f725d81745a46fbc2ec8cc84d71aaad37c3ecc9d9735bc19c7e33a866fda5ae0adfc108772ed8b2414bceedaa86bee2b2f4c48450d3f65d9b0654b26ba624bc80e36219ab9da8ba63a4c941fbfe3da4645968da6317645ef56bcb3e0491608ae2ca52bc14121093c8f4dc7819ec1b4c8d0289d92b11210b2d2bcc9a3b67cc624e03cfb3ec67128e7fe9a5c533ad272bff682bffc4cb2a3afd9145570a08c7d8b607d1c796be304531cd04d7bc08efb43ae0ff2cc1d948f096eb3060dd10fe6ef032b1a63ca99e4a4d9ebfc874dc879d86f9f2e7e908c02aa032fa67bc9cc65d1fa3752dfa16e9d029428f3dafc48a30cb281e603ee89bf0adc336926985007209d9d9f042825c390b6c35886cd2ef0248d6f178ab60d5b1a110d89d273538055cb389aea8d995e6277256442a77784db252b3cedc16b775a22eb72a44256bd6684a2e91b0cc9a0202103f61d4f29fb58b218ecc4fa8bab5baab973ddee600740a0cc5a54155886b4d3dfe545c1493a17c80b104d1ddcfe960eb848273f81abff4e5e825e5b207a2189f3a945fcca21735f8ba6f930d94ffd313b2db6e8d4d2ccbfe0d28fae93c980f4222f2a07993de0aaac8536700c37dab42c2c53f953f0c21cd4a1eb37482e861c1d8e1469001e6f57713bfc48f3b912f4b813b43b57b042890a83eadd0f490fd0fe12012ad2ca00d6e7083c8cb1a597444a218fb83d7744d9e94a57e10a3431db1d777f976b0880ca02998a06718ed5a754f41e7fc0a6f2b3b7ce16743da7d3a9558300915c05971a3955669fd9db7decc6a07e267b1083195133c2116d3ea4640ae9e2502cc6937dbd9e9b33869451e8ccc4e96198c563d64f225202d1b252e6a2d09e8a75fbfaebc46239143b289ca0cd08f877c1c3fd04d71fe65764709525a029fe34f76eab3176819c83e9dcbfc36c7846a5e1e95a69d6a20984e4206fe2c04b41dffce4b1c9dbd8cc6d597cd517d29e2c535f8a89dd8958201addbd2e00970ffc66f7d2257bd749c7c9ab81fa11d62acf5f7b9a367aacae3ab0a448c64bfef2d86b59e76991e8deef637d47df211d1564978dd02a57fa4e18d8c406f3dbf8e3fa408b871c973ad7b9ec2df3585b5c978e96775aba56324b5191fb6022376f03404b08e5fb41a1d5e870c12e1910e5252a34456daf8c436b62c6dfc915733b53ce6147785ed3dc9b74252640822ea888ca3eb20c5a5c9975dbe4942297e594256113432d28376be7ce8d764dd801016896fecc3c75e5c712bbd09ed796d9e1713f82f8e6e88eef54fd45eb63c65fd2aacab67185f43082f75025e7706035436075667cd15375fd0da0898a9a3710a08f2cf43be8be0fbb475cb734899fe8a898b3964e3e0610fca65d39ea84f6bc3b9d91a7ef3e21a432832098e79c03c05684bf6371a73caf41a3e6d2b2559532f533afe5ecbd4a3a328f233f2a3903aac7807a6ae2ea2ccbc40cbf38e139c9909324552bb2fde840e71ddddb0d9a3e1cd942dc9202c341a146160d1caf36ade7edd7beac7b3effbf8a729981e66cf42df884f8c902e1c5d82af7e66a19a95b9a6ebc5ab8e0aca8aabd0b6e568185be3ddbeac3583ef8fd8c60d227138011c8ee6a55c8a435256ccc51a17ee3bd0c89ca8967345e0eea642a77d2f1be01b1dcebd1e7da7969abd6695a2d4d0a3f6a5c16191dd67498bba2e5b6925434a257fd10bd0c38535b40b8caec3adf563ef1df9253ba2ff5496908282fa2f871128b5ff69e3520d7d4ea92c535c5c05e0bc6fec72d01b00cd1b99f5b5b39cbe194dd98262057dcd27822f958280d48ee8cbe793108205d8b8f4ffa17d43c0a3b6472b6f192bedd21e81a1ad5ebc8946b91866c2f0e8f4e2bd41732a9ce18bcb7d35312a567f54521b22911c6394edf371e59073809f082255040bc4b260ca1d0320d593b49df85b38f4b4801ef158b079dc08c76d42a0470da8e3d8a58036901a966680f4bb04239878f383c7aaca36925c1d162daed9a063727f0066f6980899c95fab251980167ec0a6efe298415ad609d14e5d02af03740ac7cf912d8709329877ac45638cbe1352599d1a2f15914e275da8e6543acf1a7db998ad82e43dd1191eb2c1ffca4ffa2237d62c4ad29c7692559e55df9a5c39790aab543e5e820ba7166513b50f98d6a22b2c3faf9f40aff57c3e72b3ee7cbec0220b7711d880f50087b450fd4f7feec6b471ce755b68ebe35716810d0c36539ef99ff3a65d8ec9678774e1e0542926dca97565ec41e4a4f8da32b8a9da3c1213f8e9ad9a291de21698ad30debde749d57a2ccf82844cd80aa82a081f4c4d71fefc52c384232d818e89082c68b482029b6b51d0ffe4d8f5332c30f233099f0643d232b16db4f554860f5b043ce1c84ccb1f7d2686c2d3027e5c7de9ee94c503baa2c7d7d9b4257e30052687e6afedb59b0cc32119b4f7ac261cae66d62bb54c2e96b4ad1f412d1dfff528609f47b64010cf12c839ef06bafabd37fc591f736d6099a7220d8ffc93e9b93627331a0e102e199b866a8a9ab40c5a590b602ad875e4b2b0437575028ceaf33bc094e056ab3845789ee3ab6758f52caafb2f21fd89bb70f789bd22e50fd312c31b492df42f7a0c7140080709bb155a6d8ad4a4ca322341e99effe4951c16b52020b42d21c5478a78b8d8d5c1ef1dcb0f65e4745019468d52b5f734bd4a0069357e668e9a2964a2d037ee96294096ed1b94bc4d94914bcb9b3223f974dc6620491a4f95ead020b613f0c80829e9c143b4bc38801989cc80c2bf89cef2f4980be07ef93037bee5c1090c2050475f4bada269d3179c6a3748b6183696d3356cb009665b11b04e8f8560f98bf8c607939b991b55dcbfd18538b1aa661efa843871d61adf620ebec2cb5bff268c2ab977bf79ba11730126365fca260e3a769e9feabea12d4e21340e92f5d9d90aff9d64ce040caaff35faec71eeefb15f0f819547d71144fbae1513bb24535b108abc0b56b3aaddaef14471a253088ae65c4b38bbc2b2ac1a18eaf93497773b43e20e5dc1419d3e625422d9529436c9da04d5b20d03bf1d3a170d25fbeb2b98712979e892aa000f7d41fca44c537dddeabb88324d04629955965a0b2a326885448c6344ca774bb3060341b0719297a8109e86318effe971908811084d73c61cdc5ed202b5d1b764d3f270d0064579786576bc8fa4a0ff7d92a809d2d81d0c63730a98767ac41019f079ba0d4214476e2bf75902685056739c5251b1df3aba1331296fadcdd990a58bf08a27a0069eb492e5e398b421751013ab00d346a1723fd57a1451464c6530e7e49c6d1cef4ed73ced8fae786dea96b5bb844696739cedc6e82e4eb37934b5aa1f5596b4865d6c1c4ae34f2cdc43dee43c6da1b776cebaa7ead058a12f68c30e30c9a7644192a7910ff4d6d063f6bf8dd446d3365ab5d5307b8353488aa4bdb133b950914aff076843e4f6ed4fe1085899d2d677a61359a8130f2e42788c2a9d0d7a325c80a6057f0bc4e837356565ec08a7303042c30431f2610909c228a2b10b9797027529030052f359d5b4574aad6c62e71b2ca6024addeda6ab24777ff315030f6f0d5b67cce353942340a2fd188d983ee93a393a4be5eae557941bb07c076385fc2fcf33b97f9ed035d05537d8fa5f523f8d0caedd541d16d0b26d4ec58dd0d3daedf9c1b5d282a641462874695c1ff96a3b977d23404fc8d1234ebd2bf43e1eb11724e66c3ed1c138095f241a82f7b90e8fd69b93885f42dbfeb1a3b9cd03b27863ae8a8e771dcb12965a28d4bd90fa4c9b23847f3024456c122bd647b794882da0b2093e0838d40af6ea3b1fb41d78be554d77a801d8a92a9a80f993742751e99f2702bf4d17a34317247fdb47907e4ad40573c0becdf8cd703d1025570c7305b827725cec97f22288da801845a55fd533b12c9ce48b8e19e1a996d5431ae2036aef1429bb0bddd03c745d04dada19d81e81884dd8a98b51fcbbb7db351a64274e352e8a3ff58bff48295f37d878e5ff47c4a734b32a0ecac0d71f977eb4c2a4d445a443fb66b333ff87126525f9e423dfa747638bb47e1ab0505638a1bfd648ff0057e46ca10a8e0e2b4a98f1b9a5515435ff7f88428d7b916508f2229980973238b377097db2808e4c7f658e74b477e5d100236f0040b888561855346688e5da2c3543895b1551e6b56f283b6bd53028cd5901ac13ee967a0760606e07a2a36ea204fae0c30354e854ab736fbc41c8769b964256ba4f48b70c43fcea3609d523a6d071e0c50ad8feca36279d36b8cfc2906ed7371f83ce4da8f75fae26ed8deb9c22de95c513badb1f9f564ca0d5981cb395a980501b5b6e4c5bd0313d413eedc47f3e4bb2c842d44dcb85c054bb7b258436df242b0fdff166f7d850c85ce680cd401aa448b8d31d6d6dd0a08fc1e3cce84ddcea6d7e28de029782947ba71fa7c08caf630c33bdfcd1870b58c1804d0b8dd6a2e3e348cae838343ba746d3102afa14c893557804cbaa3ccdac870467b1abb054e819e43de97ba9c75df230c205fc10de557378e6e6a5f03e8bf36035bc3fa3d9de5b333b51d3e3a7cf682ff913873de733847950487485ae65f0d08d423b5ceff28aafd6bd1b2a917849fcfdd19fb33b17cf1313ec9ec103800e5720d3a034ed3a27b128ba4caeabcd1f8011f1e99044609d6024fbd74ff7299eeb69a538efe2458b96eedf572bc1248728ca1f9ad52e886fd12dfd8992001255c03f273e7234026dc17cc585e5d1bd3d2b77c2ee76bbff28470e98b1731c02fa071544eb4af3ddf8cd364d8b585851f21312942760b1ce68b2090fc8c32a769193a6ce85dba816eec6499cea492316beba9060985d154d792426ab5249a03bc24763d50e8f84132bffbef82c7b3b55b608fa0fca6c1fdc7b8416a2c35b064571a47023d9b541a496bea13e942d8ab9a78cee28f647217c05cc5e088794238d4ae9d7723255913e86fa269074360184aee4ebb781820f26bcd874442573d83d25442c0a8f390c692b84e572852abbf696ca029a312601ccbf3c0b2549864167fc5a80617074e48cbd6d5a1cc97e143de8028893367f21b607c6818b7605e9ebc51d136662aa63c898e5fff070be23175513a7bb32d4d5ea8599c396b94039ada9579d1fa494f4a227feebd7b45b5db5c7a906f769e8ed050a6aa2f781858465301c1d4d5577cd35d276a3b1e29d7c3aaa06c17240479efb3cf412f7be0fb80a18060576e08bc19761c5e621fd94b02056acbc79092daeb8c91c405b709bb3c4b865c3fd4ef811a5fe4fdbef04f3a99eb119be4e0b5858a4241c6dfd4a0f7becb58dab34937e43f8d5c3284d6b00d0ebc45e1220166d363234be66389f69938e110d20f29055e5136d7010150f46b19b17aaf3307d1abf84ee029b2c49951faa06d5739ebecc1cdd2c1260c527a43d1a7e4b4c5ba04a61e3325589b5aa5c44d1b76b891c9d635da1072c61937cabbe3ae08b6172d1bba040842f2de15b3092990a41afbb50cc9c6615cd7b7e706e43f1053be7e193aacf5f09a43e706fb786c8b352cc5785c24ad4487f0e604eeb09d0fb3a26f27ec753f744ec69c5983e1e27fd7cf64cec1a60b7da24a9b543d9a752a0d291a0839af8e60bd3df6081bc4dfdcc676caf6905fcf1af9925d727ec9c7fb2e82d9dc2888c444f2149e05d03aeee223c618b94cd6274763c62dda2173a1860898249e484f31c820573691036729d7ff63b215178519cfaaabb49296f12d2f1bd79d09363520e74dbc7118640780e19088cf813e71bbac6604f16e34690081e32ebb1e769b7b2b9a6caea6796a6c13e51421e0400ffe4d387dd365025efccedd82fb1f6721d1c0c3fb9ecae4530e6f39458b7ac5b7f19e3d04c49db6efba80a209d285c77c67d0d4693bb816b9de4fc2be2cbcdfb045844fec39b6bfff20636e98cc13e79f938a2e1819ca91811b6baebb0ca5d13dfabf4e3b8d5b849fac9dc3ce473704ad1a0b5c874d0f6cd90b7b5595c2f4183c865e095d3d13c83b7db5e278066db0ffdee19e3159ee494fde34cb401a89c2f32426373f449b108fe6d5c4c77ea8a5d1bdb9af5583a88a90ec3181f49b8101af4409452fbe4d85cb1b570d437da21a334e90b9899b28eac557a75b9d385a402edd4c4e725cabda7f1e38c995209896ddcc11857f32005999549905ddf798a67c5f1b877c2cc1ca6caac8b7b19709fd1325255c21bd5592d99a5d212b69c070a2c5bf437f6f5cc87e55ee3f9a9e1685987423868cb7570d330d15b710b03b0f3eca9ad05a14797809ab4a668a6968038c236da22ac6efd43c019f841f252d20f302d876012c0e439196c6b05265e9409a3d9588f073b8d667263807aacb4333875625cb558489344d519d7b7bd1f161cab25b7716632224e71c3b1e29035af414966a266fe24aa1c6637b5387d359a9702cd30d3db0c822589c8af404989444762e9217e4838f51aac70f9d50d6d1f12c2b333b906150eb91c7d94c341b33e8bc9d5d2c05551be9c494b547dc6b019e4b6b5b38c7ae8c33e9caf5d04f590c6c5ed69e7c067b5a1383f88960bb8136116996f4ca0c456e1b19ed6489cb818111d6cb42880d26eba985a666faeaa20f29a3d4d47562a39490fb91c4e50850c3e5a4d2785a10cc64f8586823ac8dad583126df67a435a3d1d6197945d44aed4cbce9d54e4d99b525a353759b456c6c2cf4bcccd0e2d7a05c8777a618d6b75dc589aa6dddfd330e5efd8b50cd49e90829c4e37a29d09a2411bebc4da8f3f16976debb3a8d29df1d6513176d8dfbcdc6cc5fb68049b72f583c6c766a9bee5769f0f444226f6b401dce4468e44256bb3f87f7ea79ee68aef09138261d88bb83bdac8842599f1146a7bcdb52c2d7315285924f862e1a2d37fc36b25da4b49984b107c44d3a321d32a5ceb71075871cde2b9d90c40569d17d29258e3ef64b56d4beae13f964894223174c696d899cba78a8fa129967494a0e88179086ba28fb97c6a7d8fe4cf87778dedb6d77f49f4e5d1786008f2db59986e791586be8b9b61e538b8e62867e707f534299e937f43e9fc28bf77c0874819d3e6296b5a2fd6c28420fbb13d5520945a98997bddd83e1afcd1d3de160070e0687cc21266c261c1659ab69b67bb9899c09579af761f3048bf8de38a425328fbe59aaa155a79dc4538f3a7676316a19e5ca1a935f82b73a31bbe42444eb3cf8b5a43ed2ecd8b6aa9330b09ba2fbe6405d8e5e56f6ef411787d6c6cbb7665e2178f656d7afcd33939ecd0583a7277d96eb347184379800b53e8eb9c2f7a5bc953f2885ba5bf911edc0fe4382c72b2a3ad3d1dd217dc0ca9df29e8fc285fd0352924b715e688612446e911af083d7f4df1daf7fa9acbded06ed8361e61e75944fef533521d343e0dbb958874d4d1f11d7048e76d6f7ed43640c8a7d5e7a8912f53f08e1823273a7e464c4da2753ae40ac720487ed951d5481e3bb068bfb51d8843b2b8408adabbf86f84398db7af2ed642701da452e35a221dbf476425167d27bea64f93a5cf4ddf6e7f85e4736fdfce9228f49f00161f9a95843d0fcdfcd0e8c7fbd788240ef719f37376b99c652dd34c89151eef6d1d48809f54c46b9c9d8e563cbf69ed9abbd65ca58f639b571c5e089df2fc91a216a10af61eb3bcef76b23dc4a577b8bcf8d8eaa2f6a2d07448e8d38459605bd516ddfdce95f834ef418c2a5cf4ce5602a0a88ca4c95935b393ddf74bc8a9690f4a80b075a32707c512cce074118e2ab46a7ba66454c0f9f9c666265fabbab46716386c967c8f805b86b0d2b8c613c0f52cb00c01bd47bb0d5d3df9f55b259b14e16e84d6acce74dd9d26a4955df1400975d672bfc111a4ed3234e45851feee4a90be845726f6d79730d61d6de18d0dd90eb69b483ce2e2084487b7994bee091687863b3f1858a758a06e07ef0e716576b82aa6f0d3e58f85ecf9df14e844741c13bb36fff439475746ba50fee73f0ce60b19314bdce5c7d242eb31721b9bd30bc83dbcf42eb650c056ee5ec6876725ae0d595ffca9d7280f8980017a3304bec90b25790f208586b0212c8d00d1a023b6a99067609a298122052694ad9a87553a8a0ca43daeddf51a8cdb396a5883d347e08c2a353ffe618e69f2ad5cefad59752d450427d4c7f0dea8ff4799f7405e0213eead93c6dfabd9ac83fc22b5017c8cca648f5c07fc0f4df38d9cb803c84f60d676c41144507175b7165363f6b59cf9bf2c58353554725fc431fd0f5b07e7cb34e2f22953deb947bace8b6cf598dc3ee836636a44f8316d3e65a86681d6177ae05ce81e46ca89a8569818b653482867de5117ec59fce27d98f19f43944f66f00054d91c2f53b758f4f627a485bb847fae84862104a6826f13420fc1cfe577f91547062ca235b94cb2326dbfefb8ccef5c6dd9f3884d15c2c463026c0ac20f371cd7b6d561064f1e229d8df68ce0f6d166462d1a7bea46d747068fb86cd77c999480bbf3ffe03cce7bcbe056adb418eac80c5d2e3f992c6cdcbc23d19419c1dcbb8c384502fd3e8d5b4eb27af6c8b191acf2bbc5c7d761b2d725e1b09b255ad8c89dd1aa2299be2e20a41c7cfd0cd3724c9373c39e15317c24508b5905bc9d063333da57cab768f1c6571a9688de5acb8a33e2b19b3923b58c44c736da3dc209458527e6e4afaf6f222905c9b524552529798d7f8772fbc0a4d1d974cbdd47a59f2f492c1ec33bc88dd19eb28d3481c53c986bfb87f05a2fb2be6018a0c74211bc841189a43c7c61092f1029fe5fa9411460e895305a4260844dbed7bad8a9fad4fe5ad0e6ee28e529ebb1fa8bc26319bf33e9094f139036b218340ee769e8f9883b11c36c1c97dff75d4db7103b790e530202e7d6e3d729b5af66f68920535da4e22f54cc395081edf75319201973caf74a4b33962cad6ba2f0e5e5f521aff4ab21daeb93ac7c00f8ac9714301d48d673a465883606c012791e0576420ee78ddb9d26d56a651966a420184adc3f33dd8c03b9aa6aa9838fb1819b83284e820cdb7b58ff1dbf318eb78aaf90b506656d453b73fc88162733a4246190f12a659d4188a74dd32d52c9a875a1b5e4a9c27a87eb1fd764c384cf85cac6d2f953c2c270a870f8f177697bf66f0c4ac811d87ca07b9b76cade22c132e04eb9e0d61dd802268bf3f261a260104126558cd527569e644a07c0586f941c04e09cf2be70527a1622cae712003b0fa6f3f26360c1cd588917f38277c70b863ef3e29e642fc73cd091a6bfa43082debc357de968b220274851d299bb8244d2143b9bc84004ece967e39ae7e725295b2e36aff4fa329fdd351134ebd0daa4276df77527cb7a635cdc2962f0eb40e3b0e7911506b0f112cfc82e8fe42419e96c85b5dd40b5c01b5ee7705bd355cf5a48427068450f6f8e3b69b98e2783f5423efce2e8643851fe13bf6b994a4a62ffd063d49cc73b9388a03c4dc14aebf031a63d20a229de22370f66f50c7ea80ee1002daaf4f3db6f39a0e5087f940011500b9ad38a40f65c573f8c0d69c46229745207e273d146cafbb667cb7578140126eed78eb52ec41dba038a096acd588a92bc741048a3d5c617c912abe447e0a5a916747c074815713ff8a68571011035539b063b053eb1d3642efe0ca7d8aea2c0513d081bd4c2eb4938a40a6657cc0ed0256dff8e3299688d4631344693ec90e122f23ffe7b658a626dcbca0517630ed9d9be11e09c090e7e5eefc6316d4b1d1fab4e38e3d35b8b32b7046aca5b1c2e568f71c81fc8df3135d41819db8145eed28d0732d6da1d22de1fbb9a183803aa1850949d21a2f2917952ca7f33db32d2ddcf7d554cfb26b89c5189f159a2d1131819b9f5bf79a40d7bc25a7d663d838c30588cc6bb8425b92a33f9923607a8693b38f41b5ba7e3e3eaa86007164bcddf89a810c5d993c31e8ea12b32df318285c351bfabf8ab2ae5b36aacd1388190d8989af1d4be3cb7579c7f0738b51e5dfa8fc1eb1397607eb771a54b9f4f57f6d9970da6a2defe4d772c945e76a899e1faa717c30b0a3b5d5c395c2abe6f027bdd1ccd086546668eeda7f0346c2615eb34a74a6be1c25df88fa39d7bbefc974b73ecec78f5471d2d68a14748dfb799a507851ca345d008d40d3cc51b598a03edf047dc9a173d670a92e12d40ae5f87b216a730f1c9059e14ba93a72e05f2e2b2c3eb4762b363a6e9d54f89c1494ff8713d318cf01e9f0a733bbe5d42a7ae46fcc1ad2e12f38791c7c3a7f16e18f4b31e2ba1922c33e709093dd69b1c9c8d420624df701a65aea84ab322a6d762767a6761a09e2c7cccd7b7c41f8975dace2fc7a5d8442a9dd11064837173490ca4c4c34c1211fce5e0312c561b0547940d31ef478d165c1e7ded894a76159b0cb3ace99f32c50c1f15bb0c7c8b48b4a6ad137070affbb0812040b23c7b84abb9c33bc6c4b3fa685c04d9fcd0b81a8728a70f5176ffaf9a6fc29c07e32ca2d0d2c7ee52cea00aa4a62afb1fafb0568fa3e90b27144c91338d668b3218f68fed1ca233c8b8df82b23d523b736bec78a4a077ef78d20ad4a9f6e612301d89b4659f9d1455741716e3fd280160c4e1b439641adfd0fb8901349a37cea44ccd2ed5220ffbbb24b8d9a253ae6f93c115985d02346061a923224608b8329a8be86a07545944ce871019286bc10c300e8132b3bb7d80bb54f6fed23e5692550275dcf11711f5f0c974f920362643e20b16d0fddb51e8209c61b4eef2687e5cf928871bc81f98e1821245b3a0b09f1248c9e7c25db0ad0ef218d024c98c97fc7df0e78d4efae6182da8b291198ce3de17d662db15a24a387d6400acc67d5fbffb91aa34112231d3ff635ebbcb835f7629569fe3945d703c7e96a74e6a4812ff256c7c17e41e9b57fb8e2eca5f46a631046ba83db773b17bd79ecf07d23ad7b634b610cc2aeffb1176ca10e0ef7932f2b125173d252acf8a81e631f5a2e3438e629b6dc6277a801633128a41cb710ae579ad24099967dfcc6ec478351e8a9c5a8efbea7b6bc795447d50794eaae27ae73ad00e32de3eca08142bdf2f6ca378fbb19bbc912cc9fb029e9fc1d2a69b342e3621d2085777c66acd91ab1e574c40c579a86dbc2970ec55293fbf41942f86679f041515560d2fc3ef364a8d323d60586ba2258a09eeb0cdd289cf569faff8a5b7e75660f68af35cc50a5031d2f4c24c677f6e43823f80f31b50be8951cc291284ddb7d6fcf849032a41680721684d95759c638c75e5b8b9acb68b58c5f5f20337d7d0f00fb408e2c9873b026c31d0516bc7755c29f767df665e9868fb077906442d2f745e4ad75e2f3d93d37f8ffddd694c86d207e54db5381b1a0d41d3d6a435b8d50fe31b64cc7f672497bb9781ba680f0d27ed1812aa9419666cfddb4f84617022daf61ff56470afbd1d94873863940e7f900368d381af413374f6ecfca32f1b800d0130cc3b3e4bc182be9dc291274001f79203a93e0d00946ef7e0937fe740c02794cf3ca8a84b14acb3758fdd8f69119db27b27399219929258bc19a458ecef1ecba52d0e23283ac4914a59d4212181928393243c0740cc4ff0ed49a64f53172103efb5891c39a60be5ebfe9b9929469f509cd080e94d78a5c4275d44e1731895dc6b56d2edcdac1695484cf35c1947fdb51d843dc7232dd4ee2922c8539464822b51c6ff25f7628d659d753af86fa05fd19fb3156f22bdc10d119db46b39cffe83cd445b6832cab2b66ea26e0e02b1a7cd38ce7bf44a50c5dd1560e699078a0147b5dfa5f720260ea5888390b867395ce77f85424fc4356fe9e7c11c8022380d485110b114c8a046f06bba27e571a56e36df47f91132edc6671c06211aa7b6f66a1142950b08173feb710ebfc35ce69f3323cbc77285c004c8460fcef54eb92d636eecf9b9be4931a40d222b8f2bc2cd0c466f7409e1217281ffe0a9b509dffb6879e27b263cdb60232588f8948a7306f3fc84b33e415b9ebe01dffe43fadc2a02e1c208d01efd8bec8400bfc5663c825a54798eeed23163f9eae92fcc66c4c1fd05e0d060b810c0fee48823db9f26706b2aea941929ccc11c615eb4a5f93a8e49dd3f5def87345356e38182ca447feaf3f1746410ee17d25c0330ed4b352c3ef4cbae2b717dbe88b67874d152b76cc61e845dc96e8c9a1dbdf3bc2865d32c4e5d44235594f47270a53628898fa17dfaff9172fc0e948cbe701322f83574be8ba79ecc65917c85b0318bb09514eb060357a81946add55623828565a3759d73d4b595a0b5048ddd7784fe72cf0d8bb2b6effc5c23861b54b23cb223dee9a049f0bfc3d1d6bcb09f8e3086996ed61a7f871d96f167f5e07e3dc13f926c6599e07c2a4430b92e7183539e309dd4e455b42829c5d78fd14634c418c92abc2a6ff182c71632d31376861bcde690c981cdcde6eedefb4fae8215b2ac1ab54fa6493686116e0da48e127c8785e29f31a2381680c34f723bae89fc027f5e1a8e18864b2f17aa8543c708712dd01d5f3394b1e87d20ceafa909b18d2089e202d30a09eab72a50d6c2013ab40b5165a24386d26da2a19d4307eee335951bac17905de58db2e48a4c04a01719f712642af00bdc51624b1db5be4e41c8dc3a24b6211d55f09c6dc783caf765108de90709bf04f857126ccec613140541284b6526c3c07dc2c9dfb601ae81da1b9a02dc90586163f5cba723916f14c99ab3d8d7a7d8ade0bd40df30f1843841f5e6d3595797390b0225d1d8fb026c3cee8aa1627b92593568da49a32f1354cf8cb3e3fab48a60f504e07b61d096e4fad4322f5a0baf42ed53fc61eebc5182b618a7c12e87043cbff780ece3b3d745e92d220eb3c289076e55c7d3b9ba33b3b5deadffe265b7c8353cd9972178bc2333da67e4bf97d048b9e86f1994e2b87595f0f3c3daf1404a88ec9faddda8e9a9b46e712b3302efbde8740e40bceee7b20910d9fde04ccf5743f7f4fb1e34b8c5c0a56f14dbdb2a9bbaa211458363f7e0e26f9798a80eb29bef8a063a2327f48e9d64229965e4b5913e5ee94ece61894332c6d2a7054468de8da6838de39a0757dd4a32056a57adffa7fee491b43c879ce43b26da51556b5321aedeb63da6e13e8d026e2b191f846c86d527fa315a1fee74c04086e0e9fe29797e855f92f59274feb537c13e45b29b89e0e24f0a7684b6d464eb34664fed7de703eac7d2922e8dbc3b55fac54d29cb80fb65a63537615cb41d802e16a217be1410ed03f24466fde3104f72c698c4d8ddbfd9198feb123a8185c0d71781722f2738b11e3ae24d01a7610cf4384e7a35a399262c9c49572d2bda8c1b6c804a5aefee0033cdfbe3ebdad510a836a3aa0097946063c9942f9ba283309c63a2d75813a2600928dfcec00e834a445029b2c31a3c2455b021d4072e1d0fe36beec32a88781035c63fd55237c2b5f11aa44d2e019437ed52ef1813652a012e4de5eb3d01817b72ad5ca6d3c701ea3ef6323605f27eed7a183a37b39be82073a1c88d4c25f979984b925a9cab3ce98d24ff4317e394ac103d5e1727060a543830b308beaad7e39585b421fde8c754eabc13ad9a482d38ee734c7e908ce4cfd70cc5c489cc8d3d1c3be9a20c0fac57da20d3db40f26d59ed2b82800a29859da9a97d55407d9602fcf78d9ef943c9499b38aab2de125f3c77de4bc8a8f1577e15ae343cd42f32d7f18e1d0030763bdf99c6831e6df28adc377f672cf18e7bbe049ab8b3c1baf69d587455bbaee9a75cf0ce1f6f7912c0246304bc963930898fc9c3b46452fa95afa828babbb7211cf27324a3ebfa5d37d12208f8137b59a345269162b4adae4f439457103253d5b3ed271ac56d9cc97db769aafc30f01f48c7d1bc27b7f5b03c1bccbfe6a7c3525179d889e96ce06113594bc40ade3a0973227e6aab9fd79621d24404ffb750f642a6d2fa284f638e1b940fd1d902cc2408ad0e8c5eeb269c0cce5d3cdaaf1b1c6b86f9990ca42c332474caa9c746ef211f4d4913ec53bdcef2d12da76e6def56fa7c80b6ca45dff3b2bab4f92d58678c3a021d92cd7012af9d2aeb3c83c02ac02e93fb09fc4c45c229c3f1f4ea5fe4815200657e6587489d1fd9662f1b5f565cdfd7f4ab76cf67bfc40abb12d63b74a7016b351b6fca62215f40a356dfaf8d98c37505d3feda08ae1ab\n      \n      \n        \n          \n          \n            è¯·è¾“å…¥ä¸ Rhodes Islandâ„¢ å–å¾—å¼±ç¥ç»è¿æ¥æ—¶çš„å£ä»¤ï¼š\n          \n        \n        \n      \n    \n    ","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"5.åŠ é€Ÿ T2I æ‰©æ•£æ¨¡å‹çš„æ¨ç†","url":"/2024/04/10/5.%20%E5%8A%A0%E9%80%9F%20T2I%20%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86/","content":"åŠ é€Ÿ T2I æ‰©æ•£æ¨¡å‹çš„æ¨ç†ç”±äºè¿­ä»£å’Œåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œæ‰©æ•£æ¨¡å‹æ¯” GAN æ¨¡å‹æ…¢ã€‚æœ‰å¤šç§æŠ€æœ¯å¯ä»¥è§£å†³æ­¤é™åˆ¶ï¼š\n\næ¸è¿›æ—¶é—´æ­¥è’¸é¦ (LCM LoRA)\næ¨¡å‹å‹ç¼© (SSD-1B) \né‡ç”¨é™å™ªå™¨çš„ç›¸é‚»ç‰¹å¾ (DeepCache)ã€‚\n\n\n\nPerforming inference with LCM-LoRA (huggingface.co)\n\nsegmind/SSD-1B Â· Hugging Face\n\nDeepCache (huggingface.co)\n\n\n\nä½†æ˜¯ï¼Œä¸ä¸€å®šéœ€è¦ä½¿ç”¨è¿™äº›æŠ€æœ¯æ¥åŠ é€Ÿæ¨ç†ã€‚ä»…ä½¿ç”¨ PyTorch 2ï¼Œæ‚¨å°±å¯ä»¥å°†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨ç†åŠ é€Ÿæœ€å¤š 3 å€ã€‚\né™ä½ç²¾åº¦-bfloat16å¯ç”¨ç¬¬ä¸€ä¸ªä¼˜åŒ–ï¼šé™ä½ç²¾åº¦æˆ–æ›´å…·ä½“åœ°è¯´ bfloat16ã€‚ä½¿ç”¨é™ä½çš„ç²¾åº¦æœ‰å‡ ä¸ªå¥½å¤„ï¼š\n\nä½¿ç”¨é™ä½çš„æ•°å€¼ç²¾åº¦ï¼ˆä¾‹å¦‚ float16 æˆ– bfloat16ï¼‰è¿›è¡Œæ¨ç†ä¸ä¼šå½±å“ç”Ÿæˆè´¨é‡ï¼Œä½†ä¼šæ˜¾è‘—æ”¹å–„æ¨ç†å»¶è¿Ÿã€‚\nä¸ float16 ç›¸æ¯”ï¼Œä½¿ç”¨ bfloat16 çš„ä¼˜åŠ¿å–å†³äºç¡¬ä»¶ï¼Œç°ä»£ GPU å€¾å‘äºä½¿ç”¨ bfloat16ã€‚\nä¸ float16 ç›¸æ¯”ï¼Œbfloat16 åœ¨ä¸é‡åŒ–ä¸€èµ·ä½¿ç”¨æ—¶æ›´å…·å¼¹æ€§ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨çš„æœ€æ–°ç‰ˆæœ¬çš„é‡åŒ–åº“ torchao ä¸å­˜åœ¨ float16 çš„æ•°å€¼é—®é¢˜ã€‚\n\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # ä½¿ç”¨float16    variant=\"fp16\").to(\"cuda\")# Run the attention ops without SDPA.pipe.unet.set_default_attn_processor()pipe.vae.set_default_attn_processor()prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\n\nSDPAAttentionæ¨¡å—è¿è¡Œè€—æ—¶é•¿ã€‚ä½¿ç”¨ PyTorch çš„ scaled_dot_product_attention å‡½æ•°åï¼Œå®ƒçš„æ•ˆç‡è¦é«˜å¾ˆå¤šã€‚ Diffusers ä¸­é»˜è®¤ä½¿ç”¨æ­¤å‡½æ•°ï¼Œå› æ­¤æ‚¨æ— éœ€å¯¹ä»£ç è¿›è¡Œä»»ä½•æ›´æ”¹ã€‚\nfrom diffusers import StableDiffusionXLPipelineimport torch, osnum = len(os.listdir('results'))pipe = StableDiffusionXLPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    torch_dtype=torch.bfloat16,   # ä½¿ç”¨float16    variant=\"fp16\").to(\"cuda\")prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"image = pipe(prompt, num_inference_steps=30).images[0]image.save(f'results/sdxl-output{num}.jpg')\n\n\nSDPA (huggingface.co)\n\ntorch.compile\nWindows ä¸æ”¯æŒï¼š\nRuntimeError: Windows not yet supported for torch.compile\n\nPyTorch 2 åŒ…å« torch.compileï¼Œå®ƒä½¿ç”¨å¿«é€Ÿä¸”ä¼˜åŒ–çš„å†…æ ¸ã€‚åœ¨ Diffusers ä¸­ï¼Œé€šå¸¸ç¼–è¯‘ UNet å’Œ VAEæ¨¡å—ï¼Œå› ä¸ºå®ƒä»¬æ˜¯è®¡ç®—æœ€è€—æ—¶çš„æ¨¡å—ã€‚\nfrom diffusers import StableDiffusionXLPipelineimport torchtorch._inductor.config.conv_1x1_as_mm = Truetorch._inductor.config.coordinate_descent_tuning = Truetorch._inductor.config.epilogue_fusion = Falsetorch._inductor.config.coordinate_descent_check_all_directions = True\n\nåœ¨ç¼–è¯‘ UNet å’Œ VAE æ—¶å°†å…¶å†…å­˜å¸ƒå±€æ›´æ”¹ä¸ºâ€œchannels_lastâ€ä¹Ÿå¾ˆé‡è¦ï¼Œä»¥ç¡®ä¿æœ€å¤§é€Ÿåº¦ã€‚\npipe.unet.to(memory_format=torch.channels_last)pipe.vae.to(memory_format=torch.channels_last)\n\nç°åœ¨ç¼–è¯‘ UNet å’Œ VAE å¹¶æ‰§è¡Œæ¨ç†ï¼š\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)pipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\ntorch.compile æä¾›ä¸åŒçš„åç«¯å’Œæ¨¡å¼ã€‚ä¸ºäº†è·å¾—æœ€å¤§æ¨ç†é€Ÿåº¦ï¼Œè¯·å¯¹ inductor backend ä½¿ç”¨â€œmax-autotuneâ€ã€‚ \nâ€œmax-autotuneâ€ä½¿ç”¨ CUDA å›¾å¹¶ä¸“é—¨é’ˆå¯¹å»¶è¿Ÿä¼˜åŒ–ç¼–è¯‘å›¾ã€‚ CUDA å›¾é€šè¿‡ä½¿ç”¨é€šè¿‡å•ä¸ª CPU æ“ä½œå¯åŠ¨å¤šä¸ª GPU æ“ä½œçš„æœºåˆ¶ï¼Œå¤§å¤§å‡å°‘äº†å¯åŠ¨ GPU æ“ä½œçš„å¼€é”€ã€‚\nPrevent graph breaksæŒ‡å®š fullgraph=True å¯ç¡®ä¿åº•å±‚æ¨¡å‹ä¸­æ²¡æœ‰å›¾å½¢ä¸­æ–­ï¼Œä»¥å……åˆ†åˆ©ç”¨ torch.compileï¼Œè€Œä¸ä¼šé™ä½ä»»ä½•æ€§èƒ½ã€‚å¯¹äº UNet å’Œ VAEï¼Œè¿™æ„å‘³ç€æ›´æ”¹è®¿é—®è¿”å›å˜é‡çš„æ–¹å¼ã€‚\n- latents = unet(    latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds-).sample+ latents = unet(+   latents,     timestep=timestep,     encoder_hidden_states=prompt_embeds, +   return_dict=False+)[0]\n\n\n\n\n\nç»„åˆAttentionæ¨¡å—çš„æŠ•å½±çŸ©é˜µSDXL ä¸­çš„ UNet å’Œ VAE ä½¿ç”¨ç±»ä¼¼ Transformer çš„æ¨¡å—ï¼Œç”±Attentionæ¨¡å—å’Œfeed-forwardæ¨¡å—ç»„æˆã€‚\nåœ¨Attentionæ¨¡å—ä¸­ï¼Œä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„æŠ•å½±çŸ©é˜µï¼ˆQã€K å’Œ Vï¼‰å°†è¾“å…¥æŠ•å½±åˆ°ä¸‰ä¸ªå­ç©ºé—´ä¸­ã€‚è¿™äº›æŠ•å½±åœ¨è¾“å…¥ä¸Šå•ç‹¬æ‰§è¡Œã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†æŠ•å½±çŸ©é˜µæ°´å¹³ç»„åˆæˆä¸€ä¸ªçŸ©é˜µå¹¶ä¸€æ­¥æ‰§è¡ŒæŠ•å½±ã€‚è¿™å¢åŠ äº†è¾“å…¥æŠ•å½±çš„çŸ©é˜µä¹˜æ³•çš„å¤§å°å¹¶æ”¹å–„äº†é‡åŒ–çš„å½±å“ã€‚\nåªéœ€ä¸€è¡Œä»£ç å³å¯ç»„åˆæŠ•å½±çŸ©é˜µï¼š\npipe.fuse_qkv_projections()\n\n\nè¿™ç§æ–¹æ³•æ•ˆæœä¼˜å…ˆï¼Œä¸”ä»…é€‚ç”¨äºéƒ¨åˆ†æ‰©æ•£æ¨¡å‹\n\nåŠ¨æ€é‡åŒ–è¿˜å¯ä»¥ä½¿ç”¨è¶…è½»é‡çº§ PyTorch é‡åŒ–åº“ torchaoï¼ˆcommit SHA 54bcd5a10d0abbe7b0c045052029257099f83fd9ï¼‰å°†åŠ¨æ€ int8 é‡åŒ–åº”ç”¨äº UNet å’Œ VAEã€‚é‡åŒ–ç»™æ¨¡å‹å¢åŠ äº†é¢å¤–çš„è½¬æ¢å¼€é”€ï¼Œå¯ä»¥é€šè¿‡æ›´å¿«çš„ matmulsï¼ˆåŠ¨æ€é‡åŒ–ï¼‰æ¥å¼¥è¡¥ã€‚\nå¦‚æœçŸ©é˜µç›¸ä¹˜å¤ªå°ï¼Œè¿™äº›æŠ€æœ¯å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚\nNextSpeed up inference (huggingface.co)\nSDPA (huggingface.co)\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"YAML","url":"/2024/03/14/YAML%E5%9F%BA%E7%A1%80/","content":"YAMLYAML Ainâ€™t a Markup Language\nYet Another Markup Language\nYAMLæ˜¯â€YAML Ainâ€™t a Markup Languageâ€ï¼ˆYAMLä¸æ˜¯ä¸€ç§æ ‡è®°è¯­è¨€ï¼‰çš„é€’å½’ç¼©å†™ã€‚åœ¨å¼€å‘çš„è¿™ç§è¯­è¨€æ—¶ï¼ŒYAMLçš„æ„æ€å…¶å®æ˜¯ï¼šâ€Yet Another Markup Languageâ€ï¼ˆä»æ˜¯ä¸€ç§æ ‡è®°è¯­è¨€ï¼‰ï¼Œä½†ä¸ºäº†å¼ºè°ƒè¿™ç§è¯­è¨€ä»¥æ•°æ®ä¸ºä¸­å¿ƒï¼Œè€Œä¸æ˜¯ä»¥æ ‡è®°è¯­è¨€ä¸ºé‡ç‚¹ï¼Œè€Œç”¨åå‘ç¼©ç•¥è¯­é‡å‘½åã€‚YAML (wikipedia.org)\nYAMLç‰¹ç‚¹æ˜¯ä½¿ç”¨ç©ºæ ¼æ¥è¡¨è¾¾å±‚æ¬¡ç»“æ„ï¼Œç‰¹åˆ«é€‚åˆç”¨æ¥è¡¨è¾¾æˆ–ç¼–è¾‘æ•°æ®ç»“æ„ã€å„ç§é…ç½®æ–‡ä»¶ï¼Œå…¶æ–‡ä»¶ä¸€èˆ¬ä»¥ .yaml ä¸ºåç¼€ã€‚\nåŸºæœ¬è¯­æ³•\nä»¥ k: v çš„å½¢å¼æ¥è¡¨ç¤ºé”®å€¼å¯¹çš„å…³ç³»\n\nå†’å·åé¢å¿…é¡»æœ‰ä¸€ä¸ªç©ºæ ¼\n\n\nåªæ”¯æŒå•è¡Œæ³¨é‡Šï¼Œæ³¨é‡Šç¬¦å·ï¼š# \n\nå¤§å°å†™æ•æ„Ÿ\n\né€šè¿‡ç¼©è¿›æ¥è¡¨ç¤ºå±‚çº§å…³ç³»\n\nç¼©æ’ä¸­ç©ºæ ¼çš„æ•°ç›®ä¸é‡è¦ï¼Œåªè¦ç›¸åŒé˜¶å±‚çš„å…ƒç´ å·¦ä¾§å¯¹é½å°±å¯ä»¥äº†\nç¼©è¿›åªèƒ½ä½¿ç”¨ç©ºæ ¼ï¼Œä¸èƒ½ä½¿ç”¨ tab ç¼©è¿›\n\n\nå­—ç¬¦ä¸²å¯ä»¥ä¸ç”¨åŒå¼•å·\n\nä¸€ä¸ªæ–‡ä»¶ä¸­å¯ä»¥åŒ…å«å¤šä¸ªæ–‡ä»¶çš„å†…å®¹\n\nç”¨--- å³ä¸‰ä¸ªç ´æŠ˜å·è¡¨ç¤ºä¸€ä»½å†…å®¹çš„å¼€å§‹\nç”¨...å³ä¸‰ä¸ªå°æ•°ç‚¹è¡¨ç¤ºä¸€ä»½å†…å®¹çš„ç»“æŸï¼ˆéå¿…éœ€ï¼‰\n\n\n\næ•°æ®ç»“æ„ä¸ç±»å‹å¯¹è±¡ä»¥é”®å€¼å¯¹ key: value å½¢å¼ç»„ç»‡æ•°æ®\n1. ä½¿ç”¨**å†’å·+ç©ºæ ¼**æ¥åˆ†å¼€é”®ä¸å€¼\n1. æ”¯æŒå¤šå±‚åµŒå¥—ï¼ˆ**ç”¨ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»**ï¼‰\n\nmodel:  base_learning_rate: 4.5e-6  target: ldm.models.autoencoder.AutoencoderKL  params:    monitor: &quot;val/rec_loss&quot;    embed_dim: 64    lossconfig:      target: ldm.modules.losses.LPIPSWithDiscriminator      params:        disc_start: 50001        kl_weight: 0.000001        disc_weight: 0.5\n\n\næ”¯æŒæµå¼é£æ ¼ï¼ˆFlow styleï¼‰çš„è¯­æ³•ï¼šç”¨èŠ±æ‹¬å·åŒ…è£¹ï¼Œç”¨é€—å·åŠ ç©ºæ ¼åˆ†éš”\n\nkey: &#123;child-key1: value1, child-key2: value2 &#125;\n\n\n\næ•°ç»„\nä¸€ç»„ä»¥åŒºå—æ ¼å¼ï¼ˆâ€œç ´æŠ˜å·+ç©ºæ ¼â€ï¼‰å¼€å¤´çš„æ•°æ®ç»„æˆä¸€ä¸ªæ•°ç»„\n\nunet_config:  target: ldm.modules.diffusionmodules.openaimodel.UNetModel  params:    image_size: 64    in_channels: 3    out_channels: 3    model_channels: 224    attention_resolutions:    - 8    - 4    - 2    num_res_blocks: 2    channel_mult:    - 1    - 2    - 3    - 4    num_head_channels: 32\n\n\nä¹Ÿæ”¯æŒå†…è”æ ¼å¼æ¥è¡¨è¾¾ï¼ˆç”¨æ–¹æ‹¬å·åŒ…è£¹ï¼Œé€—å·åŠ ç©ºæ ¼åˆ†éš”ï¼‰\n\nddconfig:  double_z: True  z_channels: 64  resolution: 256  in_channels: 3  out_ch: 3  ch: 128  ch_mult: [1, 1, 2, 2, 4, 4]    num_res_blocks: 2  attn_resolutions: [16, 8]  dropout: 0.0\n\n\næ”¯æŒå¤šç»´æ•°ç»„ï¼ˆç”¨ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»ï¼‰\n\nvalues:  - - 1    - 2  - - 3    - 4# ç­‰ä»·ï¼š    values: [[1, 2], [3, 4]]\n\n\n\nå­—ç¬¦ä¸²\nå­—ç¬¦ä¸²ä¸€èˆ¬ä¸éœ€è¦ç”¨å¼•å·åŒ…è£¹\nå­—ç¬¦ä¸²æ¢è¡Œè§†ä¸ºä¸€ä¸ªç©ºæ ¼\nå•å¼•å·å¯ä»¥å±è”½è½¬ä¹‰\nå­—ç¬¦ä¸²ä¸­éœ€è¦ä½¿ç”¨è½¬ä¹‰å­—ç¬¦\\å°±å¿…é¡»ä½¿ç”¨åŒå¼•å·åŒ…è£¹\n\nstrings:  - Hello world # ä¸ç”¨å¼•å·åŒ…è£¹  - Hello     world # æ¢è¡Œè§†ä¸ºä¸€ä¸ªç©ºæ ¼  - &#x27;å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º&#x27;  # å•å¼•å·å¯ä»¥å±è”½è½¬ä¹‰  - &quot;å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º&quot;  # åŒå¼•å·ä½¿ç”¨è½¬ç§»ç¬¦å·# ç»“æœï¼š- Hello world- Hello world- å­—ç¬¦ä¸²\\næ¢è¡Œ\\næ¼”ç¤º- &#x27;å­—ç¬¦ä¸²  æ¢è¡Œ  æ¼”ç¤º&#x27;\n\n\nä¿ç•™æ¢è¡Œï¼šä½¿ç”¨ç«–çº¿ç¬¦â€œ | â€æ¥è¡¨ç¤ºè¯¥è¯­æ³•ï¼Œæ¯è¡Œçš„ç¼©è¿›å’Œè¡Œå°¾ç©ºç™½éƒ½ä¼šè¢«å»æ‰ï¼Œè€Œé¢å¤–çš„ç¼©è¿›ä¼šè¢«ä¿ç•™\n\nlines: |  æˆ‘æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ    æˆ‘æ˜¯å´å½¦ç¥–      æˆ‘æ˜¯ç¬¬å››è¡Œ  æˆ‘æ˜¯ç¬¬äº”è¡Œ  # ç»“æœ&quot;æˆ‘æ˜¯ç¬¬ä¸€è¡Œ\\næˆ‘æ˜¯ç¬¬äºŒè¡Œ\\n  æˆ‘æ˜¯å´å½¦ç¥–\\n    æˆ‘æ˜¯ç¬¬å››è¡Œ\\næˆ‘æ˜¯ç¬¬äº”è¡Œ\\n&quot;\n\n\næŠ˜å æ¢è¡Œï¼šä½¿ç”¨å³å°–æ‹¬å·â€œ &gt; â€æ¥è¡¨ç¤ºè¯¥è¯­æ³•ï¼Œåªæœ‰ç©ºç™½è¡Œæ‰ä¼šè¢«è¯†åˆ«ä¸ºæ¢è¡Œï¼ŒåŸæ¥çš„æ¢è¡Œç¬¦éƒ½ä¼šè¢«è½¬æ¢æˆç©ºæ ¼\n\nlines: &gt;  æˆ‘æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä»æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘ä¾æ—§æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ  è¿™ä¹ˆå·§æˆ‘ä¹Ÿæ˜¯ç¬¬äºŒè¡Œ# ç»“æœlines2: &#x27;æˆ‘æ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä¹Ÿæ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä»æ˜¯ç¬¬ä¸€è¡Œ æˆ‘ä¾æ—§æ˜¯ç¬¬ä¸€è¡Œ  æˆ‘æ˜¯ç¬¬äºŒè¡Œ è¿™ä¹ˆå·§æˆ‘ä¹Ÿæ˜¯ç¬¬äºŒè¡Œ  &#x27;\n\n\n\nå¸ƒå°”å€¼\nâ€œtrueâ€ã€â€œTrueâ€ã€â€œTRUEâ€ã€â€œyesâ€ã€â€œYesâ€å’Œâ€œYESâ€çš†ä¸ºçœŸ\nâ€œfalseâ€ã€â€œFalseâ€ã€â€œFALSEâ€ã€â€œnoâ€ã€â€œNoâ€å’Œâ€œNOâ€çš†ä¸ºå‡\n\næ•´æ•°\næ”¯æŒäºŒè¿›åˆ¶è¡¨ç¤º\n\nint:  - 666  - 0001_0000# ç»“æœint:- 666- 4096\n\n\n\næµ®ç‚¹æ•°\næ”¯æŒç§‘å­¦è®¡æ•°æ³•\n\nfloat:  - 3.14  - 6.8523015e+5 # ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•# ç»“æœfloat:- 3.14- 685230.15\n\nç©º Nullnullã€Nullã€~ å’Œä¸æŒ‡å®šå€¼éƒ½è¡¨ç¤ºç©º\nnulls:  - null  - Null  - ~  -# ç»“æœnulls:- null- null- null- null\n\n\n\nå¼ºåˆ¶ç±»å‹è½¬æ¢åŒæ„Ÿå¹å·+ç›®æ ‡ç±»å‹æ¥å¼ºåˆ¶è½¬æ¢ç±»å‹\na: !!float &#x27;666&#x27; # !! ä¸ºä¸¥æ ¼ç±»å‹æ ‡ç­¾b: !!int &#x27;666&#x27;   # å­—ç¬¦ä¸²è½¬ä¸ºæ•´å‹c: !!str 666     # æ•´æ•°è½¬ä¸ºå­—ç¬¦ä¸²d: !!str 666.66  # æµ®ç‚¹æ•°è½¬ä¸ºå­—ç¬¦ä¸²e: !!str true    # å¸ƒå°”å€¼è½¬ä¸ºå­—ç¬¦ä¸²f: !!bool &#x27;yes&#x27;  # å­—ç¬¦ä¸²è½¬ä¸ºå¸ƒå°”å€¼# ç»“æœa: 666.0b: 666c: &#x27;666&#x27;d: &#x27;666.66&#x27;e: &#x27;true&#x27;f: true\n\n\n\næ•°æ®å¤ç”¨ä¸åˆå¹¶æ•°æ®å¤ç”¨åœ¨keyçš„å†’å·åï¼Œä½¿ç”¨é”šç‚¹ç¬¦å·&amp;è®¾å®šé”šç‚¹ï¼Œä½¿ç”¨å¼•ç”¨ç¬¦å·*å¼•ç”¨é”šç‚¹\nmodel: &amp;all_parm  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params: &amp;model_parm    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model: *all_parmnew_params: *model_parm# ç»“æœnew_model:  base_learning_rate: 2.0e-06  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_params:  linear_start: 0.0015  linear_end: 0.0195  num_timesteps_cond: 1  log_every_t: 200  timesteps: 1000  first_stage_key: image  image_size: 64  channels: 3  monitor: val/loss_simple_ema\n\n\n\næ•°æ®åˆå¹¶åˆå¹¶æ ‡ç­¾ç¬¦å·â€œ&lt;&lt;â€é…åˆé”šç‚¹ç¬¦å·å’Œå¼•ç”¨ç¬¦å·ä½¿ç”¨å¯ä»¥ä¸ä»»æ„æ•°æ®è¿›è¡Œåˆå¹¶ï¼Œå¯ä»¥è§†ä¸ºé¢å‘å¯¹è±¡ä¸­çš„ç»§æ‰¿\nmodel_location: &amp;loc  target: ldm.models.diffusion.ddpm.LatentDiffusionmodel_params: &amp;params  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_emanew_model:  base_learning_rate: 2.0e-06  &lt;&lt;: *loc  &lt;&lt;: *params  # ç»“æœnew_model:  target: ldm.models.diffusion.ddpm.LatentDiffusion  params:    linear_start: 0.0015    linear_end: 0.0195    num_timesteps_cond: 1    log_every_t: 200    timesteps: 1000    first_stage_key: image    image_size: 64    channels: 3    monitor: val/loss_simple_ema  base_learning_rate: 2.0e-06\n\n\n\nå‚è€ƒä¸€æ–‡çœ‹æ‡‚ YAML - çŸ¥ä¹ (zhihu.com)\n","categories":["Tech"],"tags":["python"]},{"title":"Hello World","url":"/2024/02/02/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"condaå¸¸ç”¨å‘½ä»¤","url":"/2021/11/25/conda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/","content":"condaå¸¸ç”¨å‘½ä»¤åœ¨windowsçš„cmdä¸‹ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤è¿›å…¥condaï¼š\nactivate\n\nç¯å¢ƒç®¡ç†åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼šconda create -n [env_name] python=[X.X]\n\n\nenv_nameï¼šè¦åˆ›å»ºçš„ç¯å¢ƒçš„åå­—\nX.Xï¼šè¦åˆ›å»ºçš„ç¯å¢ƒçš„pythonçš„ç‰ˆæœ¬ï¼Œå¦‚3.7\n\næ¿€æ´»è™šæ‹Ÿç¯å¢ƒconda activate [env_name]\n\nåœç”¨å½“å‰ç¯å¢ƒconda deactivate\n\næŸ¥çœ‹å½“å‰ç¯å¢ƒçš„pythonç‰ˆæœ¬python --version\n\næŸ¥çœ‹æ‰€æœ‰å­˜åœ¨çš„è™šæ‹Ÿç¯å¢ƒconda info -econda env list\n\nåˆ é™¤è™šæ‹Ÿç¯å¢ƒï¼šconda remove -n [env_name] --all\n\né‡å‘½åç¯å¢ƒ\ncondaæ²¡æœ‰ç›´æ¥é‡å‘½åç¯å¢ƒçš„åŠŸèƒ½ï¼Œä½†å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤å®Œæˆï¼š\nå…‹éš†è¦é‡å‘½åçš„ç¯å¢ƒ\nå°†åŸç¯å¢ƒåˆ é™¤\n\n\n\nconda create --name [newname] --clone [oldname]conda remove --name [oldname] --all\n\n\n\nåŒ…ç®¡ç†å®‰è£…åŒ…conda install [pac_name]=[åŒ…çš„ç‰ˆæœ¬å·]\n\næŸ¥çœ‹å·²ç»å®‰è£…çš„åŒ…\næŸ¥çœ‹å½“å‰ç¯å¢ƒï¼š\n\nconda listpip list\n\n\næŸ¥çœ‹æŒ‡å®šç¯å¢ƒï¼š\n\nconda list -n [env_name]\n\nåˆ é™¤åŒ…conda uninstall [pac_name]\n\næ›´æ–°æŒ‡å®šåŒ…conda update [pac_name]pip install [pac_name] -U\n\næ¸…ç†åŒ…\né€šè¿‡ä»¥ä¸‹æŒ‡ä»¤æ¥åˆ é™¤ä¸€äº›æ²¡ç”¨çš„åŒ…ï¼Œè¿™ä¸ªå‘½ä»¤ä¼šæ£€æŸ¥å“ªäº›åŒ…æ²¡æœ‰åœ¨åŒ…ç¼“å­˜ä¸­è¢«ç¡¬ä¾èµ–åˆ°å…¶ä»–åœ°æ–¹ï¼Œå¹¶åˆ é™¤å®ƒä»¬\n\nconda clean -p\n\n\nåˆ é™¤condaä¿å­˜ä¸‹æ¥çš„taråŒ…\n\nconda clean -t\n\n\nåˆ é™¤æ‰€æœ‰çš„å®‰è£…åŒ…åŠcache\n\nconda clean -y --all\n\næ›´æ–°condaconda update conda\n\nå®‰è£…requirements.txtæ–‡ä»¶å†…çš„åŒ…\né¦–å…ˆé€šè¿‡cdæŒ‡ä»¤è¿›å…¥requirements.txtæ–‡ä»¶æ‰€åœ¨è·¯å¾„ï¼Œç„¶åæ‰§è¡Œå¦‚ä¸‹æŒ‡ä»¤å³å¯\n\npip install -r requirements.txt\n\nåŒ…çš„æ•°æ®æºç®¡ç†\næ˜¾ç¤ºç›®å‰condaçš„æ•°æ®æºæœ‰å“ªäº›ï¼š\n\nconda config --show channels\n\n\næ·»åŠ æ•°æ®æºï¼š(æ¸…åæº)\n\nconda config --add https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\nåˆ é™¤æ•°æ®æº\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n\n\nå®‰è£…å‡ºç°å¼‚å¸¸ï¼šåˆ›å»ºæ–‡ä»¶ï¼šC:\\Users\\[user name]\\AppData\\Roaming\\pip\\pip.iniå†™å…¥ï¼š[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host = mirrors.aliyun.com\n\n","categories":["Tech"],"tags":["python"]},{"title":"PPTæ³¨æ„äº‹é¡¹","url":"/2023/12/25/ppt%E5%88%B6%E4%BD%9C/","content":"PPTæ³¨æ„äº‹é¡¹\næ— è¡¬çº¿å­—ä½“\nè‹±æ–‡é¦–å­—æ¯å¤§å†™å³å¯\nå¸¸è§„å†…å®¹ï¼š18-36å­—å·ï¼Œåº•éƒ¨å’Œå¼•ç”¨å­—å·&lt;12\nç©ºç™½ç®€å•èƒŒæ™¯\nå¾½æ ‡ä¸è¦åœ¨å†…å®¹é¡µå‡ºç°ï¼Œç”¨äºé¦–é¡µã€è¿‡æ¸¡é¡µã€å°¾é¡µ\né¿å…é«˜é¥±å’Œé¢œè‰²çš„æ’è‰²ï¼Œé»‘ç™½æ°¸ä¸è¿‡æ—¶\nç»™é¡µé¢ç•™ç™½ï¼šä¾§é¢ç•™å‡ºç©ºé—´ï¼Œåº•éƒ¨ä¸è¦æ”¾å¤ªå¤šå†…å®¹\næ ‡é¢˜ï¼šæ¯é¡µéƒ½è¦æœ‰æ ‡é¢˜ï¼Œä¸€ä¸ªç®€å•å¥ã€ä¸è¦è¶…è¿‡ä¸¤è¡Œ\nä¸èƒ½å‡ºç°å¤§æ®µæ–‡å­—\nå•é¡µä¸èƒ½æœ‰è¿‡å¤šå†…å®¹ï¼Œç‹¬ç«‹å†…å®¹æ”¾åœ¨ä¸åŒé¡µï¼Œé¿å…å¤±å»ç„¦ç‚¹\nåŒä¸€é¡µåˆ—è¡¨ä¸è¦è¶…è¿‡3ä¸ªæ¡ç›®\nå›¾è¡¨çš„å…¨éƒ¨è¦ç´ éƒ½è¦è§£é‡Šæ¸…æ¥š\nè€ƒè™‘ä¸åŒæ—¶é—´é™åˆ¶çš„æƒ…å†µä¸‹å†…å®¹çš„å®‰æ’\nåŠ¨ç”»ï¼šå°‘å°±æ˜¯å¤šï¼Œç®€å•ä¸ºä¸»ã€‚ç”¨æ¥è¡¨è¾¾é€’è¿›ã€æ”¾å¤§ã€è¿›ä¸€æ­¥ã€å˜åŒ–ç­‰é€»è¾‘\né¡µé¢åˆ‡æ¢ï¼šå¹³æ»‘\næ€»ç»“ï¼šå¼ºè°ƒé‡è¦å†…å®¹ï¼Œå¢åŠ é¡µé¢çš„æ€»ç»“å’Œé¡µé¢ä¹‹é—´çš„ä¸²è”è®²è§£ï¼Œè®©å¬ä¼—æ˜ç™½å½“å‰çš„æ¼”è®²å¤„äºä»€ä¹ˆé˜¶æ®µ\n\nåŸºæœ¬åŸåˆ™\nå§‹ç»ˆè€ƒè™‘å¬ä¼—å¦‚ä½•æ›´å®¹æ˜“çš„æ¥å—å†…å®¹\nä¸æ‰“ç®—èŠçš„å†…å®¹åˆ é™¤\nå¥½çš„æ¼”è®²å§‹äºä¸€ä¸ªå¥½é—®é¢˜\nä¸€é¡µä¸­ä¿æŒä¸€ä¸ªå†…å®¹\n\n","categories":["Note"]},{"title":"ã€Using-Diffusers-01ã€‘Loading & Hub","url":"/2024/04/11/Loading%20&%20Hub/","content":"[TOC]\nLoading &amp; HubOverviewDiffusers ä¸ºç”Ÿæˆä»»åŠ¡æä¾›äº†è®¸å¤š Pipelineã€æ¨¡å‹å’Œé‡‡æ ·å™¨ã€‚ä¸ºäº†ä½¿åŠ è½½è¿™äº›ç»„ä»¶å°½å¯èƒ½ç®€å•ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ–¹æ³• from_pretrained() ä» Hugging Face Hub æˆ–æœ¬åœ°è·¯å¾„åŠ è½½ä»»ä½•ç»„ä»¶ã€‚\nä» Hugging Face Hub åŠ è½½æ¨¡å‹éœ€è¦é…ç½®ä»£ç†ï¼š\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'\n\næœ¬èŠ‚çš„ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š\n\nåŠ è½½ Pipeline\nåŠ è½½ Pipeline ä¸­çš„ä¸åŒç»„ä»¶\nåŠ è½½æ£€æŸ¥ç‚¹å˜ä½“\nåŠ è½½ç¤¾åŒº Pipeline æ‰€éœ€äº†è§£çš„æ‰€æœ‰ä¿¡æ¯ã€‚\nåŠ è½½é‡‡æ ·å™¨å¹¶æ¯”è¾ƒä½¿ç”¨ä¸åŒé‡‡æ ·å™¨çš„é€Ÿåº¦å’Œè´¨é‡æƒè¡¡ã€‚\nè½¬æ¢å’ŒåŠ è½½ KerasCV æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿åœ¨ PyTorch ä¸­é€šè¿‡ Diffusers ä½¿ç”¨å®ƒä»¬ã€‚\n\nLoad pipelines, models, and schedulersDiffusionPipeline å°†æ•´ä¸ªæ‰©æ•£ç³»ç»Ÿçš„å¤æ‚ç»„æˆåŒ…è£…åˆ°ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„ API ä¸­ï¼ŒåŒæ—¶ä¿æŒè¶³å¤Ÿçš„çµæ´»æ€§ï¼Œä¾‹å¦‚å°†æ¯ä¸ªç»„ä»¶å•ç‹¬åŠ è½½æ¥ç»„è£…æ‚¨è‡ªå·±çš„æ‰©æ•£ç³»ç»Ÿã€‚\næ¨ç†æˆ–è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡éƒ½å¯ä»¥é€šè¿‡ from_pretrained() æ–¹æ³•è®¿é—®ã€‚\n\næ¥è‡ª Hub å’Œæœ¬åœ°çš„ Pipeline \nå°†ä¸åŒçš„ç»„ä»¶åŠ å…¥ Pipeline ä¸­\næ£€æŸ¥ç‚¹å˜ä½“ï¼Œä¾‹å¦‚ä¸åŒçš„æµ®ç‚¹ç±»å‹æˆ–éæŒ‡æ•°å¹³å‡ (EMA) æƒé‡ \næ¨¡å‹å’Œé‡‡æ ·å™¨\n\né€šè¿‡Diffusion PipelineåŠ è½½æ£€æŸ¥ç‚¹DiffusionPipeline ç±»æ˜¯ä» Hub åŠ è½½æœ€æ–°æ‰©æ•£æ¨¡å‹çš„æœ€ç®€å•ã€æœ€é€šç”¨çš„æ–¹æ³•ã€‚\nDiffusionPipeline.from_pretrained() æ–¹æ³•è‡ªåŠ¨ä»æ£€æŸ¥ç‚¹æ£€æµ‹æ­£ç¡®çš„Pipelineç±»ï¼Œå¹¶è¿”å›ç”¨äºæ¨ç†çš„ Pipeline å®ä¾‹ã€‚\nfrom diffusers import DiffusionPipelineimport torch# ä»HF HubåŠ è½½ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½éœ€è¦ä½¿ç”¨çš„æ–‡ä»¶ï¼ˆéœ€è¦VPNï¼Œé…ç½®ä»£ç†ï¼‰# repo_id = \"runwayml/stable-diffusion-v1-5\"# ä»æœ¬åœ°è·¯å¾„åŠ è½½checkpoint_path = r'D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0'pipe = DiffusionPipeline.from_pretrained(    checkpoint_path,            # æˆ– repo_id    use_safetensors=True,       # ä½¿ç”¨åç¼€ä¸º.safetensorsçš„æƒé‡æ–‡ä»¶    torch_dtype=torch.float16,  # åŠ è½½16ä½çš„åŠç²¾åº¦æ¨¡å‹    variant=\"fp16\"              # 16ä½æƒé‡æ–‡ä»¶ä¸­åŒ…å«'fp16'å­—æ®µ)\n\nå‡ºç°å¦‚ä¸‹ä¿¡æ¯è¡¨ç¤ºæˆåŠŸåŠ è½½å„ä¸ªç»„ä»¶ï¼š\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00&lt;00:00,  9.67it/s]\n\nè¿˜å¯ä»¥åŠ è½½å…·æœ‰ç‰¹å®š Pipeline ç±»çš„æ£€æŸ¥ç‚¹ã€‚è¦è·å¾—ç›¸åŒçš„ç»“æœï¼Œè¯·ä½¿ç”¨ StableDiffusionXLPipeline ç±»ï¼š\nfrom diffusers import StableDiffusionXLPipelineimport torch# repo_id = \"runwayml/stable-diffusion-v1-5\"checkpoint_path = r'D:\\MyCode\\Torch_Deom\\SDXL\\stable-diffusion-xl-base-1.0'pipe = StableDiffusionXLPipeline.from_pretrained(    checkpoint_path,                use_safetensors=True,           torch_dtype=torch.float16,      variant=\"fp16\"              )\n\næ£€æŸ¥ç‚¹ï¼ˆä¾‹å¦‚ runwayml/stable-diffusion-v1-5 å¯ä»¥ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæˆ–å›¾åƒåˆ°å›¾åƒç”Ÿæˆã€‚ä¸ºäº†åŒºåˆ†ç‰¹å®šçš„ä»»åŠ¡ï¼Œå¿…é¡»ç›´æ¥ä½¿ç”¨å…¶ç›¸åº”ä»»åŠ¡çš„ Pipeline ç±»æ¥åŠ è½½å®ƒï¼š\nfrom diffusers import StableDiffusionXLInpaintPipelineStableDiffusionXLImg2ImgPipelineStableDiffusionXLControlNetImg2ImgPipeline...\n\n\n\næ›¿æ¢Pipelineä¸­çš„ç»„ä»¶å¯ä»¥ä½¿ç”¨å…¶ä»–å…¼å®¹çš„ç»„ä»¶è‡ªå®šä¹‰ä»»ä½•ç®¡é“çš„é»˜è®¤ç»„ä»¶ã€‚è¿™ç§å®šåˆ¶å¾ˆé‡è¦ï¼Œå› ä¸ºï¼š\n\næ›´æ”¹é‡‡æ ·å™¨å¯¹äºæ¢ç´¢ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡éå¸¸é‡è¦ã€‚\næ¨¡å‹çš„ä¸åŒç»„ä»¶é€šå¸¸æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ï¼Œæ‚¨å¯ä»¥ç”¨æ€§èƒ½æ›´å¥½çš„ç»„ä»¶æ›¿æ¢åŸæœ‰ç»„ä»¶ã€‚\nåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé€šå¸¸åªè®­ç»ƒä¸€äº›ç»„ä»¶ï¼ˆä¾‹å¦‚ UNet æˆ–æ–‡æœ¬ç¼–ç å™¨ï¼‰ã€‚\n\nä»¥ SD1.5 ä¸ºä¾‹ï¼Œè¦æ‰¾å‡ºå“ªäº›é‡‡æ ·å™¨å…¼å®¹è‡ªå®šä¹‰ï¼Œå¯ä»¥ä½¿ç”¨ compatibles æ–¹æ³•ï¼š\nfrom diffusers import DiffusionPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16')# æŸ¥çœ‹èƒ½å¤Ÿå…¼å®¹å½“å‰pipelineçš„é‡‡æ ·å™¨print(pipeline.scheduler.compatibles)\n\nè¾“å‡ºå¯ä¾›æ›¿æ¢çš„é‡‡æ ·å™¨ç±»ç»„æˆçš„åˆ—è¡¨ï¼š\n[&lt;class 'diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_pndm.PNDMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_edm_euler.EDMEulerScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler'&gt;]\n\nä½¿ç”¨ SchedulerMixin.from_pretrained() æ–¹æ³•å°†é»˜è®¤çš„ PNDMScheduler æ›¿æ¢ä¸ºæ€§èƒ½æ›´é«˜çš„é‡‡æ ·å™¨ EulerDiscreteSchedulerã€‚\néœ€è¦ subfolder=\"scheduler\" å‚æ•°æ‰èƒ½ä» Pipeline æ£€æŸ¥ç‚¹å­˜å‚¨çš„æ­£ç¡®å­æ–‡ä»¶å¤¹åŠ è½½é‡‡æ ·å™¨é…ç½®æ–‡ä»¶ã€‚\nç„¶åï¼Œå°±å¯ä»¥å°†æ–°çš„ EulerDiscreteScheduler å®ä¾‹ä¼ é€’ç»™ DiffusionPipeline ä¸­çš„é‡‡æ ·å™¨å‚æ•°ï¼š\nfrom diffusers import DiffusionPipelinefrom diffusers import EulerDiscreteSchedulerimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'scheduler = EulerDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    scheduler=scheduler,    torch_dtype=torch.float16,    variant='fp16')print(pipeline.scheduler)\n\nå¦‚ä¸‹æ‰€ç¤ºï¼Œå½“å‰é‡‡æ ·å™¨è¢«ä¿®æ”¹ä¸º EulerDiscreteScheduler\nEulerDiscreteScheduler {  \"_class_name\": \"EulerDiscreteScheduler\",  \"_diffusers_version\": \"0.27.2\",  \"beta_end\": 0.012,  \"beta_schedule\": \"scaled_linear\",  \"beta_start\": 0.00085,  \"clip_sample\": false,  \"interpolation_type\": \"linear\",  \"num_train_timesteps\": 1000,  \"prediction_type\": \"epsilon\",  \"rescale_betas_zero_snr\": false,  \"set_alpha_to_one\": false,  \"sigma_max\": null,  \"sigma_min\": null,  \"skip_prk_steps\": true,  \"steps_offset\": 1,  \"timestep_spacing\": \"linspace\",  \"timestep_type\": \"discrete\",  \"trained_betas\": null,  \"use_karras_sigmas\": false}\n\n\n\nSafety checkeråƒ Stable Diffusion è¿™æ ·çš„æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆDiffusers æœ‰ä¸€ä¸ªå®‰å…¨æ£€æŸ¥å™¨ï¼ˆSafety checkerï¼‰æ¥æ ¹æ®å·²çŸ¥çš„ NSFWï¼ˆNot Safe For Workï¼‰ å†…å®¹æ£€æŸ¥ç”Ÿæˆçš„è¾“å‡ºã€‚\nå¦‚æœæƒ³è¦ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ï¼Œè¯·å°† None ä¼ é€’ç»™ safety_checker å‚æ•°ï¼š\npipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    scheduler=scheduler,    safety_checker = None,    torch_dtype=torch.float16,    variant='fp16')\n\nåŠ è½½ç»„ä»¶çš„æ•°é‡å˜ä¸º6ä¸ªï¼š\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00&lt;00:00, 15.77it/s]\n\n\n\nè·¨Pipelineé‡ç”¨ç»„ä»¶æ‚¨è¿˜å¯ä»¥åœ¨å¤šä¸ª Pipeline ä¸­é‡å¤ä½¿ç”¨ç›¸åŒçš„ç»„ä»¶ï¼Œä»¥é¿å…å°†æƒé‡åŠ è½½åˆ° RAM ä¸­ä¸¤æ¬¡ã€‚ä½¿ç”¨ components æ–¹æ³•ä¿å­˜ç»„ä»¶ï¼š\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'stable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(    checkpoint_path,    safety_checker=None,    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')components = stable_diffusion_txt2img.components\n\nç„¶åï¼Œæ‚¨å¯ä»¥å°†ç»„ä»¶é€šè¿‡ components å˜é‡ä¼ é€’åˆ°å¦ä¸€ä¸ª Pipeline ï¼Œè€Œæ— éœ€å°†æƒé‡é‡æ–°åŠ è½½åˆ° RAM ä¸­ï¼š\nstable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n\nå¦‚æœå¸Œæœ›æ›´çµæ´»åœ°é‡ç”¨æˆ–ç¦ç”¨å“ªäº›ç»„ä»¶ï¼Œè¿˜å¯ä»¥å°†ç»„ä»¶å•ç‹¬ä¼ é€’åˆ°æ–°çš„Pipelineã€‚\nä¾‹å¦‚ï¼Œè¦åœ¨å›¾åƒåˆ°å›¾åƒ Pipeline ä¸­é‡ç”¨æ–‡æœ¬åˆ°å›¾åƒç®¡é“ä¸­çš„ç›¸åŒç»„ä»¶ï¼ˆå®‰å…¨æ£€æŸ¥å™¨å’Œç‰¹å¾æå–å™¨é™¤å¤–ï¼‰ï¼š\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'stable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(    checkpoint_path,    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')stable_diffusion_img2img = StableDiffusionImg2ImgPipeline(    vae=stable_diffusion_txt2img.vae,    text_encoder=stable_diffusion_txt2img.text_encoder,    tokenizer=stable_diffusion_txt2img.tokenizer,    unet=stable_diffusion_txt2img.unet,    scheduler=stable_diffusion_txt2img.scheduler,    safety_checker=None,    feature_extractor=None,    requires_safety_checker=False,)\n\n\n\næ£€æŸ¥ç‚¹å˜ä½“ Checkpoint variantsæ£€æŸ¥ç‚¹å˜ä½“é€šå¸¸æ˜¯æƒé‡ä¸ºï¼š\n\nå­˜å‚¨åœ¨ä¸åŒçš„æµ®ç‚¹ç±»å‹ä¸­ï¼Œä»¥å®ç°è¾ƒä½çš„ç²¾åº¦å’Œè¾ƒä½çš„å­˜å‚¨ï¼Œä¾‹å¦‚torch.float16ï¼Œå› ä¸ºå®ƒåªéœ€è¦ä¸€åŠçš„å¸¦å®½å’Œå­˜å‚¨æ¥ä¸‹è½½ã€‚å¦‚æœæ‚¨æ­£åœ¨ç»§ç»­è®­ç»ƒæˆ–ä½¿ç”¨ CPUï¼Œåˆ™æ— æ³•ä½¿ç”¨æ­¤å˜ä½“ã€‚ \néæŒ‡æ•°å¹³å‡ (EMA) æƒé‡ï¼Œè¿™ç§æƒé‡ä¸é€‚ç”¨äºæ¨ç†ï¼Œç”¨äºç»§ç»­å¾®è°ƒæ¨¡å‹ã€‚\n\nå½“æ£€æŸ¥ç‚¹å…·æœ‰ç›¸åŒçš„æ¨¡å‹ç»“æ„ï¼Œä½†å®ƒä»¬åœ¨ä¸åŒçš„æ•°æ®é›†å’Œä¸åŒçš„è®­ç»ƒè®¾ç½®ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒä»¬åº”è¯¥å­˜å‚¨åœ¨å•ç‹¬çš„å­˜å‚¨åº“ä¸­è€Œä¸æ˜¯å˜ä½“ä¸­\nå˜ä½“ä¸åŸå§‹æ£€æŸ¥ç‚¹å®Œå…¨ç›¸åŒã€‚å®ƒä»¬å…·æœ‰å®Œå…¨ç›¸åŒçš„åºåˆ—åŒ–æ ¼å¼ï¼ˆå¦‚ Safetensorsï¼‰ã€æ¨¡å‹ç»“æ„å’Œå…·æœ‰ç›¸åŒå½¢çŠ¶å¼ é‡çš„æƒé‡ã€‚\n\n\n\ncheckpointç±»å‹\næƒé‡æ–‡ä»¶å\nåŠ è½½æƒé‡æ–‡ä»¶çš„å‚æ•°\n\n\n\noriginal\ndiffusion_pytorch_model.bin\n\n\n\nfloating pointï¼ˆfp16ï¼‰\ndiffusion_pytorch_model.fp16.bin\nvariant='fp16', torch_dtype=torch.float16\n\n\nnon-EMA\ndiffusion_pytorch_model.non_ema.bin\nvariant='non_ema'\n\n\nåŠ è½½å˜ä½“å¯¹äºåŠ è½½æ£€æŸ¥ç‚¹å˜ä½“ï¼Œæœ‰ä¸¤ä¸ªé‡è¦çš„å‚æ•°ï¼š\n\ntorch_dtypeï¼š å®šä¹‰åŠ è½½æ£€æŸ¥ç‚¹çš„æµ®ç‚¹ç²¾åº¦ã€‚\nå¦‚æœæƒ³é€šè¿‡åŠ è½½ fp16 å˜ä½“æ¥èŠ‚çœå¸¦å®½ï¼Œåˆ™åº”æŒ‡å®š torch_dtype=torch.float16 å°†æƒé‡è½¬æ¢ä¸º fp16ã€‚å¦åˆ™ï¼Œfp16 æƒé‡å°†è½¬æ¢ä¸ºé»˜è®¤çš„ fp32 ç²¾åº¦ã€‚\nè¿˜å¯ä»¥åœ¨ä¸å®šä¹‰è¯¥å‚æ•°çš„æƒ…å†µä¸‹åŠ è½½åŸå§‹æ£€æŸ¥ç‚¹ï¼Œå¹¶ä½¿ç”¨ torch_dtype=torch.float16 å°†å…¶è½¬æ¢ä¸º fp16ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¦–å…ˆä¸‹è½½é»˜è®¤çš„ fp32 æƒé‡ï¼Œç„¶ååœ¨åŠ è½½åå°†å…¶è½¬æ¢ä¸º fp16ã€‚\n\n\nvariantï¼šå®šä¹‰åº”ä»å­˜å‚¨åº“åŠ è½½å“ªäº›æ–‡ä»¶ã€‚\nå¦‚æœæƒ³åŠ è½½ non_ema å˜ä½“ï¼Œåˆ™åº”æŒ‡å®švariant=\"non_ema æ¥ä¸‹è½½ non_ema æ–‡ä»¶ã€‚\nå¦‚æœæƒ³åŠ è½½ fp16 å˜ä½“ï¼Œåˆ™åº”æŒ‡å®švariant=\"fp16 æ¥ä¸‹è½½åŠç²¾åº¦æ–‡ä»¶ã€‚\n\n\n\nä¿å­˜å˜ä½“è¦å°†æ£€æŸ¥ç‚¹ä¿å­˜ä¸ºä¸åŒæµ®ç‚¹ç±»å‹æˆ–ä½œä¸ºé EMA å˜ä½“ï¼Œè¯·ä½¿ç”¨ DiffusionPipeline.save_pretrained() æ–¹æ³•å¹¶æŒ‡å®š variant å‚æ•°ã€‚\nsave_path = './save_variant'# save as fp16 variantstable_diffusion.save_pretrained(save_path, variant=\"fp16\")# save as non-ema variantstable_diffusion.save_pretrained(save_path, variant=\"non_ema\")\n\nä¿å­˜çš„æ–‡ä»¶ç»“æ„ä¸åŸå§‹æ£€æŸ¥ç‚¹çš„ç»“æ„ä¸€è‡´\nModelsæ¨¡å‹ä» ModelMixin.from_pretrained() æ–¹æ³•åŠ è½½ã€‚\nå¯ä»¥ä½¿ç”¨ subfolder å‚æ•°ä»å­æ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œstable-diffusion-v1-5 çš„U-Net\næ¨¡å‹æƒé‡å­˜å‚¨åœ¨ unet å­æ–‡ä»¶å¤¹ä¸­ï¼š\nfrom diffusers import UNet2DConditionModelimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'model = UNet2DConditionModel.from_pretrained(    checkpoint_path,    subfolder='unet',    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')\n\næˆ–è€…ç›´æ¥ä»å­˜å‚¨åº“çš„ç›®å½•åŠ è½½ï¼š\nfrom diffusers import UNet2DModelrepo_id = \"google/ddpm-cifar10-32\"model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n\nè¿˜å¯ä»¥é€šè¿‡åœ¨ ModelMixin.from_pretrained() å’Œ ModelMixin.save_pretrained() ä¸­æŒ‡å®šå˜é‡å‚æ•°æ¥åŠ è½½å’Œä¿å­˜å˜ä½“æ¨¡å‹ï¼š\nfrom diffusers import UNet2DConditionModelmodel = UNet2DConditionModel.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",     subfolder=\"unet\",     variant=\"non_ema\",     use_safetensors=True)model.save_pretrained(    \"./local-unet\",     variant=\"non_ema\")\n\n\n\né‡‡æ ·å™¨é‡‡æ ·å™¨æ˜¯ä» SchedulerMixin.from_pretrained() æ–¹æ³•åŠ è½½çš„ï¼Œä¸æ¨¡å‹ä¸åŒï¼Œè°ƒåº¦ç¨‹åºæ²¡æœ‰å‚æ•°ï¼›å®ƒä»¬ç”±é…ç½®æ–‡ä»¶å®šä¹‰ã€‚\nåŠ è½½é‡‡æ ·å™¨ä¸ä¼šæ¶ˆè€—ä»»ä½•å¤§é‡çš„å†…å­˜ï¼Œå¹¶ä¸”ç›¸åŒçš„é…ç½®æ–‡ä»¶å¯ä»¥ç”¨äºå„ç§ä¸åŒçš„é‡‡æ ·å™¨ã€‚\nä¾‹å¦‚ï¼Œä»¥ä¸‹è°ƒåº¦ç¨‹åºä¸ StableDiffusionPipeline å…¼å®¹ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨è¿™äº›ç±»ä¸­çš„ä»»ä½•ä¸€ä¸ªä¸­åŠ è½½ç›¸åŒçš„è°ƒåº¦ç¨‹åºé…ç½®æ–‡ä»¶ï¼š\nfrom diffusers import StableDiffusionPipelinefrom diffusers import (    DDPMScheduler,    DDIMScheduler,    PNDMScheduler,    LMSDiscreteScheduler,    EulerAncestralDiscreteScheduler,    EulerDiscreteScheduler,    DPMSolverMultistepScheduler,)import torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'ddpm = DDPMScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")ddim = DDIMScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")pndm = PNDMScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")lms = LMSDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")euler_anc = EulerAncestralDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")euler = EulerDiscreteScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")dpm = DPMSolverMultistepScheduler.from_pretrained(checkpoint_path, subfolder=\"scheduler\")# replace `dpm` with any of `ddpm`, `ddim`, `pndm`, `lms`, `euler_anc`, `euler`pipeline = StableDiffusionPipeline.from_pretrained(    checkpoint_path,    scheduler=dpm,    use_safetensors=True,    torch_dtype=torch.float16,    variant='fp16')\n\n\n\nDiffusionPipelineè¯¦è§£DiffusionPipeline.from_pretrained() è´Ÿè´£ä¸¤ä»¶äº‹ï¼š\n\nä¸‹è½½æ¨ç†æ‰€éœ€çš„æœ€æ–°ç‰ˆæœ¬çš„æ–‡ä»¶å¤¹ç»“æ„å¹¶å°†å…¶ç¼“å­˜ã€‚å¦‚æœæœ¬åœ°ç¼“å­˜ä¸­æœ‰æœ€æ–°çš„æ–‡ä»¶å¤¹ç»“æ„ï¼Œåˆ™é‡ç”¨ç¼“å­˜ä¸”ä¸ä¼šé‡å¤ä¸‹è½½æ–‡ä»¶ã€‚\nå°†ç¼“å­˜çš„æƒé‡åŠ è½½åˆ°æ­£ç¡®çš„ Pipeline ç±»ä¸­ï¼ˆä» model_index.json æ–‡ä»¶ä¸­æ£€ç´¢ï¼‰å¹¶è¿”å›å®ä¾‹ã€‚\n\n Pipeline çš„åº•å±‚æ–‡ä»¶å¤¹ç»“æ„ä¸å…¶ç±»å®ä¾‹ç›´æ¥å¯¹åº”ã€‚ä¾‹å¦‚ï¼ŒStableDiffusionPipeline å¯¹åº”äº runwayml/stable-diffusion-v1-5 ä¸­çš„æ–‡ä»¶å¤¹ç»“æ„ã€‚\nfrom diffusers import DiffusionPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16')print(pipeline)\n\n\n\nä½ ä¼šçœ‹åˆ° pipeline æ˜¯ StableDiffusionPipeline çš„ä¸€ä¸ªå®ä¾‹ï¼Œå®ƒç”±ä¸ƒä¸ªç»„ä»¶ç»„æˆï¼š\n\n\n\nç»„ä»¶\nç±»\nåº“\n\n\n\nfeature_extractor\nCLIPImageProcessor\nTransformers\n\n\nsafety_checker\nCLIPImageProcessor\ndiffusers\n\n\nscheduler\nPNDMScheduler\ndiffusers\n\n\ntext_encoder\nCLIPTextModel\nTransformers\n\n\ntokenizer\nCLIPTokenizer\nTransformers\n\n\nunet\nUNet2DConditionModel\ndiffusers\n\n\nvae\nAutoencoderKL\ndiffusers\n\n\nStableDiffusionPipeline {  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.27.2\",  \"_name_or_path\": \"D:\\\\MyCode\\\\Torch_Deom\\\\sd-v1.5\\\\sd-v1.5\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"image_encoder\": [    null,    null  ],  \"requires_safety_checker\": true,  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\nå°†å®ä¾‹çš„ç»„ä»¶ä¸æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ç»“æ„è¿›è¡Œæ¯”è¾ƒï¼Œæ‚¨å°†çœ‹åˆ°å­˜å‚¨åº“ä¸­çš„æ¯ä¸ªç»„ä»¶éƒ½æœ‰ä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶å¤¹\nå¯ä»¥å°†ç®¡é“çš„æ¯ä¸ªç»„ä»¶ä½œä¸ºå±æ€§è®¿é—®ä»¥æŸ¥çœ‹å…¶é…ç½®ï¼š\npipeline.tokenizer\n\næ¯ä¸ªç®¡é“éƒ½éœ€è¦ä¸€ä¸ª model_index.json æ–‡ä»¶æ¥å‘Šè¯‰ DiffusionPipelineï¼š\n\nä» _class_name åŠ è½½å“ªä¸ªç®¡é“ç±»\nä½¿ç”¨å“ªä¸ªç‰ˆæœ¬çš„Diffusers åœ¨ _diffusers_version ä¸­åˆ›å»ºæ¨¡å‹\nå­æ–‡ä»¶å¤¹ä¸­å­˜å‚¨äº†å“ªä¸ªåº“ä¸­çš„å“ªäº›ç»„ä»¶\n\n{  \"_class_name\": \"StableDiffusionPipeline\",  \"_diffusers_version\": \"0.6.0\",  \"feature_extractor\": [    \"transformers\",    \"CLIPImageProcessor\"  ],  \"safety_checker\": [    \"stable_diffusion\",    \"StableDiffusionSafetyChecker\"  ],  \"scheduler\": [    \"diffusers\",    \"PNDMScheduler\"  ],  \"text_encoder\": [    \"transformers\",    \"CLIPTextModel\"  ],  \"tokenizer\": [    \"transformers\",    \"CLIPTokenizer\"  ],  \"unet\": [    \"diffusers\",    \"UNet2DConditionModel\"  ],  \"vae\": [    \"diffusers\",    \"AutoencoderKL\"  ]}\n\n\n\n\nLoad and compare different schedulersé‡‡æ ·å™¨å®šä¹‰æ•´ä¸ªå»å™ªè¿‡ç¨‹ï¼š\n\nå»å™ªæ­¥æ•°\néšæœºæ€§è¿˜æ˜¯ç¡®å®šæ€§\nä½¿ç”¨ä»€ä¹ˆç®—æ³•\n\né€šå¸¸éœ€è¦åœ¨å»å™ªé€Ÿåº¦å’Œå»å™ªè´¨é‡ä¹‹é—´åšå‡ºæƒè¡¡ã€‚å®šé‡æµ‹é‡å“ªä¸ªé‡‡æ ·å™¨æœ€é€‚åˆç»™å®šçš„æ‰©æ•£Pipelineæ˜¯æå…¶å›°éš¾çš„ï¼Œå› æ­¤é€šå¸¸å»ºè®®ç®€å•åœ°å°è¯•å“ªä¸ªæœ€é€‚åˆã€‚\nè®¿é—®é‡‡æ ·å™¨ä»¥stable diffusion v1.5 ä¸ºä¾‹ï¼ŒåŠ è½½æ£€æŸ¥ç‚¹å¹¶è®¿é—®è°ƒåº¦å™¨\nfrom diffusers import DiffusionPipelineimport torchcheckpoint_path = r'D:\\MyCode\\Torch_Deom\\sd-v1.5\\sd-v1.5'pipeline = DiffusionPipeline.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16')print(pipeline.scheduler)\n\nè¾“å‡ºä¸ºï¼š\nPNDMScheduler {  \"_class_name\": \"PNDMScheduler\",  \"_diffusers_version\": \"0.27.2\",  \"beta_end\": 0.012,  \"beta_schedule\": \"scaled_linear\",  \"beta_start\": 0.00085,  \"clip_sample\": false,  \"num_train_timesteps\": 1000,  \"prediction_type\": \"epsilon\",  \"set_alpha_to_one\": false,  \"skip_prk_steps\": true,  \"steps_offset\": 1,  \"timestep_spacing\": \"leading\",  \"trained_betas\": null}\n\nå¯ä»¥çœ‹åˆ°é‡‡æ ·å™¨çš„ç±»ä¸º PNDMSchedulerã€‚ç°åœ¨ä¸å…¶ä»–é‡‡æ ·å™¨è¿›è¡Œæ¯”è¾ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª promptï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæµ‹è¯•æ‰€æœ‰ä¸åŒçš„é‡‡æ ·å™¨ï¼š\nprompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»éšæœºç§å­ 4 ä¸­åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œç¡®ä¿å¯ä»¥ç”Ÿæˆç±»ä¼¼çš„å›¾åƒï¼š\ngenerator = torch.Generator(device=\"cuda\").manual_seed(4)image = pipeline(    prompt,    generator=generator).images[0]save_name = 'PNDMS-50.png'save_path = './results/schedulers'if(not os.path.exists(save_path)):    os.makedirs(save_path)image.save(os.path.join(save_path, save_name))\n\n\næ›´æ”¹é‡‡æ ·å™¨é€šè¿‡ compatibles å±æ€§æŸ¥çœ‹å…¼å®¹çš„å…¶ä»–é‡‡æ ·å™¨ï¼š\nprint(pipeline.scheduler.compatibles)\n\nè¾“å‡ºä¸ºï¼š\n[&lt;class 'diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_pndm.PNDMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_edm_euler.EDMEulerScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler'&gt;, &lt;class 'diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'&gt;, &lt;class 'diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler'&gt;]\n\n\n\nè¦æ›´æ”¹ç®¡é“çš„è°ƒåº¦ç¨‹åºï¼Œå¯ä»¥ç»“åˆä½¿ç”¨æ–¹ä¾¿çš„ config å±æ€§å’Œ from_config() å‡½æ•°ã€‚\nprint(pipeline.scheduler.config)\n\nFrozenDict([    ('num_train_timesteps', 1000),     ('beta_start', 0.00085),     ('beta_end', 0.012),     ('beta_schedule', 'scaled_linear'),     ('trained_betas', None),     ('skip_prk_steps', True),     ('set_alpha_to_one', False),     ('prediction_type', 'epsilon'),     ('timestep_spacing', 'leading'),     ('steps_offset', 1),     ('_use_default_values', ['prediction_type', 'timestep_spacing']),     ('_class_name', 'PNDMScheduler'),     ('_diffusers_version', '0.27.2'),     ('clip_sample', False)])\n\nç„¶åï¼Œå¯ä»¥ä½¿ç”¨æ­¤é…ç½®å®ä¾‹åŒ–ä¸€ä¸ªä¸ Pipeline å…¼å®¹çš„ä¸åŒçš„é‡‡æ ·å™¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è°ƒåº¦ç¨‹åºæ›´æ”¹ä¸º DDIMSchedulerã€‚\nfrom diffusers import DDIMSchedulerpipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n\n\næ¯”è¾ƒé‡‡æ ·å™¨ç°åœ¨å·²ç»å‘å¸ƒäº†ä¸€äº›æ›´å¥½çš„é‡‡æ ·å™¨ï¼Œå®ƒä»¬å¯ä»¥ç”¨æ›´å°‘çš„æ­¥éª¤è¿è¡Œï¼›è®©æˆ‘ä»¬åœ¨è¿™é‡Œå¯¹å®ƒä»¬è¿›è¡Œæ¯”è¾ƒï¼š\n\nLMSDiscreteScheduleré€šå¸¸ä¼šå¸¦æ¥æ›´å¥½çš„ç»“æœ\n\nfrom diffusers import LMSDiscreteSchedulerpipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n\nLMSDiscreteScheduler 50æ­¥ï¼š\n\n\nEulerDiscreteScheduler å’Œ EulerAncestralDiscreteScheduler ä»…éœ€ 30 ä¸ªæ­¥éª¤å³å¯ç”Ÿæˆé«˜è´¨é‡ç»“æœã€‚\n\nfrom diffusers import EulerDiscreteSchedulerpipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\n1. `EulerDiscreteScheduler` 50æ­¥ï¼š\n\n\n\nEulerDiscreteScheduler 30æ­¥ï¼š\n\n\nfrom diffusers import EulerAncestralDiscreteSchedulerpipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\n 1. `EulerAncestralDiscreteScheduler` 50æ­¥ï¼š\n\n\n\nEulerAncestralDiscreteScheduler 30æ­¥ï¼š\n\n\n\nDPMSolverMultistepScheduler ç»™å‡ºäº†åˆç†çš„é€Ÿåº¦å’Œè´¨é‡çš„æƒè¡¡ï¼Œå¹¶ä¸”å¯ä»¥ç”¨æœ€å°‘ 20 ä¸ªæ­¥éª¤è¿è¡Œã€‚\n\nfrom diffusers import DPMSolverMultistepSchedulerpipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\n\nDPMSolverMultistepScheduler 50æ­¥ï¼š\n\n\n\nDPMSolverMultistepScheduler 20æ­¥ï¼š\n\n\næ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œå¤§å¤šæ•°å›¾åƒçœ‹èµ·æ¥éƒ½éå¸¸ç›¸ä¼¼ï¼Œè´¨é‡ä¹Ÿå¯ä»¥è¯´éå¸¸æ¥è¿‘ã€‚é€‰æ‹©å“ªç§é€šå¸¸å–å†³äºå…·ä½“çš„ä½¿ç”¨æƒ…å†µã€‚æœ€å¥½çš„æ–¹æ³•æ˜¯è¿è¡Œå¤šä¸ªä¸åŒçš„é‡‡æ ·å™¨æ¥æ¯”è¾ƒç»“æœã€‚\nLoad community pipelines and componentsç¤¾åŒºpipelineæ˜¯æŒ‡ä¸åŒäºå…¶è®ºæ–‡ä¸­æŒ‡å®šçš„åŸå§‹å®ç°çš„ä»»ä½• DiffusionPipeline ç±»ï¼ˆä¾‹å¦‚ï¼ŒStableDiffusionControlNetPipeline ä¸ ControlNet è®ºæ–‡ä¸­çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç›¸å¯¹åº”ï¼‰ã€‚å®ƒä»¬æä¾›äº†é¢å¤–çš„åŠŸèƒ½æˆ–æ‰©å±•äº†pipelineçš„åŸå§‹å®ç°ã€‚\n\nç¤¾åŒºpipelineç¤ºä¾‹ï¼šdiffusers/examples/community at main Â· huggingface/diffusers (github.com)\n\nè¦åœ¨ Hub ä¸ŠåŠ è½½ä»»ä½•ç¤¾åŒºç®¡é“ï¼Œè¯·åœ¨ custom_pipeline å‚æ•°ä¸­è¾“å…¥ç¤¾åŒºpipelineçš„èµ„æºåº“ idï¼Œä»¥åŠè¦åŠ è½½ç®¡é“æƒé‡å’Œç»„ä»¶çš„æ¨¡å‹èµ„æºåº“ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„ç¤ºä¾‹ä»hf-internal-testing/diffusers-dummy-pipelineä»¥åŠGoogle/DDPM-CIFAR10-32çš„ç®¡é“æƒé‡å’Œç»„ä»¶åŠ è½½äº†ä¸€ä¸ª pipelineï¼š\nfrom diffusers import DiffusionPipelinepipeline = DiffusionPipeline.from_pretrained(    \"google/ddpm-cifar10-32\",     custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\",     use_safetensors=True)\n\n\n\nåŠ è½½å®˜æ–¹ç¤¾åŒºpipelineå¯ä»¥æ··åˆåŠ è½½æ¥è‡ªå®˜æ–¹èµ„æºåº“çš„æƒé‡ï¼Œå¹¶ç›´æ¥ä¼ é€’ç®¡é“ç»„ä»¶ã€‚ä¸‹é¢çš„ç¤ºä¾‹åŠ è½½äº†ç¤¾åŒºpipeline CLIP Guided Stable Diffusion ï¼Œä½ å¯ä»¥ç›´æ¥å°† CLIP æ¨¡å‹ç»„ä»¶ä¼ é€’ç»™å®ƒï¼š\nfrom diffusers import DiffusionPipelinefrom transformers import CLIPImageProcessor, CLIPModelclip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)clip_model = CLIPModel.from_pretrained(clip_model_id)pipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",    custom_pipeline=\"clip_guided_stable_diffusion\",    clip_model=clip_model,    feature_extractor=feature_extractor,    use_safetensors=True,)\n\n\n\nä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¦‚æœæ‚¨é€šè¿‡æ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ç¤¾åŒº pipelineã€‚æ–‡ä»¶è·¯å¾„å¿…é¡»åŒ…å«ä¸€ä¸ªpipeline.pyæ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶åŒ…å«pipelineç±»ã€‚\npipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",    custom_pipeline=\"./path/to/pipeline_directory/\",    clip_model=clip_model,    feature_extractor=feature_extractor,    use_safetensors=True,)\n\nä»ç‰¹å®šç‰ˆæœ¬åŠ è½½é»˜è®¤æƒ…å†µä¸‹ï¼Œç¤¾åŒº pipeline æ˜¯ä»æœ€æ–°ç¨³å®šç‰ˆæœ¬çš„ Diffusers ä¸­åŠ è½½çš„ã€‚è¦ä»å¦ä¸€ä¸ªç‰ˆæœ¬åŠ è½½ç¤¾åŒºç®¡é“ï¼Œè¯·è®¾ç½® custom_revision å‚æ•°ã€‚\npipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",    custom_pipeline=\"clip_guided_stable_diffusion\",    custom_revision=\"main\", # æˆ–\"v0.25.0\"ç­‰ç‰¹å®šç‰ˆæœ¬    clip_model=clip_model,    feature_extractor=feature_extractor,    use_safetensors=True,)\n\n\n\nLoad safetensors\nhuggingface/safetensors: Simple, safe way to store and distribute tensors (github.com)\n\næ˜¯ä¸€ç§å®‰å…¨ã€å¿«é€Ÿçš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºå­˜å‚¨å’ŒåŠ è½½å¼ é‡ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼ŒPyTorch æ¨¡å‹æƒé‡ä¼šé€šè¿‡ Python çš„ pickle å·¥å…·ä¿å­˜åˆ° .bin æ–‡ä»¶ä¸­ã€‚ç„¶è€Œï¼Œpickle å¹¶ä¸å®‰å…¨ï¼Œæ–‡ä»¶å¯èƒ½åŒ…å«å¯è¢«æ‰§è¡Œçš„æ¶æ„ä»£ç ã€‚safetensors æ˜¯ pickle çš„å®‰å…¨æ›¿ä»£å“ï¼Œæ˜¯å…±äº«æ¨¡å‹æƒé‡çš„ç†æƒ³é€‰æ‹©ã€‚\næ¥ä¸‹æ¥ä»‹ç»å¦‚ä½•åŠ è½½ .safetensor æ–‡ä»¶ï¼Œä»¥åŠå¦‚ä½•å°†ä»¥å…¶ä»–æ ¼å¼å­˜å‚¨çš„Stable Diffusionæ¨¡å‹æƒé‡è½¬æ¢ä¸º .safetensor æ ¼å¼ã€‚å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… safetensorsï¼š\npip install safetensors\n\né»˜è®¤æƒ…å†µä¸‹ï¼ˆå¦‚runwayml/stable-diffusion-v1-5ï¼‰ï¼Œå¦‚æœæ¨¡å‹åº“ä¸­æœ‰è¿™äº› .safetensors æ–‡ä»¶ï¼ŒDiffusers ä¼šè‡ªåŠ¨ä»å®ƒä»¬çš„å­æ–‡ä»¶å¤¹ä¸­åŠ è½½è¿™äº›æ–‡ä»¶ã€‚\nè¦å®ç°æ›´æ˜ç¡®çš„æ§åˆ¶ï¼Œå¯ä»¥é€‰æ‹©è®¾ç½® use_safetensors=Trueï¼ˆå¦‚æœæœªå®‰è£… safetensorsï¼Œåˆ™ä¼šæ”¶åˆ°è¦æ±‚å®‰è£…çš„é”™è¯¯ä¿¡æ¯ï¼‰ï¼š\nfrom diffusers import DiffusionPipelinepipeline = DiffusionPipeline.from_pretrained(    \"runwayml/stable-diffusion-v1-5\",     use_safetensors=True)\n\nä¸è¿‡ï¼Œæ¨¡å‹æƒé‡å¹¶ä¸ä¸€å®šåƒä¸Šä¾‹é‚£æ ·å­˜å‚¨åœ¨å•ç‹¬çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚æœ‰æ—¶ï¼Œæ‰€æœ‰æƒé‡éƒ½å­˜å‚¨åœ¨ä¸€ä¸ª .safetensors æ–‡ä»¶ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæƒé‡æ˜¯ç¨³å®šæ‰©æ•£æƒé‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ from_single_file() æ–¹æ³•ç›´æ¥åŠ è½½è¯¥æ–‡ä»¶ï¼š\nfrom diffusers import StableDiffusionPipelinepipeline = StableDiffusionPipeline.from_single_file( \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\")\n\nè½¬æ¢ä¸ºsafetensorså¹¶é Hub ä¸Šçš„æ‰€æœ‰æƒé‡éƒ½æ˜¯ .safetensors æ ¼å¼ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä»¥ .bin æ ¼å¼å­˜å‚¨çš„æƒé‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ä½¿ç”¨ Convert Space å°†æƒé‡è½¬æ¢ä¸º .safetensors æ ¼å¼ã€‚\nConvert Spaceä¼šä¸‹è½½æƒé‡å¹¶è¿›è¡Œè½¬æ¢ï¼Œç„¶åæ‰“å¼€ä¸€ä¸ªPull Requestï¼Œå°†æ–°è½¬æ¢çš„ .safetensors æ–‡ä»¶ä¸Šä¼ åˆ°é›†çº¿å™¨ä¸Šã€‚è¿™æ ·ï¼Œå¦‚æœè…Œåˆ¶æ–‡ä»¶ä¸­åŒ…å«ä»»ä½•æ¶æ„ä»£ç ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¸Šä¼ åˆ°Hubï¼Œè€Œä¸æ˜¯ä½ çš„ç”µè„‘ï¼Œå› ä¸ºHubæœ‰ä¸€ä¸ªsecurity scanneræ¥æ£€æµ‹ä¸å®‰å…¨çš„æ–‡ä»¶ã€‚\nä¸ºä»€ä¹ˆè¦ä½¿ç”¨safetensorsï¼Ÿä½¿ç”¨ safetensors æœ‰å‡ ä¸ªåŸå› ï¼š\n\nå®‰å…¨æ˜¯ä½¿ç”¨ safetensors çš„é¦–è¦åŸå› ã€‚éšç€å¼€æºå’Œæ¨¡å‹åˆ†å‘çš„å‘å±•ï¼Œç¡®ä¿æ‚¨ä¸‹è½½çš„æ¨¡å‹æƒé‡ä¸åŒ…å«ä»»ä½•æ¶æ„ä»£ç éå¸¸é‡è¦ã€‚ç›®å‰ safetensors ä¸­æ ‡å¤´çš„ current size ä¼šé˜»æ­¢è§£æè¶…å¤§ JSON æ–‡ä»¶ã€‚\nsafetensors ä¹Ÿæ”¯æŒå»¶è¿ŸåŠ è½½ï¼ˆLazy loadingï¼‰ï¼Œè¿™åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥åªåŠ è½½éƒ¨åˆ†å¼ é‡ã€‚é€šè¿‡è¿™ç§æ ¼å¼ï¼Œåœ¨ 8 ä¸ª GPU ä¸ŠåŠ è½½ BLOOM æ¨¡å‹åªéœ€ 45 ç§’ï¼Œè€Œä½¿ç”¨æ™®é€š PyTorch æƒé‡åˆ™éœ€è¦ 10 åˆ†é’Ÿã€‚\nåˆ‡æ¢æ¨¡å‹ä¹‹é—´çš„åŠ è½½é€Ÿåº¦æ˜¯ä½¿ç”¨ safetensors çš„å¦ä¸€ä¸ªåŸå› ï¼Œå®ƒä¼šå¯¹å¼ é‡æ‰§è¡Œé›¶æ‹·è´ã€‚å¦‚æœå°†æƒé‡åŠ è½½åˆ° CPUï¼ˆé»˜è®¤æƒ…å†µï¼‰ï¼Œå®ƒçš„é€Ÿåº¦æ¯” pickle æ›´å¿«ï¼Œè€Œç›´æ¥å°†æƒé‡åŠ è½½åˆ° GPU æ—¶ï¼Œä¹ŸåŒæ ·å¿«ã€‚\n\nåŠ è½½æ•´ä¸ªpipelineæ‰€éœ€çš„æ—¶é—´ï¼š\nfrom diffusers import StableDiffusionPipelinepipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", use_safetensors=True)\"Loaded in safetensors 0:00:02.033658\"\"Loaded in PyTorch 0:00:02.663379\"\n\nä½†åŠ è½½ 500MB æ¨¡å‹æƒé‡çš„å®é™…æ—¶é—´ä»…ä¸ºï¼š\nsafetensors: 3.4873msPyTorch: 172.7537ms\n\n\n\nLoad different Stable Diffusion formatsStable Diffusion æ¨¡å‹æœ‰ä¸åŒçš„æ ¼å¼ï¼Œè¿™å–å†³äºå®ƒä»¬æ˜¯ç”¨ä»€ä¹ˆæ¡†æ¶è®­ç»ƒå’Œä¿å­˜çš„ï¼Œä»¥åŠä»å“ªé‡Œä¸‹è½½çš„ã€‚è½¬æ¢è¿™äº›æ ¼å¼ä»¥ä¾¿åœ¨ Diffusers ä¸­ä½¿ç”¨ï¼Œå¯ä»¥è®©æ‚¨ä½¿ç”¨è¯¥åº“æ”¯æŒçš„æ‰€æœ‰åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š\n\nä½¿ç”¨ä¸åŒçš„é‡‡æ ·å™¨è¿›è¡Œæ¨ç†\næ„å»ºè‡ªå®šä¹‰ç®¡é“\nä¼˜åŒ–æ¨ç†é€Ÿåº¦\n\næ¥ä¸‹æ¥ä»‹ç»å¦‚ä½•è½¬æ¢å…¶ä»–æ ¼å¼ä»¥ä¸ Diffusers å…¼å®¹ã€‚\nPyTorch .ckptåœ¨ PyTorch ä¸­ checkpointï¼ˆæˆ– .ckptï¼‰æ ¼å¼é€šå¸¸ç”¨äºä¿å­˜æ¨¡å‹ã€‚.ckpt æ–‡ä»¶åŒ…å«æ•´ä¸ªæ¨¡å‹ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨ from_single_file() æ–¹æ³•ç›´æ¥åŠ è½½å’Œä½¿ç”¨ï¼Œä½†é€šå¸¸æœ€å¥½å°† .ckpt æ–‡ä»¶è½¬æ¢ä¸º Diffusers æ¨èçš„ safetensors æ ¼å¼ã€‚\nè½¬æ¢ .ckpt æ–‡ä»¶æœ‰ä¸¤ç§æ–¹å¼ï¼š\n\nä½¿ç”¨ Space è½¬æ¢æ£€æŸ¥ç‚¹\nä½¿ç”¨è„šæœ¬è½¬æ¢ .ckpt æ–‡ä»¶\n\nç”¨Spaceè½¬æ¢\nSD To Diffusers - a Hugging Face Space by diffusers\n\nè½¬æ¢ .ckpt æ–‡ä»¶æœ€ç®€å•æ–¹ä¾¿çš„æ–¹æ³•æ˜¯ä½¿ç”¨ SD To Diffusers Spaceã€‚\nè¿™ç§æ–¹æ³•å¯¹åŸºæœ¬æ¨¡å‹å¾ˆæœ‰æ•ˆï¼Œä½†å¯¹æ›´å¤šå®šåˆ¶æ¨¡å‹å¯èƒ½ä¼šæœ‰å›°éš¾ã€‚å¦‚æœ Space è¿”å› empty pull requestæˆ–erroeï¼Œå¯ä»¥å°è¯•ç”¨è„šæœ¬è½¬æ¢ .ckpt æ–‡ä»¶ã€‚\nç”¨è„šæœ¬è½¬æ¢\ndiffusers/scripts/convert_original_stable_diffusion_to_diffusers.py at main Â· huggingface/diffusers (github.com)\n\nè¿™ç§æ–¹æ³•æ¯”ä¸Šé¢çš„Spaceæ›´å¯é ï¼Œéœ€è¦è®¾å®šå¦‚ä¸‹å‚æ•°ï¼š\n\ncheckpoint_pathï¼šè¦è½¬æ¢çš„ .ckpt æ–‡ä»¶çš„è·¯å¾„ã€‚\noriginal_config_fileï¼šå®šä¹‰åŸå§‹æ¶æ„é…ç½®çš„ YAML æ–‡ä»¶ã€‚å¦‚æœæ‰¾ä¸åˆ°æ­¤æ–‡ä»¶ï¼Œè¯·å°è¯•åœ¨æ‰¾åˆ° .ckpt æ–‡ä»¶çš„ GitHub å­˜å‚¨åº“ä¸­æœç´¢ YAML æ–‡ä»¶ã€‚\ndump_pathï¼šè½¬æ¢åæ¨¡å‹çš„ä¿å­˜è·¯å¾„\n\nA1111 LoRA filesAutomatic1111 (A1111) æ˜¯ä¸€ç§æµè¡Œçš„Stable Diffusion Web UIï¼Œæ”¯æŒCivitaiç­‰æ¨¡å‹å…±äº«å¹³å°ã€‚\nDiffusers æ”¯æŒä½¿ç”¨ load_lora_weights() å°† LoRA æ£€æŸ¥ç‚¹åŠ è½½åˆ° pipeline ä¸­ï¼š\npipeline.load_lora_weights(    \"LoRA/civitai_LoRA\",     weight_name=\"bl3uprint.safetensors\")save_name = 'lora-blueprint.png'prompt = \"bl3uprint, a highly detailed blueprint of the empire state building, explaining how to build all parts, many txt, blueprint grid backdrop\"negative_prompt = \"lowres, cropped, worst quality, low quality, normal quality, artifacts, signature, watermark, username, blurry, more than one bridge, bad architecture\"image = pipeline(    prompt=prompt,    negative_prompt=negative_prompt,    generator=torch.manual_seed(0),).images[0]save_path = './results/LoRA'image.save(os.path.join(save_path, save_name))\n\n\nLoad adaptersæœ‰å¤šç§è®­ç»ƒæŠ€æœ¯å¯ç”¨äºä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆç‰¹å®šä¸»é¢˜çš„å›¾åƒæˆ–æŸäº›é£æ ¼çš„å›¾åƒã€‚æ¯ç§è®­ç»ƒæ–¹æ³•éƒ½ä¾èµ–ä¸åŒç±»å‹çš„ Adapterã€‚æœ‰äº›é€‚é…å™¨ä¼šç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ï¼Œè€Œå…¶ä»–é€‚é…å™¨åˆ™åªä¿®æ”¹è¾ƒå°çš„åµŒå…¥æˆ–æƒé‡é›†ã€‚è¿™æ„å‘³ç€æ¯ä¸ªé€‚é…å™¨çš„åŠ è½½è¿‡ç¨‹ä¹Ÿä¸å°½ç›¸åŒã€‚\næ¥ä¸‹æ¥ä»‹ç»å¦‚ä½•åŠ è½½ DreamBoothã€Textual inversionå’Œ LoRA æƒé‡ã€‚\nDreamBooth\nsd-dreambooth-library (Stable Diffusion Dreambooth Concepts Library) (huggingface.co)\n\nDreamBooth åªéœ€åœ¨å‡ å¼ åŒä¸€ä¸»é¢˜æˆ–é£æ ¼çš„å›¾åƒä¸Šå¾®è°ƒæ•´ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œå°±èƒ½ç”Ÿæˆæ–°çš„åŒä¸€ä¸»é¢˜æˆ–é£æ ¼çš„å›¾åƒã€‚\nè¿™ç§æ–¹æ³•çš„åŸç†æ˜¯åœ¨æç¤ºä¸­ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„æç¤ºè¯ï¼Œè®©æ¨¡å‹å­¦ä¼šå°†è¯¥å•è¯ä¸ä¸»é¢˜å›¾åƒè”ç³»èµ·æ¥ã€‚åœ¨æ‰€æœ‰è®­ç»ƒæ–¹æ³•ä¸­ï¼ŒDreamBooth äº§ç”Ÿçš„æ–‡ä»¶å°ºå¯¸æœ€å¤§ï¼ˆé€šå¸¸ä¸ºå‡  GBï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„æ£€æŸ¥ç‚¹ã€‚\nåŠ è½½æ–¹å¼å’Œæ™®é€špipelineç±»ä¼¼ï¼š\nè®©æˆ‘ä»¬åŠ è½½ sd-dreambooth-library/pikachu æ£€æŸ¥ç‚¹ï¼Œå®ƒåªå¯¹ 9 å¼ ç”± çš®å¡ä¸˜ çš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è®©å®ƒæ­£å¸¸å·¥ä½œï¼Œéœ€è¦åœ¨æç¤ºä¸­åŠ å…¥ pikachu è¿™ä¸ªç‰¹æ®Šçš„å•è¯æ¥è§¦å‘æ£€æŸ¥ç‚¹ï¼š\nfrom diffusers import AutoPipelineForText2Imageimport torchcheckpoint_path = 'DreamBooth/pikachu'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16).to(\"cuda\")prompt = \"A pikachu is playing the guitar, masterpiece\"image = pipeline(prompt).images[0]image.save('./result/pikachu_DB.png')\n\n\n\n\n\nTextual inversion\nsd-concepts-library (Stable Diffusion concepts library) (huggingface.co)\n\næ–‡æœ¬åè½¬ä¸ DreamBooth éå¸¸ç›¸ä¼¼ï¼Œä»å‡ å¼ å›¾ç‰‡ä¸­å­¦ä¹ ç”Ÿæˆç‰¹å®šçš„æ¦‚å¿µï¼ˆé£æ ¼ã€å¯¹è±¡ï¼‰ã€‚è¿™ç§æ–¹æ³•çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡è®­ç»ƒå’Œå¯»æ‰¾æ–°çš„ embeddingï¼Œç”¨æç¤ºä¸­çš„ä¸€ä¸ªç‰¹æ®Šå•è¯æ¥ä»£è¡¨ä½ æä¾›çš„å›¾ç‰‡ã€‚å› æ­¤ï¼Œæ‰©æ•£æ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ï¼Œè®­ç»ƒè¿‡ç¨‹äº§ç”Ÿçš„æ–‡ä»¶ä¹Ÿç›¸å¯¹è¾ƒå°ï¼ˆå‡  KBï¼‰ã€‚\nç”±äºæ–‡æœ¬åè½¬ä¼šåˆ›å»º embeddingï¼Œå› æ­¤å®ƒä¸èƒ½åƒ DreamBooth é‚£æ ·å•ç‹¬ä½¿ç”¨ï¼Œè€Œéœ€è¦ä¾é™„äºå¦ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¿™é‡Œä½¿ç”¨ SD1.5ï¼š\nfrom diffusers import AutoPipelineForText2Imageimport torch, oscheckpoint_path = 'sd-v1.5/sd-v1.5'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16',    use_safetensors=True,    safety_checker=None,).to(\"cuda\")\n\nç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ load_textual_inversion() æ–¹æ³•åŠ è½½æ–‡æœ¬åè½¬embeddingæ¥ç”Ÿæˆå›¾åƒã€‚è®©æˆ‘ä»¬åŠ è½½ sd-concepts-library/gta5-artwork åµŒå…¥ï¼Œç„¶ååœ¨æç¤ºä¸­åŒ…å«ç‰¹æ®Šå­— &lt;gta5-artwork&gt; æ¥è§¦å‘å®ƒï¼š\nseed = 0save_name = (f'gta5-artwork-TI-{seed}.png')pipeline.load_textual_inversion(\"Textual_inversion/gta5-artwork\")prompt = \"a man is playing the guitar, masterpiece, best quality, &lt;gta5-artwork&gt; style\"image = pipeline(    prompt,    num_inference_steps=25,    generator=torch.manual_seed(seed)).images[0]save_path = './results/Textual_inversion'if not os.path.exists(save_path):    os.makedirs(save_path)image.save(os.path.join(save_path, save_name))\n\næ–‡æœ¬åè½¬ä¹Ÿå¯ä»¥é’ˆå¯¹ä¸éœ€è¦çš„äº‹ç‰©è¿›è¡Œè®­ç»ƒï¼Œä»¥åˆ›å»ºnegative embeddingsï¼Œé˜»æ­¢æ¨¡å‹ç”Ÿæˆå«æœ‰æˆ‘ä»¬ä¸å¸Œæœ›å‡ºç°å†…å®¹çš„å›¾åƒï¼Œå¦‚å¸¦æœ‰æ¨¡ç³Šçš„å›¾åƒæˆ–äººç‰©æ‰‹æŒ‡æ•°é‡å¼‚å¸¸çš„å›¾åƒã€‚\nè¿™æ˜¯å¿«é€Ÿæ”¹è¿› prompt çš„ç®€å•æ–¹æ³•ã€‚å¯ä»¥ä½¿ç”¨ load_textual_inversion() åŠ è½½embeddingï¼Œè®¾å®šè¿™ä¸¤ä¸ªå‚æ•°ï¼š\n\nweight_nameï¼šè¦åŠ è½½çš„æƒé‡æ–‡ä»¶\ntokenï¼šåœ¨ prompt ä¸­çš„è§¦å‘è¯\n\nseed = 0save_name = (f'gta5-artwork-TI-EN-{seed}.png')pipeline.load_textual_inversion(\"Textual_inversion/gta5-artwork\")# è½½å…¥ EasyNegativepipeline.load_textual_inversion(    \"Textual_inversion\",    weight_name=\"EasyNegative.safetensors\",    token=\"EasyNegative\")prompt = \"A man is playing the guitar, masterpiece, best quality, &lt;gta5-artwork&gt; style, EasyNegative\"negative_prompt = \"EasyNegative\"image = pipeline(    prompt,    negative_prompt=negative_prompt,    num_inference_steps=25,    generator=torch.manual_seed(seed),).images[0]\n\n\n\nLoRALow-Rank Adaptationï¼ˆLoRAï¼‰æ˜¯ä¸€ç§æµè¡Œçš„å¾®è°ƒæŠ€æœ¯ï¼Œå› ä¸ºå®ƒé€Ÿåº¦å¿«ï¼Œç”Ÿæˆçš„æ–‡ä»¶è¾ƒå°ï¼ˆå‡ ç™¾ MBï¼‰ã€‚ä»å‡ å¼ å›¾åƒä¸­å­¦ä¹ æ–°çš„é£æ ¼ã€‚\nå®ƒçš„å·¥ä½œåŸç†æ˜¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ’å…¥æ–°çš„æƒé‡ï¼Œç„¶ååªè®­ç»ƒæ–°æƒé‡è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™ä½¿å¾— LoRA çš„è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œä¹Ÿæ›´æ˜“äºå­˜å‚¨ã€‚\n\nLoRA æ˜¯ä¸€ç§éå¸¸é€šç”¨çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯ä»¥ä¸å…¶ä»–è®­ç»ƒæ–¹æ³•ä¸€èµ·ä½¿ç”¨ã€‚\nä½¿ç”¨ DreamBooth å’Œ LoRA è®­ç»ƒæ¨¡å‹å°±å¾ˆå¸¸è§ã€‚\n\nLoRA ä¹Ÿéœ€è¦ä¾é™„äºä¸€ä¸ªæ‰©æ•£æ¨¡å‹ä½¿ç”¨ï¼Œè¿™é‡Œä½¿ç”¨SDXLï¼š\nfrom diffusers import AutoPipelineForText2Imageimport torch, oscheckpoint_path = 'SDXL/stable-diffusion-xl-base-1.0'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16',    use_safetensors=True,    safety_checker=None,).to(\"cuda\")\n\nç„¶åä½¿ç”¨ load_lora_weights() æ–¹æ³•åŠ è½½ Pixel Art LoRA | Civitai æƒé‡å¹¶æŒ‡å®šæƒé‡æ–‡ä»¶åï¼Œå‘½åè¿™ä¸ª LoRA ä¸º pixelï¼š\npipeline.load_lora_weights(    \"LoRA/civitai_LoRA\",    weight_name=\"pixel_stormXL.safetensors\",    adapter_name=\"pixel\")seed = 0save_name = (f'pixel-LoRA-{seed}.png')prompt = \"A man is playing the guitar, masterpiece, best quality\"image = pipeline(    prompt,    num_inference_steps=30,    generator=torch.manual_seed(seed),).images[0]save_path = './results/LoRA'if not os.path.exists(save_path):    os.makedirs(save_path)image.save(os.path.join(save_path, save_name))\n\nload_lora_weights() æ–¹æ³•å¯å°† LoRA æƒé‡åŠ è½½åˆ° UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­ã€‚é€šè¿‡ get_list_adapters() æ–¹æ³•æŸ¥çœ‹åŠ è½½ä½ç½®ï¼š\nlist_adapters_component_wise = pipeline.get_list_adapters()print(list_adapters_component_wise){'text_encoder': ['pixel'], 'text_encoder_2': ['pixel'], 'unet': ['pixel']}\n\nè¿™æ˜¯åŠ è½½ LoRA çš„é¦–é€‰æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥å¤„ç†ä»¥ä¸‹æƒ…å†µ\n\nLoRA æƒé‡åœ¨ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­æ²¡æœ‰å•ç‹¬çš„æ ‡è¯†ç¬¦\nLoRA æƒé‡åœ¨ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­æœ‰å•ç‹¬çš„æ ‡è¯†ç¬¦\n\nä½†å¦‚æœä½ åªéœ€è¦å°†LoRAæƒé‡åŠ è½½åˆ°UNetä¸­ï¼Œé‚£ä¹ˆä½ å¯ä»¥ä½¿ç”¨ load_attn_procs()æ–¹æ³•ã€‚\npipeline.unet.load_attn_procs(\t\"LoRA/civitai_LoRA\",    weight_name=\"pixel_stormXL.safetensors\",)\n\nè¦å¸è½½ LoRA æƒé‡ï¼Œè¯·ä½¿ç”¨ unload_lora_weights() æ–¹æ³•ä¸¢å¼ƒ LoRA æƒé‡å¹¶å°†æ¨¡å‹æ¢å¤ä¸ºå…¶åŸå§‹æƒé‡ï¼š\npipeline.unload_lora_weights()\n\n\n\nIP-Adapter\nh94/IP-Adapter Â· Hugging Face\n\nIP-Adapter æ˜¯ä¸€ç§è½»é‡çº§Adapterï¼Œå¯ä¸ºä»»ä½•æ‰©æ•£æ¨¡å‹æä¾›å›¾åƒ promptã€‚è¯¥é€‚é…å™¨é€šè¿‡è§£è€¦å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„äº¤å‰æ³¨æ„å±‚æ¥å·¥ä½œã€‚æ‰€æœ‰å…¶ä»–æ¨¡å‹ç»„ä»¶éƒ½è¢«å†»ç»“ï¼Œåªæœ‰ UNet ä¸­çš„embedded image featuresä¼šè¢«è®­ç»ƒã€‚å› æ­¤ï¼ŒIP-Adapter æ–‡ä»¶å¤§å°é€šå¸¸åªæœ‰çº¦ 100MBã€‚\né¦–å…ˆï¼ŒåŠ è½½ Stable Diffusion æ£€æŸ¥ç‚¹ã€‚\nimport osos.environ['HTTP_PROXY'] = 'http://127.0.0.1:33210'os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:33210'from diffusers import AutoPipelineForText2Image, DPMSolverMultistepSchedulerimport torchfrom diffusers.utils import load_imagecheckpoint_path = 'sd-v1.5/sd-v1.5'pipeline = AutoPipelineForText2Image.from_pretrained(    checkpoint_path,    torch_dtype=torch.float16,    variant='fp16',    use_safetensors=True,    safety_checker=None,).to(\"cuda\")\n\nç„¶åä½¿ç”¨ load_ip_adapter() æ–¹æ³•å°†å…¶æ·»åŠ åˆ° pipeline ä¸­ã€‚\npipeline.load_ip_adapter(\"LoRA/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.safetensors\")\n\nåŠ è½½åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¸¦æœ‰å›¾åƒå’Œæ–‡æœ¬æç¤ºçš„ç®¡é“æ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")generator = torch.Generator(device=\"cpu\").manual_seed(33)images = pipeline(    prompt='best quality, high quality, wearing sunglasses',    ip_adapter_image=image,    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",    num_inference_steps=50,    generator=generator,).images[0]images.save('./results/IP-Adapter.png')\n\n\nIP-Adapter PlusIP-Adapter ä¾é å›¾åƒç¼–ç å™¨ç”Ÿæˆå›¾åƒç‰¹å¾ã€‚å¦‚æœ IP-Adapter èµ„æºåº“åŒ…å« image_encoder å­æ–‡ä»¶å¤¹ï¼Œå›¾åƒç¼–ç å™¨å°±ä¼šè‡ªåŠ¨åŠ è½½å¹¶æ³¨å†Œåˆ° pipeline ä¸­ã€‚å¦åˆ™ï¼Œå°±éœ€è¦ä½¿ç”¨ CLIPVisionModelWithProjection æ¨¡å‹æ˜¾å¼åŠ è½½å›¾åƒç¼–ç å™¨ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ pipelineã€‚\nfrom transformers import CLIPVisionModelWithProjection# æ˜¾å¼åŠ è½½å›¾åƒç¼–ç å™¨image_encoder = CLIPVisionModelWithProjection.from_pretrained(    \"h94/IP-Adapter\",    subfolder=\"models/image_encoder\",    torch_dtype=torch.float16)pipeline = AutoPipelineForText2Image.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\",    image_encoder=image_encoder,    torch_dtype=torch.float16).to(\"cuda\")pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\")\n\n\n\nPush files to the Hubå¾…å®š\n","categories":["Research"],"tags":["diffusers","AIGC"]},{"title":"AccelerateåŸºæœ¬ä½¿ç”¨æ–¹æ³•","url":"/2024/04/19/Accelerate%20base/","content":"Accelerate-NoteAccelerate æ˜¯ä¸€ä¸ªç”± Hugging Face å¼€å‘çš„ Python åº“ï¼Œå®ƒå…è®¸å¼€å‘è€…å°†ç›¸åŒçš„ PyTorch ä»£ç è¿è¡Œåœ¨ä»»ä½•åˆ†å¸ƒå¼é…ç½®ä¸Šï¼Œåªéœ€æ·»åŠ å››è¡Œä»£ç å³å¯ã€‚è¿™ä¸ªåº“ç®€åŒ–äº†åœ¨å¤šç§ç¯å¢ƒï¼ˆåŒ…æ‹¬å•æœºã€å¤š GPUã€TPU å’Œå„ç§åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼‰ä¸­è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒçš„è¿‡ç¨‹ã€‚å®ƒåŸºäº torch_xla å’Œ torch.distributed æ„å»ºï¼Œå¯ä»¥è½»æ¾åœ°å°†ç°æœ‰ä»£ç åº“è½¬æ¢ä¸ºä½¿ç”¨ DeepSpeed è¿›è¡Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œå¤„ç†ï¼Œå¹¶è‡ªåŠ¨æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚\nå°†Accelerateæ·»åŠ åˆ°PyTorchä»£ç ä¸­Accelerate æä¾›äº†ä¸€ç§å‹å¥½çš„æ–¹å¼æ¥é€‚åº”å„ç±»åˆ†å¸ƒå¼æ¡†æ¶ï¼Œè€Œæ— éœ€å­¦ä¹ æ¯ä¸ªæ¡†æ¶çš„å…·ä½“ç»†èŠ‚ã€‚\næ¥ä¸‹æ¥å°†ä»ä¸€ä¸ªåŸºæœ¬çš„ PyTorch è®­ç»ƒå¾ªç¯å¼€å§‹ï¼ˆå‡è®¾æ¨¡å‹å’Œä¼˜åŒ–å™¨ç­‰æ‰€æœ‰è®­ç»ƒå¯¹è±¡éƒ½å·²è®¾ç½®å®Œæ¯•ï¼‰ï¼Œç„¶åé€æ­¥å°† Accelerate é›†æˆåˆ°å…¶ä¸­ã€‚\n# åˆå§‹åŒ–model = ...                # torch.nn.Moduletraining_dataloader = ...  # torch.utils.data.DataLoaderoptimizer = ...            # torch.optimscheduler = ...            # torch.optim.lr_schedulerdevice = \"cuda\"model.to(device)for batch in training_dataloader:    optimizer.zero_grad()    inputs, targets = batch    inputs = inputs.to(device)    targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)    loss.backward()    optimizer.step()    scheduler.step()\n\nå®Œæ•´å®ä¾‹å¦‚ä¸‹ï¼š\nimport torchfrom torch import nnfrom torch import optimimport torch.nn.functional as Ffrom torch.optim import lr_schedulerfrom torch.utils.data import DataLoaderfrom torch.utils.data import random_splitimport torchvision.datasets as datasetsimport torchvision.transforms as transformsfrom torch.utils.tensorboard import SummaryWriterfrom tqdm import tqdmdef check_accuracy(loader, model):    num_correct = 0    num_samples = 0    model.eval()    with torch.no_grad():        # Loop through the data        for x, y in loader:            # Move data to device            x = x.to(device=device)            y = y.to(device=device)            # Get to correct shape            x = x.reshape(x.shape[0], -1)            # Forward pass            scores = model(x)            _, predictions = scores.max(1)            # Check how many we got correct            num_correct += (predictions == y).sum()            # Keep track of number of samples            num_samples += predictions.size(0)    model.train()    return num_correct / num_samples# è¶…å‚æ•°è®¾ç½®input_size = 784num_classes = 10learning_rate = 0.001batch_size = 64num_epochs = 10sw = SummaryWriter('./mnist-logs')# 0. è®¾å¤‡è®¾ç½®device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 1. æ¨¡å‹class NN(nn.Module):    def __init__(self, input_size, num_classes):        super().__init__()        self.fc1 = nn.Linear(input_size, 50)        self.fc2 = nn.Linear(50, num_classes)    def forward(self, x):        x = F.relu(self.fc1(x))        x = self.fc2(x)        return xmodel = NN(input_size=input_size, num_classes=num_classes).to(device)# 2. æ•°æ®åŠ è½½å™¨entire_dataset = datasets.MNIST(    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)train_ds, val_ds = random_split(entire_dataset, [50000, 10000])test_ds = datasets.MNIST(    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)val_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)# 3.ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨optimizer = optim.Adam(    model.parameters(),    lr=learning_rate)scheduler = lr_scheduler.CosineAnnealingWarmRestarts(    optimizer,    T_0=2,    eta_min=1e-6)# æŸå¤±å‡½æ•°loss_func = nn.CrossEntropyLoss()# è®­ç»ƒå¾ªç¯len_per_epoch = len(train_loader)global_step = 0for epoch in range(num_epochs):    for i, (data, targets) in enumerate(tqdm(train_loader)):        # æ¸…ç©ºæ¢¯åº¦        optimizer.zero_grad()        # æ•°æ®ç§»è‡³device        data = data.to(device=device)        targets = targets.to(device=device)        # è®¡ç®—æŸå¤±        data = data.reshape(data.shape[0], -1)        scores = model(data)        loss = loss_func(scores, targets)        # Backward        loss.backward()        # Gradient descent or adam step        optimizer.step()        scheduler.step(epoch + i / len_per_epoch)        global_step += 1        sw.add_scalar('lr', scheduler.get_last_lr()[0], global_step=global_step)    val_accuracy = check_accuracy(val_loader, model) * 100    sw.add_scalar('val_accuracy', val_accuracy, global_step=epoch)# Check accuracy on training &amp; test to see how good our modelmodel.to(device)print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")print(f\"Accuracy on validation set: {check_accuracy(val_loader, model)*100:.2f}\")print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n\nAcceleratorAccelerator æ˜¯ç”¨äºè°ƒæ•´ä»£ç ä»¥ä¸ Accelerate é…åˆä½¿ç”¨çš„ä¸»ç±»ã€‚å®ƒäº†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„åˆ†å¸ƒå¼è®¾ç½®ï¼Œå¦‚ä¸åŒè¿›ç¨‹çš„æ•°é‡å’Œç¡¬ä»¶ç±»å‹ã€‚ä½¿æ‚¨çš„ PyTorch ä»£ç èƒ½å¤Ÿåœ¨ä»»ä½•åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­å·¥ä½œï¼Œå¹¶ç®¡ç†å’Œæ‰§è¡Œè·¨è®¾å¤‡è¿›ç¨‹ã€‚\nä»å¯¼å…¥å’Œåˆ›å»º Accelerator å®ä¾‹å¼€å§‹ï¼š\nfrom accelerate import Acceleratoraccelerator = Accelerator()\n\nAccelerator çŸ¥é“å°† PyTorch å¯¹è±¡ç§»åŠ¨åˆ°å“ªä¸ªè®¾å¤‡ä¸Šï¼Œå› æ­¤å»ºè®®è®© Accelerate è®¾ç½® device å‚æ•°ï¼š\n- device = \"cuda\"+ device = accelerator.device  model.to(device)\n\nå‡†å¤‡ PyTorch å¯¹è±¡æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä¸ºåˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡å¥½ PyTorch å¯¹è±¡ï¼ˆæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€æ•°æ®åŠ è½½å™¨ç­‰ï¼‰ã€‚\nprepare() æ–¹æ³•ä¼šæ ¹æ®è®­ç»ƒè®¾ç½®å°†æ¨¡å‹æ”¾å…¥é€‚å½“çš„å®¹å™¨ï¼ˆå¦‚å•GPUæˆ–å¤šGPUï¼‰ä¸­ï¼Œè°ƒæ•´ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ä»¥ä½¿ç”¨ Accelerate çš„ AcceleratedOptimizer å’Œ AcceleratedSchedulerï¼Œå¹¶åˆ›å»ºå¯è·¨è¿›ç¨‹åˆ†ç‰‡çš„æ–°æ•°æ®åŠ è½½å™¨ã€‚\n\nprepare() æ–¹æ³•ä»…å‡†å¤‡ä» PyTorch ç±»ç»§æ‰¿çš„å¯¹è±¡ï¼Œä¾‹å¦‚ torch.optim.Optimizerã€‚\n\nPyTorch å¯¹è±¡çš„è¿”å›é¡ºåºä¸è¾“å…¥é¡ºåºç›¸åŒï¼š\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(    model, optimizer, training_dataloader, scheduler)\n\n\n\nè®­ç»ƒå¾ªç¯æœ€åï¼Œåˆ é™¤è®­ç»ƒå¾ªç¯ä¸­å¯¹è¾“å…¥å’Œç›®æ ‡æ•°æ®çš„ to(device) è°ƒç”¨ï¼Œå› ä¸º Accelerate çš„ DataLoader ç±»ä¼šè‡ªåŠ¨å°†å®ƒä»¬æ”¾ç½®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šã€‚\næ‚¨è¿˜åº”è¯¥å°†é€šå¸¸çš„loss.backward()ä¼ é€’æ›¿æ¢ä¸º Accelerate çš„backward()æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šä¸ºæ‚¨ç¼©æ”¾æ¢¯åº¦ï¼Œå¹¶æ ¹æ®æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®ï¼ˆä¾‹å¦‚ï¼ŒDeepSpeedæˆ–Megatronï¼‰ä½¿ç”¨é€‚å½“çš„backward()æ–¹æ³•ã€‚\n-   inputs = inputs.to(device)-   targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)-   loss.backward()+   accelerator.backward(loss)\n\n\n\næ€»ç»“å°†æ‰€æœ‰å†…å®¹æ”¾åœ¨ä¸€èµ·ï¼Œæ‚¨çš„ Accelerate è®­ç»ƒå¾ªç¯ç°åœ¨åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼\nfrom accelerate import Acceleratoraccelerator = Accelerator()# åˆå§‹åŒ–model = ...                # torch.nn.Moduletraining_dataloader = ...  # torch.utils.data.DataLoaderoptimizer = ...            # torch.optimscheduler = ...            # torch.optim.lr_schedulerdevice = accelerator.devicemodel, optimizer, training_dataloader, scheduler = accelerator.prepare(    model, optimizer, training_dataloader, scheduler)for batch in training_dataloader:    optimizer.zero_grad()        inputs, targets = batch        outputs = model(inputs)    loss = loss_function(outputs, targets)        accelerator.backward(loss)        optimizer.step()    scheduler.step()\n\n\n\nTraining featuresAccelerate æä¾›äº†é¢å¤–çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ¢¯åº¦ç´¯ç§¯ (gradient accumulation)ã€æ¢¯åº¦è£å‰ª (gradient clipping)ã€æ··åˆç²¾åº¦è®­ç»ƒ (mixed precision training)ç­‰ï¼Œæ‚¨å¯ä»¥å°†å…¶æ·»åŠ åˆ°è„šæœ¬ä¸­ä»¥æ”¹è¿›è®­ç»ƒã€‚\næ¢¯åº¦ç´¯ç§¯æ¢¯åº¦ç´¯ç§¯ä½¿æ‚¨èƒ½å¤Ÿåœ¨æ›´æ–°æƒé‡ä¹‹å‰é€šè¿‡ç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦æ¥è·å–æ›´å¤§çš„ç­‰æ•ˆ batch_sizeã€‚è¿™å¯¹äºè§£å†³æ˜¾å­˜å¯¹ batch_size çš„é™åˆ¶å¾ˆæœ‰ç”¨ã€‚\nè¦åœ¨ Accelerate ä¸­å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨åŠ é€Ÿå™¨ç±»ä¸­æŒ‡å®š gradient_accumulation_steps å‚æ•°ï¼Œå¹¶åœ¨è„šæœ¬ä¸­æ·»åŠ  accumulate() ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š\n+ accelerator = Accelerator(gradient_accumulation_steps=2)  model, optimizer, training_dataloader = accelerator.prepare(      model, optimizer, training_dataloader  )  for input, label in training_dataloader:+     with accelerator.accumulate(model):          predictions = model(input)          loss = loss_function(predictions, label)          accelerator.backward(loss)          optimizer.step()          scheduler.step()          optimizer.zero_grad()\n\næ¢¯åº¦è£å‰ªæ¢¯åº¦è£å‰ªæ˜¯ä¸€ç§é˜²æ­¢â€œæ¢¯åº¦çˆ†ç‚¸â€çš„æŠ€æœ¯ï¼ŒAccelerate æä¾›ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š\n\nclip_grad_value_ï¼šå°†å¯è¿­ä»£å‚æ•°çš„æ¢¯åº¦è£å‰ªä¸ºæŒ‡å®šå€¼ã€‚ æ¢¯åº¦å°±åœ°ä¿®æ”¹ï¼ˆin-placeï¼‰ã€‚\nparametresï¼šå¯è¿­ä»£çš„å¼ é‡æˆ–å•ä¸ªå¼ é‡ï¼Œå…¶æ¢¯åº¦å°†å½’ä¸€åŒ–\nclip_valueï¼šæ¢¯åº¦çš„é˜ˆå€¼ã€‚æ¢¯åº¦è¢«é™åˆ¶åœ¨èŒƒå›´å†…\n\n\nclip_grad_norm_ï¼šèŒƒæ•°æ˜¯å¯¹æ‰€æœ‰æ¢¯åº¦ä¸€èµ·è®¡ç®—çš„ã€‚æ¢¯åº¦å°±åœ°ä¿®æ”¹ã€‚\nparametersï¼šå¯è¿­ä»£çš„å¼ é‡æˆ–å•ä¸ªå¼ é‡ï¼Œå…¶æ¢¯åº¦å°†å½’ä¸€åŒ–\nmax_normï¼šæ¢¯åº¦çš„æœ€å¤§èŒƒæ•°\nnorm_typeï¼šfloatï¼Œé»˜è®¤ä¸º2.0ï¼Œç”¨çš„ p-èŒƒæ•°çš„ç±»å‹ã€‚infè¡¨ç¤ºæ— ç©·èŒƒæ•°ã€‚\n\n\n\nfrom accelerate import Acceleratoraccelerator = Accelerator(gradient_accumulation_steps=2)dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)for input, target in dataloader:     optimizer.zero_grad()     output = model(input)     loss = loss_func(output, target)     accelerator.backward(loss)     if accelerator.sync_gradients:     # äºŒè€…å–å…¶ä¸€ï¼š    \taccelerator.clip_grad_value_(model.parameters(), clip_value)        accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)             optimizer.step()\n\n\n\næ··åˆç²¾åº¦è®­ç»ƒæ··åˆç²¾åº¦é€šè¿‡ä½¿ç”¨ fp16ï¼ˆåŠç²¾åº¦ï¼‰ç­‰è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚è¦æƒ³ä½¿ç”¨ Accelerate è·å¾—æœ€ä½³æ€§èƒ½ï¼Œåº”åœ¨æ¨¡å‹å†…éƒ¨è®¡ç®—æŸå¤±ï¼ˆå¦‚åœ¨ Transformers æ¨¡å‹ä¸­ï¼‰ï¼Œå› ä¸ºæ¨¡å‹å¤–éƒ¨çš„è®¡ç®—æ˜¯ä»¥å…¨ç²¾åº¦è¿›è¡Œçš„ã€‚\nè®¾ç½®è¦åœ¨ accelerater ä¸­ä½¿ç”¨çš„æ··åˆç²¾åº¦ç±»å‹ï¼Œç„¶åä½¿ç”¨ autocast() ä¸Šä¸‹æ–‡ç®¡ç†å™¨å°†å€¼è‡ªåŠ¨è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹ã€‚\nfrom accelerate import Accelerator+ accelerator = Accelerator(mixed_precision=\"fp16\")+ with accelerator.autocast():      loss = complex_loss_function(outputs, target):\n\n\n\nä¿å­˜å’ŒåŠ è½½è®­ç»ƒå®Œæˆåï¼ŒåŠ é€Ÿè¿˜å¯ä»¥ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Œæˆ–è€…æ‚¨è¿˜å¯ä»¥ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆoptimizer stateï¼‰ï¼Œè¿™å¯¹äºæ¢å¤è®­ç»ƒå¾ˆæœ‰ç”¨ã€‚\næ¨¡å‹æ‰€æœ‰è¿‡ç¨‹å®Œæˆåï¼Œåœ¨ä¿å­˜æ¨¡å‹å‰ä½¿ç”¨ unwrap_model() æ–¹æ³•è§£é™¤æ¨¡å‹çš„å°è£…ï¼Œå› ä¸ºè®­ç»ƒå¼€å§‹å‰æ‰§è¡Œçš„ prepare() æ–¹æ³•å°†æ¨¡å‹å°è£…åˆ°äº†é€‚åˆçš„åˆ†å¸ƒå¼è®­ç»ƒæ¥å£ä¸­ã€‚å¦‚æœä¸è§£é™¤å¯¹æ¨¡å‹çš„å°è£…ï¼Œä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸çš„åŒæ—¶ä¹Ÿä¼šä¿å­˜å¤§æ¨¡å‹ä¸­ä»»ä½•æ½œåœ¨çš„é¢å¤–å±‚ï¼Œè¿™æ ·å°±æ— æ³•å°†æƒé‡åŠ è½½å›åŸºç¡€æ¨¡å‹ä¸­ã€‚\nä½¿ç”¨ save_model() æ–¹æ³•æ¥è§£åŒ…å¹¶ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚æ­¤æ–¹æ³•è¿˜å¯ä»¥å°†æ¨¡å‹ä¿å­˜åˆ°åˆ‡ç‰‡æ£€æŸ¥ç‚¹ sharded checkpoints æˆ–safetensorsæ ¼å¼ä¸­ã€‚\naccelerator.wait_for_everyone()accelerator.save_model(model, save_directory)\n\n\nå¯¹äº Transformers åº“ä¸­çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ save_pretrained æ–¹æ³•ä¿å­˜æ¨¡å‹ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ from_pretrainedæ–¹æ³•é‡æ–°åŠ è½½ã€‚\nfrom transformers import AutoModelunwrapped_model = accelerator.unwrap_model(model)unwrapped_model.save_pretrained(    \"path/to/my_model_directory\",    is_main_process=accelerator.is_main_process,    save_function=accelerator.save,)model = AutoModel.from_pretrained(\"path/to/my_model_directory\")\n\nè¦åŠ è½½æƒé‡ï¼Œè¯·åœ¨åŠ è½½æƒé‡ä¹‹å‰å…ˆä½¿ç”¨ unwrap_model() æ–¹æ³•è§£åŒ…æ¨¡å‹ã€‚æ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½æ˜¯å¯¹å¼ é‡çš„å¼•ç”¨ï¼Œå› æ­¤è¿™ä¼šå°†æ‚¨çš„æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚\nunwrapped_model = accelerator.unwrap_model(model)path_to_checkpoint = os.path.join(save_directory,\"pytorch_model.bin\")unwrapped_model.load_state_dict(torch.load(path_to_checkpoint))\n\nåˆ‡ç‰‡æ£€æŸ¥ç‚¹è®¾ç½® safe_serialization=True å°†æ¨¡å‹ä¿å­˜ä¸º safetensor æ ¼å¼ã€‚\naccelerator.wait_for_everyone()accelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)\n\nè¦åŠ è½½åˆ†ç‰‡æ£€æŸ¥ç‚¹æˆ– safetensor æ ¼å¼çš„æ£€æŸ¥ç‚¹ï¼Œè¯·ä½¿ç”¨ load_checkpoint_in_model() æ–¹æ³•ã€‚æ­¤æ–¹æ³•å…è®¸æ‚¨å°†æ£€æŸ¥ç‚¹åŠ è½½åˆ°ç‰¹å®šè®¾å¤‡ä¸Šã€‚\nload_checkpoint_in_model(unwrapped_model, save_directory, device_map={\"\":device})\n\n\n\nçŠ¶æ€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›ä¿å­˜æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€éšæœºç”Ÿæˆå™¨ä»¥åŠå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å½“å‰çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨åŒä¸€ä¸ªè„šæœ¬ä¸­æ¢å¤å®ƒä»¬ã€‚ä½ åº”è¯¥åœ¨è„šæœ¬ä¸­æ·»åŠ  save_state() å’Œ load_state() æ–¹æ³•æ¥ä¿å­˜å’ŒåŠ è½½çŠ¶æ€ã€‚\nä»»ä½•å…¶ä»–éœ€è¦å­˜å‚¨çš„æœ‰çŠ¶æ€é¡¹ç›®éƒ½åº”ä½¿ç”¨ register_for_checkpointing() æ–¹æ³•æ³¨å†Œï¼Œä»¥ä¾¿ä¿å­˜å’ŒåŠ è½½ã€‚ä¼ é€’ç»™æ­¤æ–¹æ³•çš„æ¯ä¸ªè¦å­˜å‚¨çš„å¯¹è±¡éƒ½å¿…é¡»å…·æœ‰ load_state_dict å’Œ state_dict å‡½æ•°ã€‚\næ‰§è¡Œè¿›ç¨‹åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ—¶ï¼Œç®¡ç†è·¨ GPU æ‰§è¡Œæµç¨‹çš„æ–¹å¼å’Œæ—¶é—´éå¸¸é‡è¦ã€‚æœ‰äº›è¿›ç¨‹æ¯”å…¶ä»–è¿›ç¨‹å®Œæˆå¾—æ›´å¿«ï¼Œæœ‰äº›è¿›ç¨‹åœ¨å…¶ä»–è¿›ç¨‹å°šæœªå®Œæˆæ—¶å°±ä¸åº”å¼€å§‹ã€‚Accelerate æä¾›äº†ç”¨äºåè°ƒè¿›ç¨‹æ‰§è¡Œæ—¶é—´çš„å·¥å…·ï¼Œä»¥ç¡®ä¿æ‰€æœ‰è®¾å¤‡ä¸Šçš„ä¸€åˆ‡ä¿æŒåŒæ­¥ã€‚\nåœ¨ä¸€ä¸ªè¿›ç¨‹ä¸Šæ‰§è¡ŒæŸäº›ä»£ç åªéœ€åœ¨ç‰¹å®šæœºå™¨ä¸Šè¿è¡Œä¸€æ¬¡ï¼Œå¦‚æ‰“å°æ—¥å¿—è¯­å¥æˆ–åªåœ¨æœ¬åœ°ä¸»è¿›ç¨‹ä¸Šæ˜¾ç¤ºä¸€ä¸ªè¿›åº¦æ¡ã€‚\nstatementåº”ä½¿ç”¨ accelerator.is_local_main_process æ¥æŒ‡ç¤ºåªåº”æ‰§è¡Œä¸€æ¬¡çš„ä»£ç ã€‚\n\naccelerator.is_local_main_process ï¼š\n\nç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æœ¬åœ°èŠ‚ç‚¹ï¼ˆæœåŠ¡å™¨ï¼‰ä¸Šçš„ä¸»è¿›ç¨‹ï¼Œ\nå¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡åœ¨å¤šå°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œæ¯å°æœåŠ¡å™¨éƒ½æœ‰ä¸€ä¸ªä¸»è¿›ç¨‹ã€‚is_local_main_process() å¦‚æœè¿”å› Trueï¼Œè¡¨ç¤ºå½“å‰è¿›ç¨‹æ˜¯æœ¬åœ°èŠ‚ç‚¹ä¸Šçš„ä¸»è¿›ç¨‹ã€‚\né€šå¸¸ï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°èŠ‚ç‚¹çš„ä¸»è¿›ç¨‹ä¸Šæ‰§è¡Œä¸€äº›åªéœ€æ‰§è¡Œä¸€æ¬¡çš„æ“ä½œï¼Œä¾‹å¦‚åˆå§‹åŒ–æ•°æ®ã€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç­‰ã€‚\n\n\nfrom tqdm.auto import tqdmprogress_bar = tqdm(    range(args.max_train_steps),     disable=not accelerator.is_local_main_process)\n\nè¿˜å¯ä»¥ä½¿ç”¨ accelerator.is_local_main_process åŒ…è£…è¯­å¥ã€‚\nif accelerator.is_local_main_process:    print(\"Accelerate is the best\")\n\n\n\nè¿˜å¯ä»¥æŒ‡ç¤º Accelerate åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­éƒ½è¦æ‰§è¡Œä¸€æ¬¡çš„ä»£ç ï¼Œè€Œä¸ç®¡æœ‰å¤šå°‘å°æœºå™¨ã€‚å¦‚æœæ‚¨è¦å°†æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚\n\naccelerator.is_main_processï¼š\n\nè¿™ä¸ªå‡½æ•°ç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æ•´ä¸ªè®­ç»ƒä»»åŠ¡ä¸­çš„ä¸»è¿›ç¨‹ã€‚\nä¸»è¿›ç¨‹é€šå¸¸è´Ÿè´£ä¸€äº›å…¨å±€æ“ä½œï¼Œä¾‹å¦‚æ¨¡å‹ä¿å­˜ã€æ—¥å¿—è®°å½•ç­‰ã€‚å› æ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨ is_main_process() æ¥ç¡®ä¿è¿™äº›æ“ä½œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡ã€‚\nå¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡åœ¨å¤šå°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œis_main_process() å°†è¿”å› Trueï¼Œåªæœ‰ä¸€ä¸ªæœåŠ¡å™¨ä¸Šçš„ä¸»è¿›ç¨‹ä¼šæ»¡è¶³è¿™ä¸ªæ¡ä»¶ã€‚\n\n\nif accelerator.is_main_process:    repo.push_to_hub()\n\n\n\nfunctionå¯¹äºåªåº”æ‰§è¡Œä¸€æ¬¡çš„å‡½æ•°ï¼Œè¯·ä½¿ç”¨ on_local_main_process è£…é¥°å™¨ã€‚\n@accelerator.on_local_main_processdef do_my_thing():    \"Something done once per server\"    do_thing_once_per_server()\n\nå¯¹äºåªåº”åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡çš„å‡½æ•°ï¼Œè¯·ä½¿ç”¨ on_main_process è£…é¥°å™¨ã€‚\n@accelerator.on_main_processdef do_my_thing():    \"Something done once per server\"    do_thing_once()\n\n\n\n\n\nåœ¨ç‰¹å®šè¿›ç¨‹ä¸Šæ‰§è¡ŒAccelerate è¿˜å¯ä»¥æ‰§è¡Œåªåº”åœ¨ç‰¹å®šè¿›ç¨‹æˆ–æœ¬åœ°è¿›ç¨‹ç´¢å¼•ä¸Šæ‰§è¡Œçš„å‡½æ•°ã€‚\nä½¿ç”¨ on_process() è£…é¥°å™¨æŒ‡å®šè¦æ‰§è¡Œå‡½æ•°çš„è¿›ç¨‹ç´¢å¼•ã€‚\n@accelerator.on_process(process_index=0)def do_my_thing():    \"Something done on process index 0\"    do_thing_on_index_zero()\n\nä½¿ç”¨ on_local_process() è£…é¥°å™¨æŒ‡å®šè¦æ‰§è¡Œå‡½æ•°çš„æœ¬åœ°è¿›ç¨‹ç´¢å¼•ã€‚\n@accelerator.on_local_process(local_process_idx=0)def do_my_thing():    \"Something done on process index 0 on each server\"    do_thing_on_index_zero_on_each_server()\n\n\n\næ¨è¿Ÿæ‰§è¡Œå½“åŒæ—¶åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œè„šæœ¬æ—¶ï¼ŒæŸäº›ä»£ç çš„æ‰§è¡Œé€Ÿåº¦å¯èƒ½ä¼šæ¯”å…¶ä»–ä»£ç å¿«ã€‚åœ¨æ‰§è¡Œä¸‹ä¸€ç»„æŒ‡ä»¤ä¹‹å‰ï¼Œæ‚¨å¯èƒ½éœ€è¦ç­‰å¾…æ‰€æœ‰è¿›ç¨‹éƒ½è¾¾åˆ°ä¸€å®šç¨‹åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¡®ä¿æ¯ä¸ªè¿›ç¨‹éƒ½å®Œæˆè®­ç»ƒä¹‹å‰ï¼Œæ‚¨ä¸åº”è¯¥ä¿å­˜æ¨¡å‹ã€‚\nä¸ºæ­¤ï¼Œè¯·åœ¨ä»£ç ä¸­æ·»åŠ  wait_for_everyone()ã€‚è¿™ä¼šé˜»æ­¢æ‰€æœ‰å…ˆå®Œæˆè®­ç»ƒçš„è¿›ç¨‹ç»§ç»­è®­ç»ƒï¼Œç›´åˆ°æ‰€æœ‰å‰©ä½™è¿›ç¨‹éƒ½è¾¾åˆ°ç›¸åŒç‚¹ï¼ˆå¦‚æœåœ¨å•ä¸ª GPU æˆ– CPU ä¸Šè¿è¡Œï¼Œåˆ™æ²¡æœ‰å½±å“ï¼‰ã€‚\naccelerator.wait_for_everyone()\n\n\n\nå¯åŠ¨Accelerateè„šæœ¬é¦–å…ˆï¼Œå°†è®­ç»ƒä»£ç é‡å†™ä¸ºå‡½æ•°ï¼Œå¹¶ä½¿å…¶å¯ä½œä¸ºè„šæœ¬è°ƒç”¨ã€‚ä¾‹å¦‚ï¼š\n  from accelerate import Accelerator  + def main():      accelerator = Accelerator()      model, optimizer, training_dataloader, scheduler = accelerator.prepare(          model, optimizer, training_dataloader, scheduler      )      for batch in training_dataloader:          optimizer.zero_grad()          inputs, targets = batch          outputs = model(inputs)          loss = loss_function(outputs, targets)          accelerator.backward(loss)          optimizer.step()          scheduler.step()+ if __name__ == \"__main__\":+     main()\n\n\n\nç„¶åï¼Œåœ¨å‘½ä»¤è¡Œä½¿ç”¨ accelerate config æ¥é…ç½®accelerateçš„è¿è¡Œç¯å¢ƒ\n\nThe Command Line (huggingface.co)\n\nä½¿ç”¨ accelerate launchAccelerate æœ‰ä¸€ä¸ªç‰¹æ®Šçš„ CLI å‘½ä»¤ï¼Œå¯å¸®åŠ©æ‚¨é€šè¿‡åŠ é€Ÿå¯åŠ¨åœ¨ç³»ç»Ÿä¸­å¯åŠ¨ä»£ç ã€‚è¯¥å‘½ä»¤åŒ…å«åœ¨å„ç§å¹³å°ä¸Šå¯åŠ¨è„šæœ¬æ‰€éœ€çš„æ‰€æœ‰ä¸åŒå‘½ä»¤\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼š\naccelerate launch --accelerate-arg {script_name.py} --script-arg1 --script-arg2 ...\n\nç”±äºè¿™ä¼šè¿è¡Œå„ç§ torch ç”Ÿæˆæ–¹æ³•ï¼Œå› æ­¤ä¹Ÿå¯ä»¥åœ¨æ­¤å¤„ä¿®æ”¹æ‰€æœ‰ç¯å¢ƒå˜é‡ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨å•ä¸ª GPU ï¼š\nCUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...\n\næ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ accelerate launchï¼Œè€Œæ— éœ€å…ˆæ‰§è¡Œ accelerate configï¼Œä½†å¯èƒ½éœ€è¦æ‰‹åŠ¨è¾“å…¥æ­£ç¡®çš„é…ç½®å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒAccelerate ä¼šä¸ºä½ åšå‡ºä¸€äº›è¶…å‚æ•°å†³å®šï¼Œä¾‹å¦‚ï¼Œå¦‚æœ GPU å¯ç”¨ï¼Œå®ƒä¼šé»˜è®¤ä½¿ç”¨æ‰€æœ‰ GPUï¼Œä¸ä½¿ç”¨æ··åˆç²¾åº¦ã€‚\næŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼š\naccelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\nä½¿ç”¨æ··åˆç²¾åº¦åœ¨ä¸¤ä¸ª GPU ä¸Šå¯åŠ¨ç›¸åŒçš„è„šæœ¬\naccelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\nè¦è·å–å¯ä»¥ä¼ å…¥çš„å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·è¿è¡Œï¼š\naccelerate launch -h\n\nä»è¯¥è‡ªå®šä¹‰ yaml æ–‡ä»¶å¯åŠ¨è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š\naccelerate launch --config_file {path/to/config/my_config_file.yaml} {script_name.py} {--arg1} {--arg2} ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["Tech"],"tags":["python","Training-Trick"]},{"title":"Log","url":"/2024/08/01/Log/","content":"\n      \n        1f455657f328e3db0faa540e3c291d18e222c6ff6344f61ef5ba21ba3cf38c2480e8920cfc8beceb13c6efdc9b51554c08464845f0298c86f5b3edbf641ce8c30e7a8941f89ac280b7861824e9b5faeb1caff0d68e52b2bd5ee0b267546636472047eabc64501582445735757460dd2ec42d987a614b294545bda32d6c6000623fbafa7f92473ed0748b00d2e87544d0\n      \n      \n        \n          \n          \n            è¯·è¾“å…¥ä¸ Rhodes Islandâ„¢ å–å¾—å¼±ç¥ç»è¿æ¥æ—¶çš„å£ä»¤ï¼š\n          \n        \n        \n      \n    \n    "},{"url":"/2024/09/01/NTIRE%20Com/","content":"\nNight Photography Rendering Challenge (nightimaging.org)\nBlind Compressed Image Enhancement Challenge CodaLab - Competition (upsaclay.fr)\nRAW Image Super Resolution Challenge CodaLab - Competition (upsaclay.fr)\nReal-Time Image Super-Resolution CodaLab - Competition (upsaclay.fr)\n\n"},{"url":"/2024/10/07/BSR/","content":"\n\n\n\nDeg\nparm\np\n\n\n\n\nBlur1\nkernel-size&#x3D;[7,21] $\\sigma$&#x3D;[0.2, 3], $\\beta$&#x3D;[0.5, 4]\n1.0\n\n\n\n2D sinc\nNone\n0.1\n\n\n\nResize1\n\n1.0\n\n\n\nNoise1\nG[1, 30]  P[0.05, 3]\n0.5-0.5\n\n\n\nGray-Noise\n\n0.4\n\n\n\nJPEG1\n[30, 95]\n1.0\n\n\n\nBlur2\nkernel-size&#x3D;[7,21] $\\sigma$&#x3D;[0.2, 1.5]\n0.8\n\n\n\nResize2\n\n1.0\n\n\n\nNoise2\nG[1, 25]  P[0.05, 2.5]\n0.5-0.5\n\n\n\nJPEG2\n[30, 95]\n1.0\n\n\n\n2D sinc\nNone\n0.8\n\n\n\n"},{"url":"/2024/09/04/SR-Dataset&Method/","content":"è¶…åˆ†æ•°æ®é›†Train Set\n\n\nName\nHomepage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Set\n\n\nName\nHomepage\nInfo\nPaper\n\n\n\nRealSRSet\nBSRGAN&#x2F;testsets&#x2F;RealSRSet\n20 real low-resolution images\n[2103.14006] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution (arxiv.org)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethodSurvey[2107.03055] Blind Image Super-Resolution: A Survey and Beyond (arxiv.org)\nReal-ESRGAN:\nlink: [2107.10833] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data (arxiv.org)\ngithub: xinntao&#x2F;Real-ESRGAN: Real-ESRGAN aims at developing Practical Algorithms for General Image&#x2F;Video Restoration. (github.com)\n\n1. Classical Degradation ModelçœŸå®å›¾åƒ $y$ é¦–å…ˆä¸æ¨¡ç³Šæ ¸ $k$ è¿›è¡Œå·ç§¯ã€‚ç„¶åï¼Œæ‰§è¡Œæ¯”ä¾‹å› å­ä¸º $r$ çš„ä¸‹é‡‡æ ·æ“ä½œã€‚æ·»åŠ å™ªå£° $n$ ã€‚æœ€åï¼Œè¿˜é‡‡ç”¨äº†å¹¿æ³›ç”¨äºç°å®ä¸–ç•Œçš„å›¾åƒçš„ JPEG å‹ç¼©$$\\boldsymbol{x}&#x3D;\\mathcal{D}(\\boldsymbol{y})&#x3D;\\left[(\\boldsymbol{y} \\circledast \\boldsymbol{k}) \\downarrow_{r}+\\boldsymbol{n}\\right]_{\\mathrm{JPEG}}$$\n2. High orderDateset\nDIV2K\nFlickr2K\nOutdoorSceneTraining\n\nDetail\nåŸºäº XPixelGroup&#x2F;BasicSR å¼€å‘\nå¼•å…¥ exponential moving average (EMA) ç¨³å®šè®­ç»ƒ\næ¨¡æ¿ï¼šxinntao&#x2F;ProjectTemplate-Python: Python Project Template (github.com)\n\nDiffBIR\nlink: [2308.15070] DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior (arxiv.org)\ngithub: XPixelGroup&#x2F;DiffBIR: Official codes of DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior (github.com)\n\nBSRGAN\nlink:[2103.14006] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution (arxiv.org)\ngithub: cszn&#x2F;BSRGAN: Designing a Practical Degradation Model for Deep Blind Image Super-Resolution (ICCV, 2021) (PyTorch) - We released the training code! (github.com)\n\né€€åŒ–æ–¹æ³•\nhttps://huggingface.co/datasets/laion/laion-high-resolution/blob/main/part-00021-45914064-d424-4c1c-8d96-dc8125c645fb-c000.snappy.parquet\nhttps://huggingface.co/datasets/laion/laion-high-resolution/resolve/main/part-\\$(printf â€œ%05dâ€ $i)-5d6701c4-b238-4c0a-84e4-fe8e9daea963-c000.snappy.parquet\n45914064-d424-4c1c-8d96-dc8125c645fb-c000.snappy.parquet\n5d6701c4-b238-4c0a-84e4-fe8e9daea963-c000.snappy.parquet\n"},{"title":"NeuSç¬”è®°","url":"/2025/05/06/NeuS%E7%AC%94%E8%AE%B0/","content":"NeuS Noteæ¸²æŸ“è¿‡ç¨‹åœºæ™¯è¡¨ç¤ºè¦é‡å»ºçš„ç‰©ä½“åœºæ™¯ç”±ä¸¤ä¸ªå‡½æ•°è¡¨ç¤ºï¼š\n\n$f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$ï¼šå°†ç©ºé—´ä½ç½® $x \\in \\mathbb{R}^3$ æ˜ å°„åˆ°ç‰©ä½“çš„ç¬¦å·è·ç¦»\n$c: \\mathbb{R}^3 \\times S^2 \\rightarrow \\mathbb{R}^3$ï¼šå°†ç©ºé—´ä½ç½® $x \\in \\mathbb{R}^3$ å’Œè§†å›¾æ–¹å‘ $v \\in \\mathbb{S}^2$ æ˜ å°„ä¸ºé¢œè‰²\n\nè¿™ä¸¤ä¸ªå‡½æ•°éƒ½é€šè¿‡ MLP å®ç°\nç‰©ä½“çš„è¡¨é¢ $S$ ç”±å…¶ SDFï¼ˆsigned distance functionï¼‰ çš„é›¶ç­‰å€¼é¢è¡¨ç¤ºï¼Œå³$$\\mathcal{S} &#x3D; \\left{ \\mathbf{x} \\in \\mathbb{R}^3 \\mid f(\\mathbf{x}) &#x3D; 0 \\right}. \\tag{1}$$\n\nç¬¦å·è·ç¦»å‡½æ•° SDF\nå…·ä½“æ¥è¯´ï¼ŒSDFæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸º $F : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ï¼Œå…¶ä¸­ $n$ æ˜¯ç©ºé—´çš„ç»´åº¦ï¼ˆä¾‹å¦‚äºŒç»´æˆ–ä¸‰ç»´ï¼‰ã€‚å¯¹äºç©ºé—´ä¸­çš„ä»»æ„ä¸€ç‚¹ $p$ï¼ŒSDF $F(p)$ è¡¨ç¤ºä»ç‚¹ $p$ åˆ°æœ€è¿‘è¡¨é¢çš„è·ç¦»ï¼Œå¹¶ä¸”ï¼š\n\nå¦‚æœ $F(p) &lt; 0$ï¼Œåˆ™ç‚¹ $p$ åœ¨ç‰©ä½“å†…éƒ¨ï¼›\nå¦‚æœ $F(p) &#x3D; 0$ï¼Œåˆ™ç‚¹ $p$ åœ¨ç‰©ä½“è¡¨é¢ä¸Šï¼›\nå¦‚æœ $F(p) &gt; 0$ï¼Œåˆ™ç‚¹ $p$ åœ¨ç‰©ä½“å¤–éƒ¨ã€‚\n\n\nS-å¯†åº¦ä¸ºäº†å°†ä½“æ¸²æŸ“çš„æ–¹æ³•ç”¨äºè®­ç»ƒ SDF ç¥ç»ç½‘ç»œï¼Œå¼•å…¥æ¦‚ç‡å¯†åº¦å‡½æ•°  $\\phi_s(f(x))$ï¼Œç§°ä¸º S-å¯†åº¦åœºï¼š$$\\phi_s(x) &#x3D; \\frac{s e^{-sx}}{(1 + e^{-sx})^2} \\tag{2}$$S-å¯†åº¦åœºæ˜¯ Sigmoid å‡½æ•° $\\Phi_s(x) &#x3D; (1 + e^{-sx})^{-1}$ çš„å¯¼æ•°ï¼Œå³ $\\phi_s(x) &#x3D; \\Phi_sâ€™(x)$ã€‚\n\nåŸåˆ™ä¸Šï¼Œ$\\phi_s(x)$ å¯ä»¥æ˜¯ä»»ä½•ä»¥ 0 ä¸ºä¸­å¿ƒçš„å•å³°ï¼ˆå³é’Ÿå½¢ï¼‰å¯†åº¦åˆ†å¸ƒï¼›è¿™é‡Œæˆ‘ä»¬é€‰æ‹©é€»è¾‘å¯†åº¦åˆ†å¸ƒæ˜¯å› ä¸ºå®ƒçš„è®¡ç®—æ–¹ä¾¿ã€‚\næ³¨æ„ï¼Œ$\\phi_s(x)$ çš„æ ‡å‡†å·®ç”± $1&#x2F;s$ ç»™å‡ºï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå³éšç€ç½‘ç»œè®­ç»ƒçš„æ”¶æ•›ï¼Œ$1&#x2F;s$ æ¥è¿‘äºé›¶ã€‚\néšç€ $s$ çš„å¢å¤§ï¼Œ$\\phi_s(x)$ çš„çš„å³°å€¼é«˜åº¦ä¼šå¢åŠ ï¼Œå®½åº¦ä¼šå˜çª„ï¼Œä¹Ÿå°±æ˜¯è¯´éšç€ s çš„å¢å¤§è€Œå˜å¾—æ›´åŠ å°–é”å’Œé›†ä¸­\n\n\nNeuS çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œåœ¨ S-å¯†åº¦åœº $\\phi_s(f(x))$ çš„å¸®åŠ©ä¸‹ï¼Œä½¿ç”¨ä½“æ¸²æŸ“é€šè¿‡ä»…ä»¥ 2D è¾“å…¥å›¾åƒä½œä¸ºç›‘ç£æ¥è®­ç»ƒ SDF ç½‘ç»œã€‚åœ¨æˆåŠŸæœ€å°åŒ–åŸºäºè¿™ç§ç›‘ç£çš„æŸå¤±å‡½æ•°ä¹‹åï¼ŒæœŸæœ›ç½‘ç»œç¼–ç çš„ SDF çš„é›¶ç­‰å€¼é¢èƒ½å¤Ÿå‡†ç¡®åœ°è¡¨ç¤ºé‡å»ºçš„è¡¨é¢ $S$ï¼Œå…¶è¯±å¯¼çš„ S-å¯†åº¦ $\\phi_s(f(x))$ åœ¨è¡¨é¢é™„è¿‘å‘ˆç°å‡ºæ˜¾è‘—çš„é«˜å€¼ã€‚\n\n $\\phi_s(f(x))$ è¶Šå¤§ $\\rarr$ $f(x)$è¶Šè¶‹è¿‘äº0 $\\rarr$ $x$ è¶Šè¶‹è¿‘äºç‰©ä½“è¡¨é¢\n\næ¸²æŸ“ç»™å®šä¸€ä¸ªåƒç´ ï¼Œæˆ‘ä»¬å°†ä»è¯¥åƒç´ å‘å‡ºçš„å°„çº¿è¡¨ç¤ºä¸º ${p(t) &#x3D; o + tv \\mid t \\geq 0}$ï¼Œå…¶ä¸­ $o$ æ˜¯ç›¸æœºçš„ä¸­å¿ƒï¼Œ$v$ æ˜¯å°„çº¿çš„å•ä½æ–¹å‘å‘é‡ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼æ²¿å°„çº¿ç´¯ç§¯é¢œè‰²ï¼š\n$$C(\\mathbf{o}, \\mathbf{v}) &#x3D; \\int_{0}^{+\\infty} w(t)c(\\mathbf{p}(t), \\mathbf{v}) \\mathrm{d}t, \\tag{3}$$\n\n$C(\\mathbf{o}, \\mathbf{v})$ æ˜¯è¯¥åƒç´ çš„è¾“å‡ºé¢œè‰²\n$w(t)$ æ˜¯ç‚¹ $p(t)$ çš„æƒé‡å‡½æ•°\n$c(\\mathbf{p}(t), \\mathbf{v})$ æ˜¯æ²¿è§†å›¾æ–¹å‘ $\\mathbf{v}$ çš„ç‚¹ $\\mathbf{p}(t)$ å¤„çš„é¢œè‰²ã€‚\n\nå¯¹æƒé‡å‡½æ•° $w(t)$ çš„è¦æ±‚ä» 2D å›¾åƒä¸­å­¦ä¹ ç²¾ç¡®çš„ SDF è¡¨ç¤ºçš„å…³é”®æ˜¯å»ºç«‹è¾“å‡ºé¢œè‰² $C(\\mathbf{o}, \\mathbf{v})$ å’Œ SDF $f$ ä¹‹é—´çš„é€‚å½“è”ç³»ï¼Œå³åŸºäºåœºæ™¯çš„ SDF $f$ å¯¼å‡ºæ²¿å°„çº¿çš„é€‚å½“æƒé‡å‡½æ•° $w(t)$ã€‚\næƒé‡å‡½æ•° $w(t)$ è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š\n\næ— åæ€§ã€‚ç»™å®šä¸€æ¡ç›¸æœºå°„çº¿ $p(t)$ï¼Œ$w(t)$ åœ¨è¡¨é¢äº¤ç‚¹ $p(t^*)$ å¤„å–å¾—å±€éƒ¨æœ€å¤§å€¼ï¼Œå³ $f(p(t^*)) &#x3D; 0$ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç‚¹ $p(t^*)$ ä½äº SDF çš„é›¶ç­‰å€¼é¢ä¸Šï¼ˆ$x$ï¼‰ã€‚\n\né®æŒ¡æ„ŸçŸ¥ã€‚ç»™å®šä»»æ„ä¸¤ä¸ªæ·±åº¦å€¼ $t_0$ å’Œ $t_1$ï¼Œæ»¡è¶³ $f(t_0) &#x3D; f(t_1)$ï¼Œ$w(t_0) &gt; 0$ï¼Œ$w(t_1) &gt; 0$ï¼Œä¸” $t_0 &lt; t_1$ï¼Œæœ‰ $w(t_0) &gt; w(t_1)$ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ä¸¤ä¸ªç‚¹å…·æœ‰ç›¸åŒçš„ SDF å€¼ï¼ˆå› æ­¤å…·æœ‰ç›¸åŒçš„ SDF è¯±å¯¼çš„ S-å¯†åº¦å€¼ï¼‰æ—¶ï¼Œæ›´é è¿‘è§†ç‚¹çš„ç‚¹åº”è¯¥å¯¹æœ€ç»ˆè¾“å‡ºé¢œè‰²æœ‰æ›´å¤§çš„è´¡çŒ®ã€‚\n\n\nä¸€ä¸ªæ— åçš„æƒé‡å‡½æ•° $w(t)$ ä¿è¯äº†ç›¸æœºå°„çº¿ä¸ SDF çš„é›¶ç­‰å€¼é¢çš„äº¤ç‚¹å¯¹åƒç´ é¢œè‰²çš„è´¡çŒ®æœ€å¤§ã€‚\né®æŒ¡æ„ŸçŸ¥å±æ€§ç¡®ä¿äº†å½“ä¸€æ¡å°„çº¿ä¾æ¬¡ç©¿è¿‡å¤šä¸ªè¡¨é¢æ—¶ï¼Œæ¸²æŸ“è¿‡ç¨‹å°†æ­£ç¡®åœ°ä½¿ç”¨ç¦»ç›¸æœºæœ€è¿‘çš„è¡¨é¢çš„é¢œè‰²æ¥è®¡ç®—è¾“å‡ºé¢œè‰²ã€‚\nNeRFçš„æœ´ç´ è§£NeRFä¸­çš„æƒé‡å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š$$w(t) &#x3D; T(t) \\sigma(t),\\tag{4}$$\n\n$\\sigma(t)$ æ˜¯NeRFä¸­æ‰€è°“çš„ä½“å¯†åº¦ï¼Œè¿™é‡Œå°† $\\sigma(t)$ è®¾ç½®ä¸ºç­‰äº S-å¯†åº¦å€¼ï¼Œå³ $\\sigma(t) &#x3D; \\phi_s(f(p(t)))$ \n$T(t) &#x3D; \\exp\\left(-\\int_{0}^{t} \\sigma(u) \\mathrm{d}u\\right)$ è¡¨ç¤ºæ²¿å°„çº¿çš„ç´¯ç§¯é€å°„ç‡ã€‚\n\nå°½ç®¡ç”±æ­¤äº§ç”Ÿçš„æƒé‡å‡½æ•°å…·æœ‰é®æŒ¡æ„ŸçŸ¥æ€§ï¼Œä½†å®ƒæ˜¯æœ‰åçš„ï¼Œå› ä¸ºå®ƒåœ¨é‡å»ºçš„è¡¨é¢ä¸­å¼•å…¥äº†å›ºæœ‰çš„è¯¯å·®ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæƒé‡å‡½æ•° $w(t)$ åœ¨å°„çº¿è¾¾åˆ°æ»¡è¶³ $f(p(t^*)) &#x3D; 0$ çš„è¡¨é¢ç‚¹ $p(t^*)$ ä¹‹å‰çš„æŸç‚¹è¾¾åˆ°å±€éƒ¨æœ€å¤§å€¼\n\nNeuSå¯¹æƒé‡å‡½æ•° $w(t)$ çš„è§£ä¸ºäº†ä»‹ç» NeuS çš„è§£å†³æ–¹æ¡ˆï¼Œé¦–å…ˆä»‹ç»ä¸€ç§ç›´æ¥ä½¿ç”¨å½’ä¸€åŒ– S-å¯†åº¦ä½œä¸ºæƒé‡æ¥æ„å»ºæ— åæƒé‡å‡½æ•°çš„ç®€å•æ–¹æ³•ï¼š$$w(t) &#x3D; \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_{0}^{+\\infty} \\phi_s(f(\\mathbf{p}(u))) \\mathrm{d}u}.\\tag{5}$$è¿™ç§æƒé‡å‡½æ•°çš„æ„é€ æ˜¯æ— åçš„ï¼Œä½†ä¸å…·å¤‡é®æŒ¡æ„ŸçŸ¥æ€§ã€‚\n\nä¾‹å¦‚ï¼Œå¦‚æœå°„çº¿ç©¿é€ä¸¤ä¸ªè¡¨é¢ï¼ŒSDF å‡½æ•° $f$ å°†åœ¨å°„çº¿ä¸Šæœ‰ä¸¤ä¸ªé›¶ç‚¹ï¼Œè¿™å¯¼è‡´æƒé‡å‡½æ•° $w(t)$ ä¸Šæœ‰ä¸¤ä¸ªå³°å€¼ï¼Œå¹¶ä¸”ç”±æ­¤äº§ç”Ÿçš„æƒé‡å‡½æ•°å°†å¹³å‡æ··åˆè¿™ä¸¤ä¸ªè¡¨é¢çš„é¢œè‰²ï¼Œè€Œä¸è€ƒè™‘é®æŒ¡ã€‚\n\nä¸ºäº†ç¡®ä¿æƒé‡å‡½æ•° $w(t)$ çš„é®æŒ¡æ„ŸçŸ¥å±æ€§ï¼Œæˆ‘ä»¬ä»å°†éµå¾ªNeRF çš„åŸºæœ¬æ¡†æ¶ï¼Œä»¥ä¸€ç§æ–°çš„æ–¹å¼ä» S-å¯†åº¦å®šä¹‰æˆ‘ä»¬çš„æƒé‡å‡½æ•° $w(t)$ï¼š$$w(t) &#x3D; T(t) \\rho(t), \\text{ where } T(t) &#x3D; \\exp\\left(-\\int_{0}^{t} \\rho(u) \\mathrm{d}u\\right).\\tag{6}$$\n\n$\\rho(t)$ï¼šä¸é€æ˜å¯†åº¦å‡½æ•°ï¼Œæ˜¯æ ‡å‡†ä½“ç§¯æ¸²æŸ“ä¸­ä½“ç§¯å¯†åº¦ $\\sigma$ çš„å¯¹åº”\n\nä¸é€æ˜å¯†åº¦å‡½æ•° $\\rho(t)$ çš„æ¨å¯¼é¦–å…ˆè€ƒè™‘ä¸€ä¸ªç®€å•çš„ç†æƒ³æƒ…å†µï¼šè¡¨é¢æ˜¯ä¸€ä¸ªè¿œç¦»ç›¸æœºçš„å¹³é¢ï¼Œä¸”åªæœ‰ä¸€ä¸ªäº¤ç‚¹\næ­¤æ—¶çš„ç¬¦å·è·ç¦»å‡½æ•° SDF å¾ˆæ˜æ˜¾æ˜¯ï¼š$$f(p(t)) &#x3D; -|\\cos(\\theta)| \\cdot (t - t^*)$$\n\nç„¦ç‚¹ä½ç½®ï¼š$f(p(t^*)) &#x3D; 0$\n$\\theta$ æ˜¯è§†å›¾æ–¹å‘ $v$ å’Œå‘å¤–è¡¨é¢æ³•çº¿å‘é‡ $n$ ä¹‹é—´çš„è§’åº¦\n\nåœ¨è¿™ç§å‡è®¾ä¸‹å…¬å¼ 5 ç¡®å®æ»¡è¶³è¦æ±‚ï¼Œç”±äº$|\\cos(\\theta)|$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¯ä»¥å¾—å‡ºï¼š$$\\begin{align*}w(t) &amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_0^{+\\infty} \\phi_s(f(\\mathbf{p}(u))) \\mathrm{d}u} \\&amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_0^{+\\infty} \\phi_s(-|\\cos(\\theta)|(u - t^*)) \\mathrm{d}u} \\&amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{\\int_{-t^*}^{+\\infty} \\phi_s(-|\\cos(\\theta)|u^*) \\mathrm{d}u^*} \\tag{7} \\&amp;&#x3D; \\lim_{t^* \\to +\\infty} \\frac{\\phi_s(f(\\mathbf{p}(t)))}{|\\cos(\\theta)|^{-1} \\int_{-|\\cos(\\theta)|t^*}^{+\\infty} \\phi_s(\\hat{u}) \\mathrm{d}\\hat{u}} \\&amp;&#x3D; |\\cos(\\theta)| \\phi_s(f(\\mathbf{p}(t))).\\end{align*}$$\næœ´ç´ è§£ä¸­çš„åå·®æƒé‡å‡½æ•°å®šä¹‰ä¸º $w(t) &#x3D; T(t) \\sigma(t)$ï¼Œå…¶ä¸­ä¸é€æ˜åº¦ $\\sigma(t) &#x3D; \\phi_s(f(\\mathbf{p}(t)))$ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰$$\\begin{equation}\\begin{split}\\frac{\\mathrm{d}w}{\\mathrm{d}t} &amp;&#x3D; \\frac{\\mathrm{d}(T(t)\\sigma(t))}{\\mathrm{d}t} \\&amp;&#x3D; \\frac{\\mathrm{d}T(t)}{\\mathrm{d}t}\\sigma(t) + T(t)\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} \\&amp;&#x3D; \\left[\\exp\\left(-\\int_0^t \\sigma(t) \\mathrm{d}t\\right)(-\\sigma(t))\\right]\\sigma(t) + T(t)\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} \\&amp;&#x3D; T(t)(-\\sigma(t))\\sigma(t) + T(t)\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} \\&amp;&#x3D; T(t)\\left(\\frac{\\mathrm{d}\\sigma(t)}{\\mathrm{d}t} - \\sigma(t)^2\\right).\\end{split}\\end{equation}$$å¾…å®š\n","categories":["Note"],"tags":["ä¸‰ç»´é‡å»º"]},{"url":"/2024/11/19/%E6%9C%AA%E5%91%BD%E5%90%8D/","content":"æ­¦å˜‰çªé‚®ç®±: dl2wjq@gmail.com\nç”µè¯: 18153421584\nåœ°å€: ç”µå­ç§‘æŠ€å¤§å­¦æ¸…æ°´æ²³æ ¡åŒº\næ•™è‚²ç»å†\næœ¬ç§‘ | ç”µå­ç§‘æŠ€å¤§å­¦ | é€šä¿¡å·¥ç¨‹ | 2019å¹´9æœˆ - 2023å¹´6æœˆ\n\nç¡•å£« | ç”µå­ç§‘æŠ€å¤§å­¦ | ç”µå­ä¿¡æ¯ | 2023å¹´9æœˆ - ç°åœ¨\n\n\nç ”ç©¶é¢†åŸŸ\nç”Ÿæˆå¼æ¨¡å‹ï¼šæ‰©æ•£æ¨¡å‹åŠå…¶åº”ç”¨\n3D è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ï¼šä¸‰ç»´é‡å»ºï¼ˆNeRF &amp; 3DGSï¼‰\nè®¡ç®—æœºè§†è§‰ï¼šå›¾åƒå¢å¼ºï¼ˆä½å…‰å›¾åƒå¢å¼ºï¼‰ï¼Œå›¾åƒèåˆ\n\nç ”ç©¶é¡¹ç›®åŸºäºæ‰©æ•£æ¨¡å‹çš„æ˜Ÿç©º&amp;ä½å…‰å›¾åƒå¢å¼º\n\næ„å»ºäº†ç¬¬ä¸€ä¸ªç”±355ä¸ªå®æ‹å’Œ854ä¸ªåŠåˆæˆå›¾åƒå¯¹ç»„æˆçš„æ˜Ÿé‡å›¾åƒå¢å¼ºåŸºå‡†æ•°æ®é›†ï¼Œè¿™ä½¿å¾—ä¸åŒæ–¹æ³•å¯¹æ˜Ÿç©ºå›¾åƒå¢å¼ºçš„æ¯”è¾ƒæˆä¸ºå¯èƒ½\nå¼€å‘äº†ç¬¬ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM, Denoising Diffusion Probabilistic Models) çš„æ˜Ÿç©ºå›¾åƒå¢å¼ºæ–¹æ³•ï¼Œåœ¨ä½å…‰å›¾åƒå¢å¼ºå’Œæ˜Ÿé‡å›¾åƒå¢å¼ºä»»åŠ¡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„ä½å…‰å›¾åƒå¢å¼ºç®—æ³•\n\næŠ€èƒ½\nç¼–ç¨‹è¯­è¨€: Python, Matlab, C++\næ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šPyTorch\n\n"}]