<!DOCTYPE html><html lang="zh-CN" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Accelerate基本使用方法 | TO-Hitori</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"Enter here","blurHolder":"Search","noResult":"Data \"$0\" not found"},"code":{"codeInfo":"$0 - $1 lines","copy":"copy"}}</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>:root {
 --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
 --light-background: url('/img/bk.jpg');
 --theme-encrypt-confirm: 'confirm'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 7.1.1"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="Search" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>Accelerate基本使用方法</h1><div id="post-info"><span>First Post: <div class="control"><time datetime="2024-04-19T14:29:30.000Z" id="date"> 2024-04-19</time></div></span><br><span>Last Update: <div class="control"><time datetime="2024-04-19T14:30:39.644Z" id="updated"> 2024-04-19</time></div></span><br><span>Word Count: <div class="control">3.4k</div></span><br><span>Read Time: <div class="control">14 min</div></span></div></div><hr><div id="post-content"><h1 id="Accelerate-Note"><a href="#Accelerate-Note" class="headerlink" title="Accelerate-Note"></a>Accelerate-Note</h1><p>Accelerate 是一个由 Hugging Face 开发的 Python 库，它允许开发者将相同的 PyTorch 代码运行在任何分布式配置上，只需添加四行代码即可。<br>这个库简化了在多种环境（包括单机、多 GPU、TPU 和各种分布式训练环境）中进行深度学习训练的过程。它基于 <code>torch_xla</code> 和 <code>torch.distributed</code> 构建，可以轻松地将现有代码库转换为使用 DeepSpeed 进行全分片数据并行处理，并自动支持混合精度训练。</p>
<h2 id="将Accelerate添加到PyTorch代码中"><a href="#将Accelerate添加到PyTorch代码中" class="headerlink" title="将Accelerate添加到PyTorch代码中"></a>将Accelerate添加到PyTorch代码中</h2><p>Accelerate 提供了一种友好的方式来适应各类分布式框架，而无需学习每个框架的具体细节。</p>
<p>接下来将从一个基本的 PyTorch 训练循环开始（假设模型和优化器等所有训练对象都已设置完毕），然后逐步将 Accelerate 集成到其中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化</span><br>model = ...                <span class="hljs-comment"># torch.nn.Module</span><br>training_dataloader = ...  <span class="hljs-comment"># torch.utils.data.DataLoader</span><br>optimizer = ...            <span class="hljs-comment"># torch.optim</span><br>scheduler = ...            <span class="hljs-comment"># torch.optim.lr_scheduler</span><br><br>device = <span class="hljs-string">"cuda"</span><br>model.to(device)<br><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:<br>    optimizer.zero_grad()<br>    inputs, targets = batch<br>    inputs = inputs.to(device)<br>    targets = targets.to(device)<br>    outputs = model(inputs)<br>    loss = loss_function(outputs, targets)<br>    loss.backward()<br>    optimizer.step()<br>    scheduler.step()<br></code></pre></td></tr></table></figure>

<p>完整实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> lr_scheduler<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> random_split<br><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> datasets<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">check_accuracy</span>(<span class="hljs-params">loader, model</span>):<br>    num_correct = <span class="hljs-number">0</span><br>    num_samples = <span class="hljs-number">0</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># Loop through the data</span><br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> loader:<br><br>            <span class="hljs-comment"># Move data to device</span><br>            x = x.to(device=device)<br>            y = y.to(device=device)<br><br>            <span class="hljs-comment"># Get to correct shape</span><br>            x = x.reshape(x.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># Forward pass</span><br>            scores = model(x)<br>            _, predictions = scores.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># Check how many we got correct</span><br>            num_correct += (predictions == y).<span class="hljs-built_in">sum</span>()<br><br>            <span class="hljs-comment"># Keep track of number of samples</span><br>            num_samples += predictions.size(<span class="hljs-number">0</span>)<br><br>    model.train()<br>    <span class="hljs-keyword">return</span> num_correct / num_samples<br><br><br><br><span class="hljs-comment"># 超参数设置</span><br>input_size = <span class="hljs-number">784</span><br>num_classes = <span class="hljs-number">10</span><br>learning_rate = <span class="hljs-number">0.001</span><br>batch_size = <span class="hljs-number">64</span><br>num_epochs = <span class="hljs-number">10</span><br>sw = SummaryWriter(<span class="hljs-string">'./mnist-logs'</span>)<br><br><span class="hljs-comment"># 0. 设备设置</span><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br><span class="hljs-comment"># 1. 模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, num_classes</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.fc1 = nn.Linear(input_size, <span class="hljs-number">50</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">50</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.fc1(x))<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br><br>model = NN(input_size=input_size, num_classes=num_classes).to(device)<br><br><span class="hljs-comment"># 2. 数据加载器</span><br>entire_dataset = datasets.MNIST(<br>    root=<span class="hljs-string">"dataset/"</span>, train=<span class="hljs-literal">True</span>, transform=transforms.ToTensor(), download=<span class="hljs-literal">True</span><br>)<br>train_ds, val_ds = random_split(entire_dataset, [<span class="hljs-number">50000</span>, <span class="hljs-number">10000</span>])<br><br>test_ds = datasets.MNIST(<br>    root=<span class="hljs-string">"dataset/"</span>, train=<span class="hljs-literal">False</span>, transform=transforms.ToTensor(), download=<span class="hljs-literal">True</span><br>)<br><br>train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>val_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 3.优化器和调度器</span><br>optimizer = optim.Adam(<br>    model.parameters(),<br>    lr=learning_rate<br>)<br>scheduler = lr_scheduler.CosineAnnealingWarmRestarts(<br>    optimizer,<br>    T_0=<span class="hljs-number">2</span>,<br>    eta_min=<span class="hljs-number">1e-6</span><br>)<br><br><br><span class="hljs-comment"># 损失函数</span><br>loss_func = nn.CrossEntropyLoss()<br><br><br><span class="hljs-comment"># 训练循环</span><br>len_per_epoch = <span class="hljs-built_in">len</span>(train_loader)<br>global_step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> i, (data, targets) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(train_loader)):<br>        <span class="hljs-comment"># 清空梯度</span><br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># 数据移至device</span><br>        data = data.to(device=device)<br>        targets = targets.to(device=device)<br><br>        <span class="hljs-comment"># 计算损失</span><br>        data = data.reshape(data.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>        scores = model(data)<br>        loss = loss_func(scores, targets)<br><br>        <span class="hljs-comment"># Backward</span><br>        loss.backward()<br><br>        <span class="hljs-comment"># Gradient descent or adam step</span><br>        optimizer.step()<br>        scheduler.step(epoch + i / len_per_epoch)<br>        global_step += <span class="hljs-number">1</span><br>        sw.add_scalar(<span class="hljs-string">'lr'</span>, scheduler.get_last_lr()[<span class="hljs-number">0</span>], global_step=global_step)<br><br>    val_accuracy = check_accuracy(val_loader, model) * <span class="hljs-number">100</span><br>    sw.add_scalar(<span class="hljs-string">'val_accuracy'</span>, val_accuracy, global_step=epoch)<br><br><br><span class="hljs-comment"># Check accuracy on training &amp; test to see how good our model</span><br>model.to(device)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Accuracy on training set: <span class="hljs-subst">{check_accuracy(train_loader, model)*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>"</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Accuracy on validation set: <span class="hljs-subst">{check_accuracy(val_loader, model)*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>"</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Accuracy on test set: <span class="hljs-subst">{check_accuracy(test_loader, model)*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>"</span>)<br></code></pre></td></tr></table></figure>



<h3 id="Accelerator"><a href="#Accelerator" class="headerlink" title="Accelerator"></a>Accelerator</h3><p>Accelerator 是用于调整代码以与 Accelerate 配合使用的主类。它了解你正在使用的分布式设置，如不同进程的数量和硬件类型。使您的 PyTorch 代码能够在任何分布式训练环境中工作，并管理和执行跨设备进程。</p>
<p>从导入和创建 <code>Accelerator</code> 实例开始：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br><br>accelerator = Accelerator()<br></code></pre></td></tr></table></figure>

<p><code>Accelerator</code> 知道将 PyTorch 对象移动到哪个设备上，因此建议让 Accelerate 设置 <code>device</code> 参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">- device = <span class="hljs-string">"cuda"</span><br>+ device = accelerator.device<br>  model.to(device)<br></code></pre></td></tr></table></figure>

<h3 id="准备-PyTorch-对象"><a href="#准备-PyTorch-对象" class="headerlink" title="准备 PyTorch 对象"></a>准备 PyTorch 对象</h3><p>接下来，您需要为分布式训练准备好 PyTorch 对象（模型、优化器、调度器、数据加载器等）。</p>
<p><code>prepare()</code> 方法会根据训练设置将<strong>模型</strong>放入适当的容器（如单GPU或多GPU）中，调整<strong>优化器</strong>和<strong>调度器</strong>以使用 Accelerate 的 <code>AcceleratedOptimizer</code> 和 <code>AcceleratedScheduler</code>，并创建可跨进程分片的新<strong>数据加载器</strong>。</p>
<blockquote>
<p><code>prepare()</code> 方法仅准备从 PyTorch 类继承的对象，例如 torch.optim.Optimizer。</p>
</blockquote>
<p>PyTorch 对象的返回顺序与输入顺序相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model, optimizer, training_dataloader, scheduler = accelerator.prepare(<br>    model, optimizer, training_dataloader, scheduler<br>)<br></code></pre></td></tr></table></figure>



<h3 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h3><p>最后，删除训练循环中对输入和目标数据的 <code>to(device)</code> 调用，因为 <code>Accelerate</code> 的 <code>DataLoader</code> 类会自动将它们放置在正确的设备上。</p>
<p>您还应该将通常的<code>loss.backward()</code>传递替换为 <code>Accelerate</code> 的<code>backward()</code>方法，该方法会为您<strong>缩放梯度</strong>，并根据您的<strong>分布式设置</strong>（例如，DeepSpeed或Megatron）使用适当的backward()方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">-   inputs = inputs.to(device)<br>-   targets = targets.to(device)<br>    outputs = model(inputs)<br>    loss = loss_function(outputs, targets)<br>-   loss.backward()<br>+   accelerator.backward(loss)<br></code></pre></td></tr></table></figure>



<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>将所有内容放在一起，您的 Accelerate 训练循环现在应该如下所示！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br>accelerator = Accelerator()<br><br><span class="hljs-comment"># 初始化</span><br>model = ...                <span class="hljs-comment"># torch.nn.Module</span><br>training_dataloader = ...  <span class="hljs-comment"># torch.utils.data.DataLoader</span><br>optimizer = ...            <span class="hljs-comment"># torch.optim</span><br>scheduler = ...            <span class="hljs-comment"># torch.optim.lr_scheduler</span><br><br>device = accelerator.device<br>model, optimizer, training_dataloader, scheduler = accelerator.prepare(<br>    model, optimizer, training_dataloader, scheduler<br>)<br><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:<br>    optimizer.zero_grad()<br>    <br>    inputs, targets = batch<br>    <br>    outputs = model(inputs)<br>    loss = loss_function(outputs, targets)<br>    <br>    accelerator.backward(loss)<br>    <br>    optimizer.step()<br>    scheduler.step()<br></code></pre></td></tr></table></figure>



<h3 id="Training-features"><a href="#Training-features" class="headerlink" title="Training features"></a>Training features</h3><p>Accelerate 提供了额外的功能，例如梯度累积 (gradient accumulation)、梯度裁剪 (gradient clipping)、混合精度训练 (mixed precision training)等，您可以将其添加到脚本中以改进训练。</p>
<h4 id="梯度累积"><a href="#梯度累积" class="headerlink" title="梯度累积"></a>梯度累积</h4><p>梯度累积使您能够在更新权重之前通过累积多个批次的梯度来获取更大的等效 <code>batch_size</code>。这对于解决显存对 <code>batch_size</code> 的限制很有用。</p>
<p>要在 Accelerate 中启用此功能，请在加速器类中指定 <code>gradient_accumulation_steps</code> 参数，并在脚本中添加 <code>accumulate()</code> 上下文管理器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">+ accelerator = Accelerator(gradient_accumulation_steps=<span class="hljs-number">2</span>)<br>  model, optimizer, training_dataloader = accelerator.prepare(<br>      model, optimizer, training_dataloader<br>  )<br><br>  <span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, label <span class="hljs-keyword">in</span> training_dataloader:<br>+     <span class="hljs-keyword">with</span> accelerator.accumulate(model):<br>          predictions = model(<span class="hljs-built_in">input</span>)<br>          loss = loss_function(predictions, label)<br>          accelerator.backward(loss)<br>          optimizer.step()<br>          scheduler.step()<br>          optimizer.zero_grad()<br></code></pre></td></tr></table></figure>

<h4 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h4><p>梯度裁剪是一种防止“梯度爆炸”的技术，Accelerate 提供以下两种方法：</p>
<ul>
<li><code>clip_grad_value_</code>：将可迭代参数的梯度裁剪为指定值。 梯度就地修改（in-place）。<ul>
<li><code>parametres</code>：可迭代的张量或单个张量，其梯度将归一化</li>
<li><code>clip_value</code>：梯度的阈值。梯度被限制在范围<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="21.535ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 9518.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mo" transform="translate(278,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1056,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1489,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1787,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(2132,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mi" transform="translate(3060.9,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3589.9,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(3887.9,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4459.9,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(4925.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5370.6,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(5803.6,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(6101.6,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(6446.6,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mi" transform="translate(7375.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(7904.6,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(8202.6,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8774.6,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(9240.6,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>内</li>
</ul>
</li>
<li><code>clip_grad_norm_</code>：范数是对所有梯度一起计算的。梯度就地修改。<ul>
<li><code>parameters</code>：可迭代的张量或单个张量，其梯度将归一化</li>
<li><code>max_norm</code>：梯度的最大范数</li>
<li><code>norm_type</code>：float，默认为2.0，用的 p-范数的类型。<code>inf</code>表示无穷范数。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br><br>accelerator = Accelerator(gradient_accumulation_steps=<span class="hljs-number">2</span>)<br>dataloader, model, optimizer, scheduler = accelerator.prepare(<br>dataloader, model, optimizer, scheduler<br>)<br><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataloader:<br>     optimizer.zero_grad()<br>     output = model(<span class="hljs-built_in">input</span>)<br>     loss = loss_func(output, target)<br>     accelerator.backward(loss)<br>     <span class="hljs-keyword">if</span> accelerator.sync_gradients:<br>     <span class="hljs-comment"># 二者取其一：</span><br>    	accelerator.clip_grad_value_(model.parameters(), clip_value)<br>        accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)<br>        <br>     optimizer.step()<br></code></pre></td></tr></table></figure>



<h4 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h4><p>混合精度通过使用 fp16（半精度）等较低精度的数据类型来计算梯度，从而加速训练。要想使用 Accelerate 获得最佳性能，应在模型内部计算损失（如在 Transformers 模型中），因为模型外部的计算是以全精度进行的。</p>
<p>设置要在 <code>accelerater</code> 中使用的混合精度类型，然后使用 <code>autocast()</code> 上下文管理器将值自动转换为指定的数据类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br>+ accelerator = Accelerator(mixed_precision=<span class="hljs-string">"fp16"</span>)<br><br>+ <span class="hljs-keyword">with</span> accelerator.autocast():<br>      loss = complex_loss_function(outputs, target):<br></code></pre></td></tr></table></figure>



<h3 id="保存和加载"><a href="#保存和加载" class="headerlink" title="保存和加载"></a>保存和加载</h3><p>训练完成后，加速还可以保存和加载模型，或者您还可以保存模型和优化器状态（optimizer state），这对于恢复训练很有用。</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>所有过程完成后，在保存模型前使用 <code>unwrap_model()</code> 方法解除模型的封装，因为训练开始前执行的 <code>prepare()</code> 方法将模型封装到了适合的分布式训练接口中。如果不解除对模型的封装，保存模型状态字典的同时也会保存大模型中任何潜在的额外层，这样就无法将权重加载回基础模型中。</p>
<p>使用 <code>save_model()</code> 方法来解包并保存模型状态字典。此方法还可以将模型保存到切片检查点 <code>sharded checkpoints</code> 或<code>safetensors</code>格式中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">accelerator.wait_for_everyone()<br>accelerator.save_model(model, save_directory)<br></code></pre></td></tr></table></figure>

<blockquote>
<p>对于 Transformers 库中的模型，请使用 <code>save_pretrained</code> 方法保存模型，以便可以使用 <code>from_pretrained</code>方法重新加载。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel<br><br>unwrapped_model = accelerator.unwrap_model(model)<br>unwrapped_model.save_pretrained(<br>    <span class="hljs-string">"path/to/my_model_directory"</span>,<br>    is_main_process=accelerator.is_main_process,<br>    save_function=accelerator.save,<br>)<br><br>model = AutoModel.from_pretrained(<span class="hljs-string">"path/to/my_model_directory"</span>)<br></code></pre></td></tr></table></figure>
</blockquote>
<p>要加载权重，请在加载权重之前先使用 <code>unwrap_model()</code> 方法解包模型。所有模型参数都是对张量的引用，因此这会将您的权重加载到模型中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">unwrapped_model = accelerator.unwrap_model(model)<br>path_to_checkpoint = os.path.join(save_directory,<span class="hljs-string">"pytorch_model.bin"</span>)<br>unwrapped_model.load_state_dict(torch.load(path_to_checkpoint))<br></code></pre></td></tr></table></figure>

<h5 id="切片检查点"><a href="#切片检查点" class="headerlink" title="切片检查点"></a>切片检查点</h5><p>设置 <code>safe_serialization=True</code> 将模型保存为 <code>safetensor</code> 格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">accelerator.wait_for_everyone()<br>accelerator.save_model(model, save_directory, max_shard_size=<span class="hljs-string">"1GB"</span>, safe_serialization=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>要加载分片检查点或 <code>safetensor</code> 格式的检查点，请使用 <code>load_checkpoint_in_model()</code> 方法。此方法允许您将检查点加载到特定设备上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">load_checkpoint_in_model(unwrapped_model, save_directory, device_map={<span class="hljs-string">""</span>:device})<br></code></pre></td></tr></table></figure>



<h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><p>在训练过程中，你可能希望保存模型、优化器、随机生成器以及学习率调度器的当前状态，以便在同一个脚本中恢复它们。你应该在脚本中添加 <code>save_state()</code> 和 <code>load_state()</code> 方法来保存和加载状态。</p>
<p>任何其他需要存储的有状态项目都应使用 <code>register_for_checkpointing()</code> 方法注册，以便保存和加载。传递给此方法的每个要存储的对象都必须具有 <code>load_state_dict</code> 和 <code>state_dict</code> 函数。</p>
<h2 id="执行进程"><a href="#执行进程" class="headerlink" title="执行进程"></a>执行进程</h2><p>在使用分布式训练系统时，管理跨 GPU 执行流程的方式和时间非常重要。有些进程比其他进程完成得更快，有些进程在其他进程尚未完成时就不应开始。Accelerate 提供了用于协调进程执行时间的工具，以确保所有设备上的一切<strong>保持同步</strong>。</p>
<h3 id="在一个进程上执行"><a href="#在一个进程上执行" class="headerlink" title="在一个进程上执行"></a>在一个进程上执行</h3><p>某些代码只需在特定机器上运行一次，如打印日志语句或只在本地主进程上显示一个进度条。</p>
<h4 id="statement"><a href="#statement" class="headerlink" title="statement"></a>statement</h4><p>应使用 <code>accelerator.is_local_main_process</code> 来指示只应执行一次的代码。</p>
<blockquote>
<p><code>accelerator.is_local_main_process</code> ：</p>
<ul>
<li>用于判断当前进程是否是<strong>本地节点</strong>（服务器）上的主进程，</li>
<li>如果你的训练任务在多台服务器上运行，每台服务器都有一个主进程。<code>is_local_main_process()</code> 如果返回 <code>True</code>，表示当前进程是本地节点上的主进程。</li>
<li>通常，你可以在本地节点的主进程上执行一些只需执行一次的操作，例如初始化数据、加载预训练模型等。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br>progress_bar = tqdm(<br>    <span class="hljs-built_in">range</span>(args.max_train_steps), <br>    disable=<span class="hljs-keyword">not</span> accelerator.is_local_main_process<br>)<br></code></pre></td></tr></table></figure>

<p>还可以使用 <code>accelerator.is_local_main_process</code> 包装语句。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> accelerator.is_local_main_process:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Accelerate is the best"</span>)<br></code></pre></td></tr></table></figure>



<p>还可以指示 Accelerate 在所有进程中都要执行一次的代码，而不管有多少台机器。如果您要将最终模型上传到 Hub，这将非常有用。</p>
<blockquote>
<p><code>accelerator.is_main_process</code>：</p>
<ul>
<li>这个函数用于判断当前进程是否是<strong>整个训练任务</strong>中的主进程。</li>
<li>主进程通常负责一些全局操作，例如模型保存、日志记录等。因此，你可以使用 <code>is_main_process()</code> 来确保这些操作只在主进程中执行一次。</li>
<li>如果你的训练任务在多台服务器上运行，<code>is_main_process()</code> 将返回 <code>True</code>，只有一个服务器上的主进程会满足这个条件。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> accelerator.is_main_process:<br>    repo.push_to_hub()<br></code></pre></td></tr></table></figure>



<h4 id="function"><a href="#function" class="headerlink" title="function"></a>function</h4><p>对于只应执行一次的函数，请使用 <code>on_local_main_process</code> 装饰器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_local_main_process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():<br>    <span class="hljs-string">"Something done once per server"</span><br>    do_thing_once_per_server()<br></code></pre></td></tr></table></figure>

<p>对于只应在所有进程中执行一次的函数，请使用 <code>on_main_process</code> 装饰器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_main_process</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():<br>    <span class="hljs-string">"Something done once per server"</span><br>    do_thing_once()<br></code></pre></td></tr></table></figure>





<h3 id="在特定进程上执行"><a href="#在特定进程上执行" class="headerlink" title="在特定进程上执行"></a>在特定进程上执行</h3><p>Accelerate 还可以执行只应在特定进程或本地进程索引上执行的函数。</p>
<p>使用 on_process() 装饰器指定要执行函数的进程索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_process(<span class="hljs-params">process_index=<span class="hljs-number">0</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():<br>    <span class="hljs-string">"Something done on process index 0"</span><br>    do_thing_on_index_zero()<br></code></pre></td></tr></table></figure>

<p>使用 <code>on_local_process()</code> 装饰器指定要执行函数的本地进程索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_local_process(<span class="hljs-params">local_process_idx=<span class="hljs-number">0</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():<br>    <span class="hljs-string">"Something done on process index 0 on each server"</span><br>    do_thing_on_index_zero_on_each_server()<br></code></pre></td></tr></table></figure>



<h3 id="推迟执行"><a href="#推迟执行" class="headerlink" title="推迟执行"></a>推迟执行</h3><p>当同时在多个 GPU 上运行脚本时，某些代码的执行速度可能会比其他代码快。在执行下一组指令之前，您可能需要等待所有进程都达到一定程度。例如，在确保每个进程都完成训练之前，您不应该保存模型。</p>
<p>为此，请在代码中添加 <code>wait_for_everyone()</code>。这会阻止所有先完成训练的进程继续训练，直到所有剩余进程都达到相同点（如果在单个 GPU 或 CPU 上运行，则没有影响）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">accelerator.wait_for_everyone()<br></code></pre></td></tr></table></figure>



<h2 id="启动Accelerate脚本"><a href="#启动Accelerate脚本" class="headerlink" title="启动Accelerate脚本"></a>启动Accelerate脚本</h2><p>首先，将训练代码重写为函数，并使其可作为脚本调用。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">  <span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br>  <br>+ <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>      accelerator = Accelerator()<br><br>      model, optimizer, training_dataloader, scheduler = accelerator.prepare(<br>          model, optimizer, training_dataloader, scheduler<br>      )<br><br>      <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:<br>          optimizer.zero_grad()<br>          inputs, targets = batch<br>          outputs = model(inputs)<br>          loss = loss_function(outputs, targets)<br>          accelerator.backward(loss)<br>          optimizer.step()<br>          scheduler.step()<br><br>+ <span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:<br>+     main()<br></code></pre></td></tr></table></figure>



<p>然后，在命令行使用 <code>accelerate config</code> 来配置accelerate的运行环境</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/accelerate/v0.29.3/en/package_reference/cli#accelerate-config">The Command Line (huggingface.co)</a></p>
</blockquote>
<h3 id="使用-accelerate-launch"><a href="#使用-accelerate-launch" class="headerlink" title="使用 accelerate launch"></a>使用 accelerate launch</h3><p>Accelerate 有一个特殊的 CLI 命令，可帮助您通过加速启动在系统中启动代码。该命令包含在各种平台上启动脚本所需的所有不同命令</p>
<p>使用以下命令快速启动脚本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch --accelerate-arg {script_name.py} --script-arg1 --script-arg2 ...<br></code></pre></td></tr></table></figure>

<p>由于这会运行各种 torch 生成方法，因此也可以在此处修改所有环境变量。例如，以下是如何使用单个 GPU ：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">CUDA_VISIBLE_DEVICES="0" accelerate launch {script_name.py} --arg1 --arg2 ...<br></code></pre></td></tr></table></figure>

<p>您也可以使用<code> accelerate launch</code>，而无需先执行 <code>accelerate config</code>，但可能需要手动输入正确的配置参数。在这种情况下，Accelerate 会为你做出一些超参数决定，例如，如果 GPU 可用，它会默认使用所有 GPU，不使用混合精度。</p>
<p>指定要使用的 GPU 数量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...<br></code></pre></td></tr></table></figure>

<p>使用混合精度在两个 GPU 上启动相同的脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...<br></code></pre></td></tr></table></figure>

<p>要获取可以传入的参数的完整列表，请运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch -h<br></code></pre></td></tr></table></figure>

<p>从该自定义 yaml 文件启动脚本如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch --config_file {path/to/config/my_config_file.yaml} {script_name.py} {--arg1} {--arg2} ...<br></code></pre></td></tr></table></figure>



















































<div id="paginator"></div></div><div id="post-footer"><div id="pages" style="justify-content: flex-end"><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2024/04/11/Loading%20&amp;%20Hub/">【Using-Diffusers-01】Loading &amp; Hub Prev →</a></div></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="To Top" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="To Catalog">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="Change Theme"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">TO-Hitori</a></h1><div id="description"><p></p></div></div><div id="aside-block"><div id="toc-div"><h1>Catalog</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Accelerate-Note"><span class="toc-number">1.</span> <span class="toc-text">Accelerate-Note</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86Accelerate%E6%B7%BB%E5%8A%A0%E5%88%B0PyTorch%E4%BB%A3%E7%A0%81%E4%B8%AD"><span class="toc-number">1.1.</span> <span class="toc-text">将Accelerate添加到PyTorch代码中</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Accelerator"><span class="toc-number">1.1.1.</span> <span class="toc-text">Accelerator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87-PyTorch-%E5%AF%B9%E8%B1%A1"><span class="toc-number">1.1.2.</span> <span class="toc-text">准备 PyTorch 对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">1.1.3.</span> <span class="toc-text">训练循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.1.4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-features"><span class="toc-number">1.1.5.</span> <span class="toc-text">Training features</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">梯度累积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA"><span class="toc-number">1.1.5.2.</span> <span class="toc-text">梯度裁剪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.5.3.</span> <span class="toc-text">混合精度训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.1.6.</span> <span class="toc-text">保存和加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%87%E7%89%87%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-number">1.1.6.1.1.</span> <span class="toc-text">切片检查点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8A%B6%E6%80%81"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">状态</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.2.</span> <span class="toc-text">执行进程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B%E4%B8%8A%E6%89%A7%E8%A1%8C"><span class="toc-number">1.2.1.</span> <span class="toc-text">在一个进程上执行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#statement"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">statement</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#function"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E7%89%B9%E5%AE%9A%E8%BF%9B%E7%A8%8B%E4%B8%8A%E6%89%A7%E8%A1%8C"><span class="toc-number">1.2.2.</span> <span class="toc-text">在特定进程上执行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%BF%9F%E6%89%A7%E8%A1%8C"><span class="toc-number">1.2.3.</span> <span class="toc-text">推迟执行</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Accelerate%E8%84%9A%E6%9C%AC"><span class="toc-number">1.3.</span> <span class="toc-text">启动Accelerate脚本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-accelerate-launch"><span class="toc-number">1.3.1.</span> <span class="toc-text">使用 accelerate launch</span></a></li></ol></li></ol></li></ol></div></div><footer><nobr>Published with <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> Theme <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> by <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>